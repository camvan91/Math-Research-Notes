\documentclass[]{article}
\input{../mathdoc}
\input{../mathsym}
%\input{../preview}

\author{Book: James Epperson 2nd, Presenter: Thomas Lewis, Notes by Michael Reed}
\title{Numerical Methods and Analysis}
%date{}

\begin{document}
\maketitle

%\begin{abstract}
%\end{abstract}

\subsection*{Introduction}

Two sources of error:
\begin{enumerate}
	\item Going from a continuous/infinite problem to a discrete/finite problem.
		\begin{example}
			$f'(a) = \lim_{h\ra0} \frac{f(a+h)-f(a)}{h}$, Approximation: fix $h=0.01$, $f'(a)\approx \frac{f(a+0.01)-f(a)}{0.01}$. This is called \ul{truncation error}.
		\end{example}
	\item Rounding. Computers only represent finitely many numbers. (all rational)
		\begin{example}
			Area of a circle $ = \pi r^2 \approx 3.14159r^2$.
		\end{example}
\end{enumerate}

\ul{Focus in this class}
\begin{enumerate}
	\item Root finding: solve $f(x)=0$ for $x$.
	\item Interpolation. Given several function values, estimate others.
	\item Integration: $\int_a^b f(x)\,dx =\ ?$
	\item Applications: Differential Equations.
\end{enumerate}

\subsection*{Basic tools of Calculus}

\begin{theorem}
	[Mean Value Theorem (MVT)]
	Let $f$ be continuous on $[a,b]$ and differentiable on $(a,b)$. Then there exists $\xi$ in $(a,b)$ such that $f'(\xi)=\frac{f(b)-f(a)}{b-a}$.
\end{theorem}

\begin{example}
	Estimate $\cos(0.01)$.
	Know $\cos(0)=1$ and $\frac{d}{dx}[\cos x] = -\sin(x)$ with $-1\leq \sin(x) \leq 1$.
	MVT: $\exists \xi\in(0,0.01)$ such that $\frac{\cos(0.01)-\cos(0)}{0.01-0} = \cos'(\xi)=-\sin(\xi)$. This implies $\cos(0.01) = \cos(0)-0.01\sin(\xi)$.
	So $\cos(0)-0.01\leq \cos(0.01)\leq \cos(0)+0.01$ and $0.99\leq \cos(0.01)\leq 1\leq 1.01$. Thus $\cos(0.01)\approx 0.995$ with $|\text{Error}|\leq 0.005$.
\end{example}

\begin{theorem}
	[Taylor's Remainder Theorem]
	Let $f(x)$ have $n+1$ continuous derivatives on $[a,b]$ for some $n\geq 0$, and let $x,x_0\in[a,b]$.
	Then, $f(x)=P_n(x)+R_n(x)$ for $P_n(x) = \sum_{k=0}^n \frac{(x-x_0)^k}{k!}f^{(k)}(x_0)$ and $R_n(x) = \frac{(x-x_0)^{n+1}}{(n+1)!} f^{(n+1)}(\xi_x)$ for some $\xi_x$ between $x$ and $x_0$, where $P_n(x)$ is an $n$th degree polynomial in $x$ and $R_n(x)$ is a remainder/error.
\end{theorem}

\begin{example}
	For $x_0=0$, 
	\begin{align*}
		e^x &= \sum_{k=0}^n \frac{x^k}{k!}\frac{d}{dx}[e^x]\bigg|_{x=0} + R_n(x) 
		= \sum_{k=0}^n \frac{x^k}{k!} e^x \bigg|_{x=0} + R_n(x) 
		= \sum_{k=0}^n \frac{x^k}{k!} + R_n(x) \\
		e^x &= 1+x+\frac{1}{2!}x^2 + \dots + \frac{1}{n!}x^n + R_n(x)
	\end{align*}
	where $R_n(x) = \frac{(n+1)!}x^{n+1}\cdot e^x$ for $\xi_x$ in $(0,x)$ or $(x,0)$.
\end{example}

\begin{example}
	Calculate $\cos(0.01)$ with an error less than $10^{-6}$.
	$\cos(x) = P_n(x)+R_n(x)$ with $P_n(x) = \sum_{k=0}^n \frac{(-1)^k}{(2k)!}x^{2k}$, $x_0=0$.
	So $\cos(x)-P_n(x) = R_n(x)$ and $|\cos(x)-P_n(x)|\leq |R_n(x)|$.
	If $|R_n(x)|\leq 10^{-6}$, then absolute error is also.
	\begin{align*}
		|R_n|&=\bigg|\frac{(-1)^{n+1}}{(2n+2)!}x^{2n+2}\cos(\xi_x)\bigg| &\qquad \text{ for } x=0.01 \\
			 &\leq \frac{|(-1)^{n+1}|}{(2n+2)!}(0.01)^{2n+2}|\cos(\xi_x)| \\
			 &\leq \frac{1}{(2n+2)!}(0.01)^{2n+2}\cdot 1.
	\end{align*}
For $n=0$: $|R_0(0.01)|\leq 5\cdot 10^{-5}$.
For $n=1$: $|R_1(0.01)|\leq 4.2\times 10^{-10}$.
Use $n=1$, $$\cos(0.01)\approx P_1(0.01) = 1-\frac{1}{2!}(0.01)^2 = 0.99995.$$
\end{example}

$$f(x) = \sum_{k=0}^n \us{P_n(x)}{\frac{(x-x_0)^k}{k!} f^{(k)}(x_0)} + \us{R_n(x)}{\frac{(x-x_0)^{n+1}}{(n+1)!} f^{(n+1)}(\xi_x)}$$
$\xi_x$ is between $x_0$ and $x$.

\begin{example}
	Bound the error when using $P_3(x)$ to approximate $f(x) = \frac{1}{1-x}$ over the interval $[-\frac12,0]$.
\end{example}
\begin{fact}
	$\frac{1}{1-x} = \sum_{k=0}^\infty x^k$ (geometric series), $P_3(x) = 1+x+x^2+x^3$ and $R_3(x) = \frac1{4!} x^4 f^{(4)}(\xi_x)$ with $\xi_x$ between 0 and $x$, know $x$ is in $[-\frac12,0] \implies \xi_x$ is in $[-\frac12,0]$.
	So $f^{(4)}(x) = -\frac{24}{(x-1)^5}$.
\end{fact}
Then (absolute error at $\displaystyle x) = |f(x)-P_3(x)| = |R_3(x)| = \left|\frac1{4!} x^4 f^{(4)}(\xi_x)\right| = \frac1{24}|x^4|\cdot\left|-\frac{24}{(x-1)^5}\right|$.
	It follows ($x$ in $[-\frac12,0]$) $\displaystyle \implies |x^4|\leq \left|-\frac12\right|^4 = \left(\frac12\right)^4$.
	$\displaystyle\left|-\frac{24}{(\xi_x-1)^5}\right| = \frac{24}{|\xi_x-1|^5} \leq \frac{24}{\min|\xi_x-1|^5} = \frac{24}{|-\frac12-1|^5} = \frac{24}{(3/2)^5} = 24\cdot\left(\frac12\right)^5\cdot\left(\frac13\right)^5$.
	Therefore $\displaystyle|f(x)-P_3(x)| \leq \frac1{24}\cdot\left(\frac14\right)^4\cdot 24\cdot 2^5\cdot\left(\frac13\right)^5 = \frac2{3^5}\approx 0.008$.

\begin{theorem}
	[Intermediate Value Theorem]
	Let $f$ be continuous on $[a,b]$ and $w$ such that $f(a)\leq w\leq f(b)$ \ul{or} $f(b)\leq w\leq f(a)$.
	Then there exists $c$ in $[a,b]$ such that $f(c)=w$.
\end{theorem}
\begin{example}
	Show $f(x) = 2x+\cos(x)$ has only 1 zero.

	$f^{(n)}$ is continuous on $(-\infty,\infty)$ for all $n\geq 0$.
	\say{solve} $f(x)=0 \implies w=0$.
	$\lim_{x\ra-\infty} f(x) = -\infty$ and $\lim_{x\ra\infty} f(x) = +\infty$ $\implies$ there exist (at least 1) number $c$ such that $f(c) = 0$.
	Assume there are two numbers $c_1<c_2$ such that $f(c_1)=f(c_2)=0$.
	MVT: there exists $\xi$ in the interval $[c_1,c_2]$ such that $f'(\xi) = \frac{f(c_2)-f(c_1)}{c_2-c_1}$. 
	Thus $f'(\xi) = 2-\sin(x) \geq 2-1 = 1>0$ (contradiction) $\implies c_1=c_2$ (unique solution).
\end{example}

\subsection*{Error, Big Oh}

Exact Value: $A$.
Approximation: $A_n$ ($h$ is a param. fixed in the approx. method. $h\approx 0$, $n$ param. $n\ra\infty$)

\begin{example}
	$A = f'(x)$.
	$A_n = \frac{f(x+h)-f(x)}{h}$, $h$ small.
\end{example}
\begin{example}
	$A = \sum_{k=0}^\infty a_k$.
	$A_n = \sum_{k=0}^n a_k$.
\end{example}

\begin{itemize}
	\item Error $ = A-A_n$
	\item Absolute Error $= |A-A_n|$
	\item Relative Error $= \frac{|A-A_n|}{|A|}$.
\end{itemize}

\begin{example}
	$A = 10^6$. $A_n = 10^6+1$.
	Error $=-1$.
	Absolute Error $=1$.
	Relative Error $=\frac1{10^6} = 10^{-6}$.
\end{example}
\begin{example}
	$A = 10^{-6}$. $A_n = 10^{-7}$.
	Absolute Error $=\frac9{10^7}$.
	Relative Error $=\frac9{10}$.
\end{example}

\newpage

Last time:
Given $A$ and approximations $A_h$ or $A_n$, defined error and absolute and relative error.
Convergence means $\lim_{h\ra0} A_n=A$ (implies $\lim_{h\ra0}\text{error}=0$) and $\lim_{n\ra\infty} A_n = A$ (implies $\lim_{n\ra\infty}\text{error}=0$).
Often we have 2 methods, both convergent, and we want to compare \say{rate or order} of convergence.

A good example to keep in mind: 
Approximate $A = \int_1^3 e^{x^2}\,dx$.
\ul{Method 1}: Use rectangles of width $h = \frac{3-1}{n} = \frac2n$ to get Riemann sums $A_h^1$ or $A_n^1$.
\ul{Method 2}: Use trapezoids of width $h$ to get sums $A_h^2$ or $A_n^2$.

\begin{definition}
	$A = A_h + \mc O(\beta(h))$ if there exists $C>0$, independent of $h$ such that $|A-A_h|\leq C\beta(h)$.
\end{definition}
\begin{note}
	If $\lim_{h\ra0}\beta(h)=0$, then $A_h\ra A$.
\end{note}
\begin{definition}
	$A = A_n + \mc O(\beta(n))$ if there exists $C>0$, independent of $n$, such that $|A-A_n|\leq C\beta(n)$.
\end{definition}
Sometimes consider $a_n$ which is \# flops or amount of memory.

\subsection*{Comparison of methods}

Suppose method 1 has $A^1\sim\mc O(\beta_1(h))$ and method 2 has $A^2\sim\mc O(\beta_2(h))$.
\begin{itemize}
	\item If $\displaystyle\lim_{h\ra0} \frac{\beta_1(h)}{\beta_2(h)}=0$, then method 1 converges faster.
	\item If $\displaystyle\lim_{h\ra0} \frac{\beta_1(h)}{\beta_2(h)}=M\in(0,\infty)$, then methods have same rate.
	\item If $\displaystyle\lim_{h\ra0} \frac{\beta_1(h)}{\beta_2(h)}=+\infty$, then method 2 converges faster.
\end{itemize}
\begin{example}
	Suppose $f$ is smooth on $[a,b]$, and fix $x\in(a,b)$.
	Set $A=f'(x)$, and $A_h = \frac{f(x+h)-f(x)}{h}$.
	Show $A=A_h + \mc O(h)$.
	By Taylor's Theorem, $f(x+h) = f(x) + hf'(x) + \frac12h^2f''(\xi)$, where $\xi$ is between $x$ and $x+h$.
	So $f'(x) = \frac{f(x+h)-f(x)}{h} -\frac12hf''(\xi)$.
	Thus $A = A_h - \frac12hf''(\xi)$. Set $C = \frac12\displaystyle\max_{a\leq\xi\leq b} |f''(\xi)|$.
	Then $|A-A_h|\leq Ch$.
\end{example}
\begin{example}
	Suppose $f$ smooth on $[a,b]$, and fix $x\in(a,b)$.
	$A = f'(x)$, $A_h = \frac{f(x+h)-f(x-h)}{2h}$.
	Show $A = A_h + \mc O(h^2)$. By Taylor's Theorem: $f(x+h) = f(x) + hf'(x) + \frac12h^2f''(x) + \frac16h^2f'''(\xi_1)$, where $\xi_1$ is between $x$ and $x+h$.
	Therefore $f(x-h) = f(x) - hf'(x) + \frac12h^2f''(x) - \frac16h^3f'''(\xi_2)$, where $\xi_2$ is between $x-h$ and $x$.
	So $$ A_h = \frac{2hf'(x) + \frac16h^3[f'''(\xi_1)+f'''(\xi_2)]}{2h} = f'(x) + \frac1{12}h^2[f'''(\xi_1)+f'''(\xi_2)].$$
	Let $C = \frac1{12}\displaystyle\max_{a\leq\xi\leq b}|f'''(\xi)|$.
	So $\displaystyle|A-A_h| \leq \frac1{12}h^2(C+C) = \frac16Ch^2$.
\end{example}

\subsection{Computer Arithmetic}

Computers can only represent a finite amount of numbers using a fixed, finite number of digits.

\begin{definition}
	[Floating point numbers]
	$x = \sigma\times f\times\beta^{t-p}$, where $\sigma = \text{sgn}$ $(+,-)$, $f=$ fraction $ = .d_1d_2\dots d_m$, $\beta = $ base (typically $d \implies d_i\in{0,1}$ for all $i$), $t-p = $ exponent where $t$ variable, $p$ fixed (called the shift).
	
	\ul{Storage}: $x = \us{1\text{ bit}}{\sigma}\mid\us{N\text{ bits}}{t}\mid \us{M\text{ bits}}{f}$, so $M+N+1$ bits.
\end{definition}

\begin{example}
	$\beta = 2$, $p = 15$, $N=5$, $M=8$. $x = \pm 0.d_1d_2\dots d_8\times 2^{t-15}$.
	So $0\leq t\leq 11111_2 = 2^0+2^1+2^2+2^3+2^4 = 31$. So $-15\leq t-p\leq 16$ nearly equal range of positive and negative exponents.
	\begin{enumerate}
		\item $x = -0.25 = -[2^{-2}] = 0.01_2$. So $t-p = -1$ and $t=-1+5 = 14 = 2^3+2^2 + 2^1 = 1110_2$. Thus $x = \text{neg}\mid 01110\mid 10000000$
		\item $x = 50 = 2^5+2^4+2^1 = 110010._2$. So $t-p=6$ implies $t=6+15=21 = 2^4 + 2^2 + 2^0 = 10101_2$. Thus $t = \text{pos}\mid 10101\mid 11001000$.
	\end{enumerate}
\end{example}

\begin{note}
	[Biggest Number] all 1's: $+.\ub{M}{11\dots1}\times 2^{\ob{N}{11\dots1}-p}$
\end{note}
\begin{note}
	[Normailzed] $d_1 = \beta-1$.
\end{note}
\begin{note}
	[Overflow] exponent too large.
\end{note}
\begin{note}
	[Underflow] exponent too small.
\end{note}
\begin{definition}
	[Rounding Error] rounding a real number to a corresponding floating point number.
2 methods: rounding and chopping (round down).
\end{definition}
\begin{example}
	$\frac16+\frac1{10}$ base 10 with 3 digit arithmetic.
	\begin{enumerate}
		\item Exact: $\frac16+\frac1{10} = \frac{10+6}{60} = \frac4{15} = 0.2\conj6$
		\item Chopping: $\frac16 = 0.1\conj6\mapsto 0.166$, $\frac1{10} = 0.1$, add $= 0.266$, rel. error $= \frac{|0.2\conj6-0.266|}{|0.2\conj6|} = \frac{|\frac4{15} - \frac{266}{1000}|}{4/15} = 0.0025$.
		\item Rounding: 0.666\ul{\ul{6}}, $\frac16\mapsto 0.167$, add $= 0.267$, rel error $= \frac{|0.2\conj6-0.267|}{0.2\conj6} = 0.00125$.
	\end{enumerate}
\end{example}

\begin{note}
	:
	\begin{enumerate}
		\item If changing the precision (single to double) drastically changes the output, then rounding errors are having a large effect.
		\item The order in which operations are performed makes a difference $(a+b)+c\neq a+(b+c)$ for some $a,b,c$.
		\item Subtraction can lead to a major loss of accuracy if the two numbers are nearly equal.
	\end{enumerate}
	$e^x = p(x) + R_x$ or $e^{-x} = $ alt. sign in poly or $= \frac1{e^x}$.
\end{note}

\ul{Goal}: Bound the relative error when using floating point arithmetic.
$a\circledast b \mapsto fl[rd(e)*rd(b)]$

\begin{definition}
	Machine epsilon is defined by $$\epsilon_{mach} = \max\setm{x\in\text{Computer \#s}}{1+x = 1\text{ in computer arithmetic}}.$$
\end{definition}
\begin{note}
	$\epsilon_{mach}$ is  not the smallest number in magnitude.
\end{note}
\begin{fact}
	$\left|\frac{rd(x)-x}{x}\right| = \mc O(\epsilon_{mach}) \leq c\epsilon_{mach}$, $x$ is a real number, $rd(x)$ is $fl$.
\end{fact}

\begin{theorem}
	Computer Arithmetic Error $* = +,-,\times,\div$. Then, $\frac{|x*y-fl(x*y)|}{|x*y|} = \mc O(\epsilon_{mach})$.
	\begin{enumerate}
		\item[*] single arithmetic calculations have bounded rel. error.
	\end{enumerate}
\end{theorem}

\end{document}
