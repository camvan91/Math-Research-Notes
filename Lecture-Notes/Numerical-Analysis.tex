\documentclass[]{article}
\input{../mathdoc}
\input{../mathsym}
\input{../theorem}
%\input{../preview}

\author{Book: James Epperson 2nd, Presenter: Thomas Lewis, Notes by Michael Reed}
\title{Numerical Methods and Analysis}
%date{}

\begin{document}
\maketitle

%\begin{abstract}
%\end{abstract}

%\subsection*{Introduction}

Two sources of error:
\begin{enumerate}
	\item Going from a continuous/infinite problem to a discrete/finite problem.
		\begin{example}
			$f'(a) = \lim_{h\ra0} \frac{f(a+h)-f(a)}{h}$, 
			
			Approximation: fix $h=0.01$, $f'(a)\approx \frac{f(a+0.01)-f(a)}{0.01}$. This is called \ul{truncation error}.
		\end{example}
	\item Rounding. Computers only represent finitely many numbers. (all rational)
		\begin{example}
			Area of a circle $ = \pi r^2 \approx 3.14159r^2$.
		\end{example}
\end{enumerate}

\ul{Focus in this class}
\begin{enumerate}
	\item Root finding: solve $f(x)=0$ for $x$.
	\item Interpolation. Given several function values, estimate others.
	\item Integration: $\int_a^b f(x)\,dx =\ ?$
	\item Applications: Differential Equations.
\end{enumerate}

\subsection*{Basic tools of Calculus}

\begin{theorem}
	[Mean Value Theorem (MVT)]
	Let $f$ be continuous on $[a,b]$ and differentiable on $(a,b)$. Then there exists $\xi$ in $(a,b)$ such that $f'(\xi)=\frac{f(b)-f(a)}{b-a}$.
\end{theorem}

\begin{example}
	Estimate $\cos(0.01)$.
	Know $\cos(0)=1$ and $\frac{d}{dx}[\cos x] = -\sin(x)$ with $-1\leq \sin(x) \leq 1$.
	MVT: $\exists \xi\in(0,0.01)$ such that $\frac{\cos(0.01)-\cos(0)}{0.01-0} = \cos'(\xi)=-\sin(\xi)$. This implies $\cos(0.01) = \cos(0)-0.01\sin(\xi)$.
	So $\cos(0)-0.01\leq \cos(0.01)\leq \cos(0)+0.01$ and $0.99\leq \cos(0.01)\leq 1\leq 1.01$. 
	
	Thus $\cos(0.01)\approx 0.995$ with $|\text{Error}|\leq 0.005$.
\end{example}

\begin{theorem}
	[Taylor's Remainder Theorem]
	Let $f(x)$ have $n+1$ continuous derivatives on $[a,b]$ for some $n\geq 0$, and let $x,x_0\in[a,b]$.
	Then, $f(x)=p_n(x)+R_n(x)$ for $$p_n(x) = \sum_{k=0}^n \frac{(x-x_0)^k}{k!}f^{(k)}(x_0)$$ and 
	$$R_n(x) = \frac{(x-x_0)^{n+1}}{(n+1)!} f^{(n+1)}(\xi_x)$$ 
	for some $\xi_x$ between $x$ and $x_0$, where $p_n(x)$ is an $n$th degree polynomial in $x$ and $R_n(x)$ is a remainder/error.
\end{theorem}

\begin{example}
	For $x_0=0$, 
	\begin{align*}
		e^x &= \sum_{k=0}^n \frac{x^k}{k!}\frac{d}{dx}[e^x]\bigg|_{x=0} + R_n(x) 
		= \sum_{k=0}^n \frac{x^k}{k!} e^x \bigg|_{x=0} + R_n(x) 
		= \sum_{k=0}^n \frac{x^k}{k!} + R_n(x) \\
		e^x &= 1+x+\frac{1}{2!}x^2 + \dots + \frac{1}{n!}x^n + R_n(x)
	\end{align*}
	where $R_n(x) = \frac{(n+1)!}x^{n+1}\cdot e^x$ for $\xi_x$ in $(0,x)$ or $(x,0)$.
\end{example}

\begin{example}
	Calculate $\cos(0.01)$ with an error less than $10^{-6}$.
	Let $\cos(x) = p_n(x)+R_n(x)$ with $x_0 = 0$ and $$p_n(x) = \sum_{k=0}^n \frac{(-1)^k}{(2k)!}x^{2k}.$$ %, $x_0=0$.
	So $\cos(x)-p_n(x) = R_n(x)$ and $|\cos(x)-p_n(x)|\leq |R_n(x)|$.
	If $|R_n(x)|\leq 10^{-6}$, then absolute error is also.
	\begin{align*}
		|R_n|&=\bigg|\frac{(-1)^{n+1}}{(2n+2)!}x^{2n+2}\cos(\xi_x)\bigg| \qquad \text{ for } x=0.01 \\
			 &\leq \frac{|(-1)^{n+1}|}{(2n+2)!}(0.01)^{2n+2}|\cos(\xi_x)| 
			 \leq \frac{1}{(2n+2)!}(0.01)^{2n+2}\cdot 1.
	\end{align*}
For $n=0$: $|R_0(0.01)|\leq 5\cdot 10^{-5}$.
For $n=1$: $|R_1(0.01)|\leq 4.2\times 10^{-10}$.
Use $n=1$, $$\cos(0.01)\approx p_1(0.01) = 1-\frac{1}{2!}(0.01)^2 = 0.99995.$$
\end{example}

\begin{recall}
	[Taylor's Theorem]
$\ds f(x) = \sum_{k=0}^n \us{p_n(x)}{\frac{(x-x_0)^k}{k!} f^{(k)}(x_0)} + \us{R_n(x)}{\frac{(x-x_0)^{n+1}}{(n+1)!} f^{(n+1)}(\xi_x)}$,
$\xi_x$ is between $x_0$ and $x$.
\end{recall}

\begin{example}
	Bound the error when using $p_3(x)$ to approximate $f(x) = \frac{1}{1-x}$ over the interval $[-\frac12,0]$.
\end{example}
\begin{fact}
	$\ds\frac{1}{1-x} = \sum_{k=0}^\infty x^k$ (geometric series), $p_3(x) = 1+x+x^2+x^3$ and $\ds R_3(x) = \frac1{4!} x^4 f^{(4)}(\xi_x)$ with $\xi_x$ between 0 and $x$, know $x$ is in $[-\frac12,0] \implies \xi_x$ is in $[-\frac12,0]$.
	So $f^{(4)}(x) = -\frac{24}{(x-1)^5}$.
\end{fact}
Then (absolute error at $\displaystyle x) = |f(x)-p_3(x)| = |R_3(x)| = \left|\frac1{4!} x^4 f^{(4)}(\xi_x)\right| = \frac1{24}|x^4|\cdot\left|-\frac{24}{(x-1)^5}\right|$.
	It follows ($x$ in $[-\frac12,0]$) $\displaystyle \implies |x^4|\leq \left|-\frac12\right|^4 = \left(\frac12\right)^4$
	and $\displaystyle\left|-\frac{24}{(\xi_x-1)^5}\right| = \frac{24}{|\xi_x-1|^5} \leq \frac{24}{\min|\xi_x-1|^5} = \frac{24}{|-\frac12-1|^5} = \frac{24}{(3/2)^5} = 24\cdot\left(\frac12\right)^5\cdot\left(\frac13\right)^5$.
	Therefore $\displaystyle|f(x)-p_3(x)| \leq \frac1{24}\cdot\left(\frac14\right)^4\cdot 24\cdot 2^5\cdot\left(\frac13\right)^5 = \frac2{3^5}\approx 0.008$.

\begin{theorem}
	[Intermediate Value Theorem]
	Let $f$ be continuous on $[a,b]$ and $w$ such that $f(a)\leq w\leq f(b)$ \ul{or} $f(b)\leq w\leq f(a)$.
	Then there exists $c$ in $[a,b]$ such that $f(c)=w$.
\end{theorem}
\begin{example}
	Show $f(x) = 2x+\cos(x)$ has only 1 zero.

	$f^{(n)}$ is continuous on $(-\infty,\infty)$ for all $n\geq 0$.
	\say{solve} $f(x)=0 \implies w=0$.
	$\lim_{x\ra-\infty} f(x) = -\infty$ and $\lim_{x\ra\infty} f(x) = +\infty$ $\implies$ there exist (at least 1) number $c$ such that $f(c) = 0$.
	Assume there are two numbers $c_1<c_2$ such that $f(c_1)=f(c_2)=0$.
	MVT: there exists $\xi$ in the interval $[c_1,c_2]$ such that $f'(\xi) = \frac{f(c_2)-f(c_1)}{c_2-c_1}$. 
	Thus $f'(\xi) = 2-\sin(x) \geq 2-1 = 1>0$ (contradiction) $\implies c_1=c_2$ (unique solution).
\end{example}

\subsection*{Error, Big Oh}

Exact Value: $A$.
Approximation: $A_n$ ($h$ is a param. fixed in the approx. method. $h\approx 0$, $n$ param. $n\ra\infty$)

\begin{example}
	$A = f'(x)$.
	$A_n = \frac{f(x+h)-f(x)}{h}$, $h$ small.
\end{example}
\begin{example}
	$A = \sum_{k=0}^\infty a_k$.
	$A_n = \sum_{k=0}^n a_k$.
\end{example}

\begin{itemize}
	\item Error $ = A-A_n$
	\item Absolute Error $= |A-A_n|$
	\item Relative Error $= \frac{|A-A_n|}{|A|}$.
\end{itemize}

\begin{example}
	$A = 10^6$. $A_n = 10^6+1$.
	Error $=-1$.
	Absolute Error $=1$.
	Relative Error $=\frac1{10^6} = 10^{-6}$.
\end{example}
\begin{example}
	$A = 10^{-6}$. $A_n = 10^{-7}$.
	Absolute Error $=\frac9{10^7}$.
	Relative Error $=\frac9{10}$.
\end{example}

\newpage

Last time:
Given $A$ and approximations $A_h$ or $A_n$, defined error and absolute and relative error.
Convergence means $\lim_{h\ra0} A_n=A$ (implies $\lim_{h\ra0}\text{error}=0$) and $\lim_{n\ra\infty} A_n = A$ (implies $\lim_{n\ra\infty}\text{error}=0$).
Often we have 2 methods, both convergent, and we want to compare \say{rate or order} of convergence.

A good example to keep in mind: 
Approximate $A = \int_1^3 e^{x^2}\,dx$.
\ul{Method 1}: Use rectangles of width $h = \frac{3-1}{n} = \frac2n$ to get Riemann sums $A_h^1$ or $A_n^1$.
\ul{Method 2}: Use trapezoids of width $h$ to get sums $A_h^2$ or $A_n^2$.

\begin{definition}
	$A = A_h + \mc O(\beta(h))$ if there exists $C>0$, independent of $h$ such that $|A-A_h|\leq C\beta(h)$.
\end{definition}
\begin{note}
	If $\lim_{h\ra0}\beta(h)=0$, then $A_h\ra A$.
\end{note}
\begin{definition}
	$A = A_n + \mc O(\beta(n))$ if there exists $C>0$, independent of $n$, such that $|A-A_n|\leq C\beta(n)$.
\end{definition}
Sometimes consider $a_n$ which is \# flops or amount of memory.

\subsection*{Comparison of methods}

Suppose method 1 has $A^1\sim\mc O(\beta_1(h))$ and method 2 has $A^2\sim\mc O(\beta_2(h))$.
\begin{itemize}
	\item If $\displaystyle\lim_{h\ra0} \frac{\beta_1(h)}{\beta_2(h)}=0$, then method 1 converges faster.
	\item If $\displaystyle\lim_{h\ra0} \frac{\beta_1(h)}{\beta_2(h)}=M\in(0,\infty)$, then methods have same rate.
	\item If $\displaystyle\lim_{h\ra0} \frac{\beta_1(h)}{\beta_2(h)}=+\infty$, then method 2 converges faster.
\end{itemize}
\begin{example}
	Suppose $f$ is smooth on $[a,b]$, and fix $x\in(a,b)$.
	Set $A=f'(x)$, and $A_h = \frac{f(x+h)-f(x)}{h}$.
	Show $A=A_h + \mc O(h)$.
	By Taylor's Theorem, $f(x+h) = f(x) + hf'(x) + \frac12h^2f''(\xi)$, where $\xi$ is between $x$ and $x+h$.
	So $f'(x) = \frac{f(x+h)-f(x)}{h} -\frac12hf''(\xi)$.
	Thus $A = A_h - \frac12hf''(\xi)$. Set $C = \frac12\displaystyle\max_{a\leq\xi\leq b} |f''(\xi)|$.
	Then $|A-A_h|\leq Ch$.
\end{example}
\begin{example}
	Suppose $f$ smooth on $[a,b]$, and fix $x\in(a,b)$.
	$A = f'(x)$, $A_h = \frac{f(x+h)-f(x-h)}{2h}$.
	Show $A = A_h + \mc O(h^2)$. By Taylor's Theorem: $f(x+h) = f(x) + hf'(x) + \frac12h^2f''(x) + \frac16h^2f'''(\xi_1)$, where $\xi_1$ is between $x$ and $x+h$.
	Therefore $f(x-h) = f(x) - hf'(x) + \frac12h^2f''(x) - \frac16h^3f'''(\xi_2)$, where $\xi_2$ is between $x-h$ and $x$.
	So $$ A_h = \frac{2hf'(x) + \frac16h^3[f'''(\xi_1)+f'''(\xi_2)]}{2h} = f'(x) + \frac1{12}h^2[f'''(\xi_1)+f'''(\xi_2)].$$
	Let $C = \frac1{12}\displaystyle\max_{a\leq\xi\leq b}|f'''(\xi)|$.
	So $\displaystyle|A-A_h| \leq \frac1{12}h^2(C+C) = \frac16Ch^2$.
\end{example}

\subsection*{Computer Arithmetic}

Computers can only represent a finite amount of numbers using a fixed, finite number of digits.

\begin{definition}
	[Floating point numbers]
	$x = \sigma\times f\times\beta^{t-p}$, where $\sigma = \text{sgn}$ $(+,-)$, $f=$ fraction $ = .d_1d_2\dots d_m$, $\beta = $ base (typically $d \implies d_i\in{0,1}$ for all $i$), $t-p = $ exponent where $t$ variable, $p$ fixed (called the shift).
	
	\ul{Storage}: $x = \us{1\text{ bit}}{\sigma}\mid\us{N\text{ bits}}{t}\mid \us{M\text{ bits}}{f}$, so $M+N+1$ bits.
\end{definition}

\begin{example}
	$\beta = 2$, $p = 15$, $N=5$, $M=8$. $x = \pm 0.d_1d_2\dots d_8\times 2^{t-15}$.
	So $0\leq t\leq 11111_2 = 2^0+2^1+2^2+2^3+2^4 = 31$. So $-15\leq t-p\leq 16$ nearly equal range of positive and negative exponents.
	\begin{enumerate}
		\item $x = -0.25 = -[2^{-2}] = 0.01_2$. So $t-p = -1$ and $t=-1+5 = 14 = 2^3+2^2 + 2^1 = 1110_2$. Thus $x = \text{neg}\mid 01110\mid 10000000$
		\item $x = 50 = 2^5+2^4+2^1 = 110010._2$. So $t-p=6$ implies $t=6+15=21 = 2^4 + 2^2 + 2^0 = 10101_2$. Thus $t = \text{pos}\mid 10101\mid 11001000$.
	\end{enumerate}
\end{example}

\begin{note}
	[Biggest Number] all 1's: $+.\ub{M}{11\dots1}\times 2^{\ob{N}{11\dots1}-p}$
\end{note}
\begin{note}
	[Normailzed] $d_1 = \beta-1$.
\end{note}
\begin{note}
	[Overflow] exponent too large.
\end{note}
\begin{note}
	[Underflow] exponent too small.
\end{note}
\begin{definition}
	[Rounding Error] rounding a real number to a corresponding floating point number.
\end{definition}
2 methods: rounding and chopping (round down).
\begin{example}
	$\frac16+\frac1{10}$ base 10 with 3 digit arithmetic.
	\begin{enumerate}
		\item Exact: $\frac16+\frac1{10} = \frac{10+6}{60} = \frac4{15} = 0.2\conj6$
		\item Chopping: $\frac16 = 0.1\conj6\mapsto 0.166$, $\frac1{10} = 0.1$, add $= 0.266$, 
			
			relative error $\ds= \frac{|0.2\conj6-0.266|}{|0.2\conj6|} = \frac{|\frac4{15} - \frac{266}{1000}|}{4/15} = 0.0025$.
		\item Rounding: 0.666\ul{\ul{6}}, $\frac16\mapsto 0.167$, add $= 0.267$, 
			
			relative error $\ds= \frac{|0.2\conj6-0.267|}{0.2\conj6} = 0.00125$.
	\end{enumerate}
\end{example}

\begin{note}
	:
	\begin{enumerate}
		\item If changing the precision (single to double) drastically changes the output, then rounding errors are having a large effect.
		\item The order in which operations are performed makes a difference $(a+b)+c\neq a+(b+c)$ for some $a,b,c$.
		\item Subtraction can lead to a major loss of accuracy if the two numbers are nearly equal.
	\end{enumerate}
	$e^x = p(x) + R_x$ or $e^{-x} = $ alternating sign in polynomial or $= \frac1{e^x}$.
\end{note}

\ul{Goal}: Bound the relative error when using floating point arithmetic.
$a\circledast b \mapsto fl[rd(e)*rd(b)]$

\begin{definition}
	Machine epsilon is defined by $$\epsilon_{mach} = \max\setm{x\in\text{Computer \#s}}{1+x = 1\text{ in computer arithmetic}}.$$
\end{definition}
\begin{note}
	$\epsilon_{mach}$ is  not the smallest number in magnitude.
\end{note}
\begin{fact}
	$\ds\left|\frac{rd(x)-x}{x}\right| = \mc O(\epsilon_{mach}) \leq c\epsilon_{mach}$, $x$ is a real number, $rd(x)$ is $fl$.
\end{fact}

\begin{theorem}
	[Computer Arithmetic Error] Let $* = +,-,\times,\div$. Then, $$\ds\frac{|x*y-fl(x*y)|}{|x*y|} = \mc O(\epsilon_{mach}).$$
	\begin{enumerate}
		\item[$*$] single arithmetic calculations have bounded relative error.
	\end{enumerate}
\end{theorem}

\subsection*{\S1.5}

\ul{Goal}: Approximate $e^x$ and integrals involving $e^x$ using simple functions.

\ul{Book}: $\ds f(x) = \frac2{\sqrt\pi}\int_0^x e^{-t^2}\,dt\equiv\text{erf}(x)$.

\begin{example}
	$f(x) = \int_0^x t^2e^{-t^2}\,dt$. Approximate over $(0,2)$. Bound Error.
	
	Strategy: $t^2e^{-t^2}\approx p(t)$. Integrate $p(t)$ exactly.
	\begin{align*}
		e^t &= p_k(t) + R_k(t) 
			= 1+t+\frac12t^2+\dots+\frac1{k!}t^k \us{c_t\text{ between 0 and }t}{+ \frac{t^{k+1}}{(k+1)!}e^{c_t}} \\
		\implies t^2e^{-t^2} &= t^2 \bigg[ 1+(-t^2) + \frac12(-t^2)^2 + \dots + \frac1{k!}(-t^2)^k + \us{c_{-t^2}\text{ between }-t^2\text{ and }0}{\frac{(-t^2)^{k+1}}{(k+1)!} e^{c_{(-t^2)}}} \bigg] \\
			&= \ub{q_k(t)}{\sum_{i=0}^k(-1)^k\cdot\frac{t^{2k+2}}{k!}} + \ub{r_k(t)}{\frac{(-1)^{k+1}t^{2k+4}}{(k+1)!}e^c} \\
		f(x) &= \int_0^x t^2e^{-t^2}\,dt 
			 = \int_0^x \blr{q_k(t) + r_k(t)}\,dt 
			 = \sum_{i=0}^k(-1)^k\cdot \frac{t^{2k+3}}{(2k+3)k!}\bigg|_0^x + \int_0^x r_k(t)\,dt \\
			 &= \sum_{i=0}^k(-1)^k\frac{x^{2k+3}}{(2k+3)k!} + \ub{E_k(x)}{\int_0^x r_k(t) \, dt}.
	\end{align*}
	Bound $|E_k(x)|$: 
	Let $x$ between 0 and 2, $t$ between 0 and $x$, $c$ between 0 and $-t^2 \implies c\in[-x^2,0]$ with
	$$E_k(x) = \frac{(-1)^{k+1}}{(k+1)!} \int_0^x \ub{>0\text{ for all }t>0}{t^{2k+4}e^c}\, dt.$$
	$*$ Naive bound: $\int_0^x t^{2k+4}e^c\,dt \leq \max |t^{2k+4}|\cdot\max|e^c|\cdot|x-0| = (2^{2k+4})(e^0)(2)$.
	So $$|E_k(t)| \leq \frac{2\cdot2^{2k+4}}{(k+1)!}\ra0$$ as $k\ra\infty.$
	\begin{recall}
		$\int_a^b f(x)\, dx \leq \max|f(x)|\cdot(b-a)$.
	\end{recall}
	MVT, $\xi_x\in(-x^2,0)$, 
	\begin{align*}
		E_k(x) &= \frac{(-1)^{k+1}}{(k+1)!}e^{\xi_x}\int_0^x t^{2k+4}\,dt 
			   = \frac{(-1)^{k+1}}{(k+1)!}e^{\xi_x}\cdot\frac{t^{2k+5}}{2k+5}\bigg|_0^x 
			   = \ub{\delta_k(x)}{\frac{(-1)^{k+1}}{(2k+5)(k+1)!}x^{2k+5}}\ub{\us{\leq M}{\mid}}{e^{\xi_x}},
	\end{align*}
	$M$ based on original problem.
	For $x$ fixed; $\lim_{k\ra\infty}\delta_k(x)=0$.
	So 
	\begin{align*}
		|E_k(x)| &= \frac1{(k+1)!(2k+5)}\cdot|x|^{2k+5}\cdot e^{\xi_x} 
				 \leq \frac1{(k+1)!(2k+5)}|x|^{2k+5}M
	\end{align*}
	where $M=e^0=1$ because $\xi_x\in(-x^2,0) \implies \xi_x\in(-4,0)$ since $x<2$.
	For $x$ in $(0,2)$: $|x|^{2k+5} \leq 2^{2k+5}$.
	Thus $$|E_k(x)|\leq \frac{2^{2k+5}}{\ub{\text{fairly precise}}{(k+1)!(2k+5)}} \leq \frac{2^54^k}{7(k+1)!} = \frac{32}{7}\cdot\frac{4^k}{(k+1)!}\ (\forall k\geq1).$$
	$\therefore \forall x\in(0,2)$:
	\begin{align*}
		f(x) &= \int_0^xt^2e^{-t^2}\,dt = \sum_{i=0}^k\frac{(-1)^k}{k!(2k+3)}x^{2k+3} + \mc O\plr{\frac{4^k}{(k+1)!}} \\
		k = 10 &: |E_k(x)| \leq \frac{32}{7}\cdot \frac{4^{10}}{11!}\leq 0.12009 \\
		k = 20 &: |E_k(x)| \leq 0.84\times 10^{-8}
	\end{align*}
\end{example}

\begin{example}
	Find the order of accuracy when approximating $e^x$ with the rational function
	$$r(x) = \frac{1+\frac12x}{1-\frac12x}$$ for $-1\leq x\leq 0$. So $|e^x-r(x)|\leq$ ?
	Then
	\begin{align*}
		e^x-r(x) &= 1+x+\frac12x^2+\frac16x^3+\dots+\frac1{k!}x^k+\dots -\frac{1+\frac12x}{1-\frac12x} \\
				 &= \frac1{1-\frac12x}\blr{\plr{1-\frac12x}\plr{1+x+\frac12x^2+\frac16x^3 + \mc O(x^4)} - \plr{1+\frac12x}} \\
				 &= \frac1{1-\frac12x}\blr{1+x+\frac12x^2+\frac16x^3+\mc O(x^4) - \frac12x^2 - \frac14x^3 - 1 - \frac12x} \\
				 &= \ub{\text{Exact Error}}{\frac1{1-\frac12x}\bigg[\us{-\frac1{12}}{\plr{\frac16-\frac14}}x^3+\mc O(x^4)\bigg]}
	\end{align*}
	For $x\leq0$: $\frac1{1-\frac12x}\leq 1 \implies |e^x-r(x)| \leq \frac1{12}|x|^3 + \mc O(x^4) \implies \ul{e^x = r(x) + \mc O(|x|^3)}$,
	($x^4\leq|x|^3$ for $-1\leq x\leq0)$.

	\ul{? $x<-1$ ?}
	Then $e^x = 1+x+\frac12x^2+\os{\downarrow\text{won't cancel.}}{\frac16x^3e^\xi}$ for $\xi$ between 0 and $x$.
\end{example}

\subsection*{\S2.1 Horner's Rule}

\ul{Goal}: Compute polynomials efficinetly and more accurately.

\begin{align*}
	p(x) &= a_0+a_1x+a_2x^2 + \dots + a_nx^n \qquad \text{(Standard/expanded form)} \\
		 &= a_0 + x(a_1 + a_2x+\dots + a_nx^{n-1}) \\
		 &= a_0 + x\blr{a_1+x(a_2+\dots+a_nx^{n-2})} \\
		 & \qquad \vdots \\
	\text{nested form -- } &= a_0 + x(a_1 + x(a_2 + \dots + x(a_{n-1}+a_nx)\dots)).
\end{align*}
\begin{note}:
	\begin{enumerate}
		\item nested form $n+1$ multiplications, $n$ additions.
			
			standard - same + cost of computing $x^2,x^3,\dots,x^n$.
		\item typically more accurate.
		\item Most accurate and efficient is fully factored form:
			$p(x) = a_n(x-c_1)^{m_1}(x-c_2)^{m_2}\dots(x-c_k)^{m_k}$
			where $m_1+m_2+\dots+m_k = n$.
			Not known in general.
	\end{enumerate}
\end{note}

\begin{algorithm}
	\caption{Horner's Method}
	\begin{algorithmic}
		\Require $a_0,a-1,\dots,a_n$ and $x$
		\State $px\gets a_n$
		\For{$k=n-1$ to 0}
		\State $px\gets a_k + px\cdot x$
		\EndFor
		\State\Return $px = p(x)$
		\Comment{a slight modification outputs $p'(x)$.}
	\end{algorithmic}
\end{algorithm}
%Given $a_0,a_1,\dots,a_n$ and $x$.
%$px = a_n$
%for $k=n-1$ to 0
%	$px = a_k + px\cdot x$
%end
%output $px = p(x)$.
%$*$ A slight modification outputs $p'(x)$.


\begin{example}
		$p(x) = 1-x+\frac12x^2-\frac14x^5 
			 = 1+x(-1+\frac12x-\frac14x^4) 
			 = 1+x(-1+x(\frac12-\frac14x^3)) $.
\end{example}
\begin{example}
		$p(x) = 1+x^2 + \frac12x^4 
			 = 1+x^2(1+\frac12x^2)$.
\end{example}

\subsection*{\S1.6 Approximating natural log}

\begin{align*}
	x &= f\times \beta^{t-p} \\
	\ln(x) &= \ln(f\times\beta^{t-p})
		   = \ln(f) + (t-p)\ln(\beta)
\end{align*}
For $\beta=2$: $\frac12\leq f\leq 1$.
Assume $\ln(2)$ is known to high precision.
We only need to approximate $\ln(f)$ for $\frac12\leq f\leq 1$ to high precision for any $x\in(0,\infty)$.
This implies only need a Taylor's series on a small interval, which implies we can bound error uniformly.

\ul{Method in book}: $f_0 = \frac34$, $|R_n(z)|\leq\frac12\plr{\frac13}^n$ for all $\frac12\leq z\leq 1$.
Set $n=33$ in $p_n(z) \implies$ Error $\leq10^{-16}$.

\ul{Goal}: use $\log$ properties to improve the convergence rate. (based on problem 1.6.6)

\begin{enumerate}
	\item Find $w$ such that $\frac{1-w}{1+w}=z$ for $\frac12\leq z\leq 1$.
		This implies $w = \frac{1-z}{1+z}$ with $0\leq w\leq\frac13$ (shrunk domain size).
	\item $\ln(z) = \ln\plr{\frac{1-w}{1+w}} = \ln(1-w)-\ln(1+w)$.
	\item Taylor's expansion for $\ln(1-w),\ln(1+w)$ centered at $w_0=\frac16$ (midpoint for $w$).
\end{enumerate}
Integral form: 
%\begin{align*}
	$f(x) = p_n(x)+R_n(\xi_x)$, 
	$\ds p_n(x)	 = \sum_{k=0}^n \frac{(x-x_0)^k}{k!}f^{(k)}(x_0)$,
	$\ds R_n(\xi_x) = \int_{x_0}^x\frac{(x-t)^n}{n!}f^{(n+1)}(t)\,dt$.
%\end{align*}
Then
%\begin{align*}
	$y = \ln(x)$, 
	$y' = \frac1x = x\inv$, 
	$y'' = -x^{-2}$, 
	$y''' = 2x^{-3}$,
	$y^{(4)} = -6x^{-4}$,
	$\implies y^{(k)} = (-1)^{k-1}(k-1)! x^{-k}$,
%\end{align*}
with $k\geq 1$.
\begin{align*}
	\ln(1+w) = \ln(1+w_0) + \frac{w-w_0}{1+w_0} & + \sum_{k=2}^n \frac{(w-w_0)^k}{k!} (-1)^k (k+1)! (1+w_0)^{-k} \\
			  & + \int_{w_0}^w (w-t)^n\cdot(-1)^n(n+1-1)!(1+t)^{-n-1}\,dt \\
			 = \ln(1+w_0) + \frac{w-w_0}{1+w_0} & + \sum_{k=2}^n \frac{(-1)^{k-1}(w-w_0)^k}{k(1+w_0)^k} + \int_{w_0}^w (-1)^n(w-t)^n(1-t)^{-n-1}\,dt \\
	\ln(1-w) = \ln(1-w_0) - \frac{w-w_0}{1-w_0} & + \sum_{k=2}^n \frac{(-1)^k}k \frac{(w-w_0)^k}{(1-w_0)^k} + \int_{w_0}^w (-1)^{n+1}(w-t)^n(1-t)^{-n-1}\,dt
\end{align*}
\begin{enumerate}
	\item[4.] Bound $R_n(w)$ over $[0,\frac13]$.
		Note that $\ds\mlr{\int_a^b f(x)\,dx} \leq |b-a|\cdot\max_{a\leq x\leq b}|f(x)|$.
		So
		\begin{align*}
			\mlr{\int_{w_0}^w (-1)^n(w-t)^n(1+t)^{-n-1}\,dt}
				&\leq |w-w_0|\cdot\max_t\mlr{(-1)^n(w-t)^n(1+t)^{-n-1}} \\
				&\leq \frac16\cdot\max_t\mlr{\frac{(w-t)^n}{(1+t)^{n+1}}}
				\leq \frac16\cdot\max_t\mlr{\frac1{1+t}}\cdot\us{\us{\implies\text{ max at a point}}{\text{no critical points on }[0,\frac13]}}{\max_t\mlr{\frac{w-t}{1+t}}^n} \\
				&\leq \frac16 \cdot 1\cdot\max\set{\mlr{\frac{w-w_0}{1+w}}^n,\mlr{\frac{w-w_0}{1+w_0}}^n}
				= \frac16\cdot\max\mlr{\frac{w-w_0}{1+w_0}}^n \\
				&\leq \frac16\cdot\frac{(1/3)^n}{(2/6)^n} 
				= \frac16\cdot\plr{\frac17}^n
		\end{align*}
		$0\leq t\leq\frac13$ (lazy)
		\begin{align*}
			\mlr{\int_{w_0}^w (-1)^{n+1}(w-t)^n(1-t)^{-n-1}\,dt} 
			&\leq \frac16\cdot\max_t\mlr{\frac1{1-t}}\cdot\max_t\mlr{\frac{w-t}{1-t}}^n \\
			&\leq \frac16\cdot\frac32\cdot\mlr{\frac{1/6}{5/6}}^n
			= \frac14\plr{\frac15}^n.
		\end{align*}
		Combining:
		Total Error $\leq \frac16\plr{\frac17}^n + \us{\text{bigger}}{\frac14\plr{\frac15}^n} \leq \frac12\plr{\frac15}^n$.
		
		If error $\leq 10^{-16} \implies \frac12\plr{\frac15}^n\leq 10^{-16} \implies n\geq \log_2(\frac12\cdot10^{-16}) \approx 22.46$.
		
		Given $z$ in $[\frac12,1]$, $n\geq 23$ will give $10^{-16}$ accuracy for
		\begin{align*}
			\ln(z) &= \ln(1-w)-\ln(1+w) 
				   \approx p_n(1-w)-q_n(1+w) \qquad \text{for} \\
			p_n(x) &= \ln\plr{\frac56} + \sum_{k=1}^n \frac{(-1)^k}k\plr{\frac{x-\frac16}{5/6}}^k \\
			q_n(x) &= \ln\plr{\frac76} + \sum_{k=1}^n\frac{(-1)^{k-1}}k\plr{\frac{x-\frac16}{\frac76}}^k
		\end{align*}
		with $w = \frac{1-z}{1+z}$, need $\ln(2),\ln(\frac56),\ln(\frac76)$ to high precision.
\end{enumerate}

\subsection*{\S2.2 Difference Approximation of Derivatives}

$$ f'(x) = \lim_{h\ra0} \frac{f(x+h)-f(x)}h.$$
Forward difference approximation ($h\ra0^+$)
$$ D_h^+f(x) = \frac{f(x+h)-f(x)}h = f'(x) + \mc O(h)$$
Backward difference approximation ($h\ra0^-$)
$$ D_h^- f(x) = \frac{f(x)-f(x-h)}h = f'(x) + \mc O(h)$$
Central difference approximation
$$D_hf(x) = \frac{f(x+h)-f(x-h)}{2h} = f'(x) + \mc O(h^2) = \frac12 \blr{D_h^+f(x) + D_h^-f(x)} $$
\begin{note}:
	\begin{enumerate}
		\item Prone to cancellation error due to subtraction in the numerator, after threshold roundoff dominates.
		\item Using more nodes can increase the accuracy.
	\end{enumerate}
\end{note}
\begin{example}
	$\ds f'(x) = \frac{8f(x+h)-8f(x-h)-f(x+2h) + f(x-2h)}{12h} + \mc O(h^4)$.
\end{example}

\subsubsection*{Calculating Rates of Convergence}

Suppose $E = \mc (h^p)$ for some $p$. This implies $E\approx Ch^p$ for some constant $C$ when $h\ra0$.
Pick $h_1,h_2>0$ with $h_1\neq h_2$.
So $E_1\approx Ch_1^p$ and $E_2\approx Ch_2^p$ imply
\begin{align*}
	\frac{E_1}{E_2} &\approx \plr{\frac{h_1}{h_2}}^p, \\ %&\quad
	\log\plr{\frac{E_1}{E_2}} &\approx p\log\plr{\frac{h_1}{h_2}}, \\%&\quad
	p &\approx \frac{\log(E_1/E_2)}{\log(h_1/h_2)}.
\end{align*}
Approximating for several $h_i$ values lets us estimate $p$.
\ul{Book}: $h_1=2h_2 \implies \frac{E_1}{E_2} \approx 2^p$.
If $p=1$: $\frac{E_1}{E_2} \approx 2^1 = 2$ (error halves when $h\ra h/2$)
$p=2$: $\frac{E_1}{E_2} \approx 2^2 = 4$ (error $\ra$ error/4 as $h\ra h/2$)

\subsubsection*{Second derivatives}

Centered difference approximation $$\ds f''(x) = \frac{f(x+2h)-2f(x)+f(x-h)}{h^2} + \mc O(h^2) \equiv D_h^2 f(x) = D_h^- D_h^+ f(x) = D_h^+ D_h^- f(x).$$

\begin{proof}
	Observe, 
\begin{align*}
	D_h^+ D_h^- f(x) &= D_h^+ \blr{ \frac{f(x)-f(x+h)}h }
					 = \frac1h\blr{D_h^+f(x)-D_h^+f(x-h)} \\
					 &= \frac1h\blr{\frac{f(x+h)-f(x)}h-\frac{f(x-h+h)-f(x-h)}h} \\
					 &= \frac1{h^2}\blr{f(x+h)-f(x)-f(x)+f(x+h)}
					 = D_h^2f(x)
\end{align*}
\end{proof}
\begin{proof}
	[Accuracy] Goal: $f''(x) \la$ everything at $x$.
	\begin{align*}
		D_h^2f(x) &= \frac{f(x+h)-2f(x)+f(x-h)}{h^2} \\
				  &= \frac1{h^2} \bigg[ f(x) + hf'(x) + \frac12h^2f''(x) + \frac16h^3f''(x) + \frac1{24}h^4f^{(4)}(x) + \mc O(h^5) \\
				  &\quad - 2f(x) + f(x) - hf'(x) + \frac12h^2f''(x)-\frac16h^3f'''(x) + \frac1{24}h^4f^{(4)}(x) \bigg] \\
				  &= \frac1{h^2}\blr{h^2f''(x) + \frac1{12}h^4f^{(4)}(x)+\mc O(h^5)}
				  = f''(x) + \ub{\mc O(h^2)}{\frac1{12}h^2f^{(4)}(x)} + \mc O(h^5)
	\end{align*}
\end{proof}
\begin{note}:
	\begin{align*}
		\ds D_h D_hf(x) &= f''(x) + \mc O\plr{(2h)^2} = \frac{f(x+2h)-2f(x)+f(x-2h)}{4h^2}, \\
		%$D_h^2 f(x)$:	
		D_h^2f(x) &= f''(x) + \us{\uparrow f^{(4)}(x)}{\mc O(h^2)}.
	\end{align*}
	Error = 0 if $f(x)$ is cubic.
\end{note}

\subsection*{\S2.3 Application: Euler's Methods for Initial Value Problems}

\ul{Goal}: Approximate the unknown \ul{function} $y(t)$ over the interval $[t_0,T]$ such that $\begin{cases} y'(t) = f(t,y(t)) \\ y(t_0) = y_0 \end{cases}$ for $t_0 < t\leq T$ for some given function $f$.
\begin{example}
$\begin{cases} y'(t) = 4t \\ y(0)=1 \end{cases}$, so $y(t) = \int 4t\,dt = 2t^2+c$. $y(0)=1\implies 2(0)^2+c=1$ so $c=1$. \fbox{$y(t)=2t^2+1$}
\end{example}
\begin{example}
	$\begin{cases} y'(t) = y(t) \\ y(0)=2 \end{cases}$. $f(t,y)=y$.
	\ul{Guess}: $y(t)=e^t$ so $y'(t)=e^t=y(t) \checkmark$ but $y(0)=e^0 = 1\neq 2$.
	\ul{General solution}: $y(t) = ce^t = y'(t) \checkmark$ and $y(0)=2\implies \ul{2=c}$ \fbox{$y(t)=2e^t$}
\end{example}
\begin{example}
	$y'(t) = 3y(t)$, general solution: $y(t) = ce^{3t}$ and $y'(t)=3ce^{3t} = 3y(t)\checkmark$.
\end{example}

\subsubsection*{Approximation} $y'(t) = \frac{y(t+h)-y(t)}h + \frac12hy''(t_n)$ where $t_n$ between $t$ and $t+h$.
Plug into differential equation: $y'=f$
\begin{align*}
	\frac{y(t+h)-y(t)}h + \frac12hy''(t_n) = f(t,y(t))
\end{align*}
Solve for $y(t+h)$:
$$ y(t+h) = y(t) + hf(t,y(t)) - \ub{\mc O(h^2)}{\frac12h^2y''(t_n)}$$
\ul{Idea}: Given $y(t)$, can estimate $y(t+h)$ with $\mc O(h^2)$ accuracy,
given $y(t+h)$, can estimate $y(t+2h)$, etc ... $$t+Nh=T$$
\ul{Euler's Method}: $y_0 = y(t_0)$ [given], choose $h>0$, $t_j=t_0+jh$. For $n=0,1,2,\dots$ (stop at $t=T$). Then $$y_{n+1} = y_n+hf(t_n,y_n)$$

\begin{fact}
	Let $T=t_00+Nh$. Then $|y(T)-y_N|=\mc O(h)$. $h^2\mapsto h$ due to accumulation of error (without rounding).
\end{fact}
\begin{example}
	$y'+4y=t$, $y(0)=1$.
	Estimate $y(1)$ using $N=4$ steps, so $4=N=\frac{T-t_0}h = \frac1h\implies h=\frac14$.
	\begin{itemize}
		\item $t_0=0,t_1=\frac14,t_2=\frac12,t_3=\frac34,t_4=1=T$. code: $t_0+ih$, not $t=t+h$.
		\item $y_0=1$. $y' = t-4y \implies f(t,y) = t-4y$.
		\item $y_1 = y_0 + hf(t_0,y_0) = 1+\frac14[t_0+4y_0] = 1+\frac14[0-4\cdot1] = 0$.
		\item $y_2 = y_1 + \frac14[t_1-4y_1] = \frac1{16}$.
		\item $y_3 = \frac18$.
		\item $y_4 = \frac3{16}$.
	\end{itemize}
	$y(1) \approx y_4 = \frac3{16}$.
\end{example}

\subsection*{\S2.4 Linear Interpolation}

\ul{Goal}: Given data point $(x_k,y_k)$, where $x_i\neq x_j$ for $i\neq j$ and $y_k=f(x_k)$ for some function $f$, find a nice funciton $p$ such that
\begin{itemize}
	\item $p(x_k)=y_k=f(x_k)$ for all $k$
	\item $p(x) \approx f(x)$ otherwise.
\end{itemize}
\begin{enumerate}
	\item[$*$] Typically $f(x)$ unknown.
		Consider a function $f(x)$ whose values are known at $x_0<x_1<x_2<x_3<x_4$.

	\item[$*$] linear $\implies p(x)$ is piecewise linear. $p$ is linear on $[x_{k-1},x_k]$ for all $k$. $p(x) = p_k(x)$ if $x_{k-1}\leq x\leq x_k$.
\end{enumerate}

Let $x\in [x_{k-1},x_k]$. $p_k(x) = $ secant line connecting $f(x_{k-1})$ and $f(x_k)$.
Then $\ds m = \frac{f(x_k)-f(x_{k-1})}{x_k-x_{k-1}}$ and $y-f(x_{k-1}) = m (x-x_{k-1})$, so $\ds y = f(x_{k-1}) + \frac{f(x_k)-f(x_{k-1})}{x_k-x_{k-1}}(x-x_{k-1})$
(point slope form)
\begin{align*}
	\implies p_k(x) &= f(x_{k-1}) + \frac{f(x_k)-f(x_{k-1})}{x_k-x_{k-1}}(x-x_{k-1}) \\
					&= \frac{x_k-x}{x_k-x_{k-1}}f(x_{k-1}) + \frac{x-x_{k-1}}{x_k-x_{k-1}}f(x_t)
\end{align*}
for all $x\in[x_{k-1},x_k]$
(parameterized form).

\begin{theorem}
	Let $f\in C^2[a,b]$ and let $p(x)$ be the piecewise linear interpolant for the nodes $a=x_0<x_1<x_2<\dots<x_N=b$ for some $N>0$.
	Then for all $x\in[a,b]$,
	$$ |f(x)-p(x)| \leq \frac18(\max_k|x_k-x_{k-1}|)^2\max_{a\leq\zeta\leq b}|f''(\zeta)|. $$
\end{theorem}
\begin{note}:
	\begin{enumerate}
		\item If $f$ is linear, then $f''=0 \implies p(x)=f(x)$ for all $x$.
		\item The error only holds over $[a,b]$ where $x_0=a,x_N=b$.
	\end{enumerate}
\end{note}
\begin{theorem}
	[Linear Interpolation with $N=1$]
	Let $f\in C^2[x_0,x_1]$ and let $p_1(x)$ be the linear polynomial that interpolates $f$ at $x_0$ and $x_1$.
	Then, for all $x\in[x_0,x_1]$,
	$$|f(x)-p(x)|\leq\frac12|(x-x_0)(x-x_1)|\max_{x_0\leq\zeta\leq x_1}|f''(\zeta)|
		\leq \frac18 (x_1-x_0)^2 \max_{x_0\leq\zeta\leq x_1}|f''(\zeta)|. $$
\end{theorem}
\begin{proof}
	Let $E(x) = f(x)-p_1(x)$ (error at $x$).
	Define
	\begin{align*}
		\omega(x) &= (x-x_0)(x-x_1), \\
		G(x) &= E(x)-\frac{\omega(x)}{\omega(t)}\os{\text{aux. fun.}}{E(t)},
	\end{align*}
	where $x_0<t<x_1$.
	Then
	\begin{itemize}
		\item $G(x_0)=E(x_0)=0$
		\item $G(x_1)=E(x_1)=0$
		\item $G(t) = 0$.
	\end{itemize}
	Rolle's Theorem (MVT) $\implies$ there exists $\eta_0$ in $(x_0,t)$ and $\eta_1$ in $(t,x_1)$ such that $G'(\eta_0)=G'(\eta_1)=0$.
	Rolle's Theorem $\implies$ there exists $\xi_t$ in $(\eta_0,\eta_1)$ such that $G''(\xi_t)=0$.
	\begin{note}
		$G''(x) = f''(x) - \frac2{\omega(t)}E(t)$ and $p_1''(x)=0$ and $w''(x)=2$, $\omega$ is quadratic.
	\end{note}
	$G''(\xi_t)=0 \implies f''(\xi_t)-\frac2{\omega(t)}E(t)=0 \implies E(t) = \frac12\omega(t)\cdot f''(\xi_t) \implies f(t)-p(t) = \frac12(t-x_0)(t-x_1)f''(\xi_t)$.
	$*$ exact error for any $t$ in $(x_0,x_1)$.
	Thus $|E(x)| \leq \frac12 |\ub{\omega(x)\,\text{[quadratic]}\,x^2+\dots}{(x-x_0)(x-x_1)}|\cdot\ds\max_{x_0\leq\xi\leq x_1}|f''(\xi)|$. $\checkmark$
	
	Then $x_c = \frac{x_0+x_1}2$ and $|\omega(x)|$ is maximized over $[x_0,x_1]$ at the vertex $x_c$.
	So
	\begin{align*}
		|\omega(x_c)| &= \mlr{\omega\plr{\frac{x_0+x_1}2}}
					  = \mlr{\plr{\frac{x_0+x_1}2-x_0}\plr{\frac{x_0+x_1}2-x_1}} \\
					  &= \mlr{\frac{x_1-x_0}2\cdot\frac{x_0-x_1}2}
					  = \frac14|x_1-x_0|^2.
	\end{align*}
	implies $\ds |E(x)| \leq \frac12\cdot\frac14|x_1-x_0|^2\cdot\max_{x_0\leq\xi\leq x_1}|f''(\xi)|^2$.
\end{proof}

\end{document}
