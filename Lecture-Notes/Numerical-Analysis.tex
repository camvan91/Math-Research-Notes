\documentclass[]{article}
\input{../mathdoc}
\input{../mathsym}
\input{../theorem}
%\input{../preview}

\author{Presenter: Thomas Lewis, Notes by Michael Reed, Book: James Epperson 2nd}
\title{Numerical Methods and Analysis}
%date{}

\begin{document}
\maketitle

%\begin{abstract}
%\end{abstract}

%\subsection*{Introduction}

Two sources of error:
\begin{enumerate}
	\item Going from a continuous/infinite problem to a discrete/finite problem.
		\begin{example}
			$f'(a) = \lim_{h\ra0} \frac{f(a+h)-f(a)}{h}$, 
			
			Approximation: fix $h=0.01$, $f'(a)\approx \frac{f(a+0.01)-f(a)}{0.01}$. This is called \ul{truncation error}.
		\end{example}
	\item Rounding. Computers only represent finitely many numbers. (all rational)
		\begin{example}
			Area of a circle $ = \pi r^2 \approx 3.14159r^2$.
		\end{example}
\end{enumerate}

\ul{Focus in this class}
\begin{enumerate}
	\item Root finding: solve $f(x)=0$ for $x$.
	\item Interpolation. Given several function values, estimate others.
	\item Integration: $\int_a^b f(x)\,dx =\ ?$
	\item Applications: Differential Equations.
\end{enumerate}

\subsection*{Basic tools of Calculus}

\begin{theorem}
	[Mean Value Theorem (MVT)]
	Let $f$ be continuous on $[a,b]$ and differentiable on $(a,b)$. Then there exists $\xi$ in $(a,b)$ such that $f'(\xi)=\frac{f(b)-f(a)}{b-a}$.
\end{theorem}

\begin{example}
	Estimate $\cos(0.01)$.
	Know $\cos(0)=1$ and $\frac{d}{dx}[\cos x] = -\sin(x)$ with $-1\leq \sin(x) \leq 1$.
	MVT: $\exists \xi\in(0,0.01)$ such that $\frac{\cos(0.01)-\cos(0)}{0.01-0} = \cos'(\xi)=-\sin(\xi)$. This implies $\cos(0.01) = \cos(0)-0.01\sin(\xi)$.
	So $\cos(0)-0.01\leq \cos(0.01)\leq \cos(0)+0.01$ and $0.99\leq \cos(0.01)\leq 1\leq 1.01$. 
	
	Thus $\cos(0.01)\approx 0.995$ with $|\text{Error}|\leq 0.005$.
\end{example}

\begin{theorem}
	[Taylor's Remainder Theorem]
	Let $f(x)$ have $n+1$ continuous derivatives on $[a,b]$ for some $n\geq 0$, and let $x,x_0\in[a,b]$.
	Then, $f(x)=p_n(x)+R_n(x)$ for $$p_n(x) = \sum_{k=0}^n \frac{(x-x_0)^k}{k!}f^{(k)}(x_0)$$ and 
	$$R_n(x) = \frac{(x-x_0)^{n+1}}{(n+1)!} f^{(n+1)}(\xi_x)$$ 
	for some $\xi_x$ between $x$ and $x_0$, where $p_n(x)$ is an $n$th degree polynomial in $x$ and $R_n(x)$ is a remainder/error.
\end{theorem}

\begin{example}
	For $x_0=0$, 
	\begin{align*}
		e^x &= \sum_{k=0}^n \frac{x^k}{k!}\frac{d}{dx}[e^x]\bigg|_{x=0} + R_n(x) 
		= \sum_{k=0}^n \frac{x^k}{k!} e^x \bigg|_{x=0} + R_n(x) 
		= \sum_{k=0}^n \frac{x^k}{k!} + R_n(x) \\
		e^x &= 1+x+\frac{1}{2!}x^2 + \dots + \frac{1}{n!}x^n + R_n(x)
	\end{align*}
	where $R_n(x) = \frac{(n+1)!}x^{n+1}\cdot e^x$ for $\xi_x$ in $(0,x)$ or $(x,0)$.
\end{example}

\begin{example}
	Calculate $\cos(0.01)$ with an error less than $10^{-6}$.
	Let $\cos(x) = p_n(x)+R_n(x)$ with $x_0 = 0$ and $$p_n(x) = \sum_{k=0}^n \frac{(-1)^k}{(2k)!}x^{2k}.$$ %, $x_0=0$.
	So $\cos(x)-p_n(x) = R_n(x)$ and $|\cos(x)-p_n(x)|\leq |R_n(x)|$.
	If $|R_n(x)|\leq 10^{-6}$, then absolute error is also.
	\begin{align*}
		|R_n|&=\bigg|\frac{(-1)^{n+1}}{(2n+2)!}x^{2n+2}\cos(\xi_x)\bigg| \qquad \text{ for } x=0.01 \\
			 &\leq \frac{|(-1)^{n+1}|}{(2n+2)!}(0.01)^{2n+2}|\cos(\xi_x)| 
			 \leq \frac{1}{(2n+2)!}(0.01)^{2n+2}\cdot 1.
	\end{align*}
For $n=0$: $|R_0(0.01)|\leq 5\cdot 10^{-5}$.
For $n=1$: $|R_1(0.01)|\leq 4.2\times 10^{-10}$.
Use $n=1$, $$\cos(0.01)\approx p_1(0.01) = 1-\frac{1}{2!}(0.01)^2 = 0.99995.$$
\end{example}

\begin{recall}
	[Taylor's Theorem]
$\ds f(x) = \sum_{k=0}^n \us{p_n(x)}{\frac{(x-x_0)^k}{k!} f^{(k)}(x_0)} + \us{R_n(x)}{\frac{(x-x_0)^{n+1}}{(n+1)!} f^{(n+1)}(\xi_x)}$,
$\xi_x$ is between $x_0$ and $x$.
\end{recall}

\begin{example}
	Bound the error when using $p_3(x)$ to approximate $f(x) = \frac{1}{1-x}$ over the interval $[-\frac12,0]$.
\end{example}
\begin{fact}
	$\ds\frac{1}{1-x} = \sum_{k=0}^\infty x^k$ (geometric series), $p_3(x) = 1+x+x^2+x^3$ and $\ds R_3(x) = \frac1{4!} x^4 f^{(4)}(\xi_x)$ with $\xi_x$ between 0 and $x$, know $x$ is in $[-\frac12,0] \implies \xi_x$ is in $[-\frac12,0]$.
	So $f^{(4)}(x) = -\frac{24}{(x-1)^5}$.
\end{fact}
Then (absolute error at $\displaystyle x) = |f(x)-p_3(x)| = |R_3(x)| = \left|\frac1{4!} x^4 f^{(4)}(\xi_x)\right| = \frac1{24}|x^4|\cdot\left|-\frac{24}{(x-1)^5}\right|$.
	It follows ($x$ in $[-\frac12,0]$) $\displaystyle \implies |x^4|\leq \left|-\frac12\right|^4 = \left(\frac12\right)^4$
	and $\displaystyle\left|-\frac{24}{(\xi_x-1)^5}\right| = \frac{24}{|\xi_x-1|^5} \leq \frac{24}{\min|\xi_x-1|^5} = \frac{24}{|-\frac12-1|^5} = \frac{24}{(3/2)^5} = 24\cdot\left(\frac12\right)^5\cdot\left(\frac13\right)^5$.
	Therefore $\displaystyle|f(x)-p_3(x)| \leq \frac1{24}\cdot\left(\frac14\right)^4\cdot 24\cdot 2^5\cdot\left(\frac13\right)^5 = \frac2{3^5}\approx 0.008$.

\begin{theorem}
	[Intermediate Value Theorem]
	Let $f$ be continuous on $[a,b]$ and $w$ such that $f(a)\leq w\leq f(b)$ \ul{or} $f(b)\leq w\leq f(a)$.
	Then there exists $c$ in $[a,b]$ such that $f(c)=w$.
\end{theorem}
\begin{example}
	Show $f(x) = 2x+\cos(x)$ has only 1 zero.

	$f^{(n)}$ is continuous on $(-\infty,\infty)$ for all $n\geq 0$.
	\say{solve} $f(x)=0 \implies w=0$.
	$\lim_{x\ra-\infty} f(x) = -\infty$ and $\lim_{x\ra\infty} f(x) = +\infty$ $\implies$ there exist (at least 1) number $c$ such that $f(c) = 0$.
	Assume there are two numbers $c_1<c_2$ such that $f(c_1)=f(c_2)=0$.
	MVT: there exists $\xi$ in the interval $[c_1,c_2]$ such that $f'(\xi) = \frac{f(c_2)-f(c_1)}{c_2-c_1}$. 
	Thus $f'(\xi) = 2-\sin(x) \geq 2-1 = 1>0$ (contradiction) $\implies c_1=c_2$ (unique solution).
\end{example}

\subsection*{Error, Big Oh}

Exact Value: $A$.
Approximation: $A_n$ ($h$ is a param. fixed in the approx. method. $h\approx 0$, $n$ param. $n\ra\infty$)

\begin{example}
	$A = f'(x)$.
	$A_n = \frac{f(x+h)-f(x)}{h}$, $h$ small.
\end{example}
\begin{example}
	$A = \sum_{k=0}^\infty a_k$.
	$A_n = \sum_{k=0}^n a_k$.
\end{example}

\begin{itemize}
	\item Error $ = A-A_n$
	\item Absolute Error $= |A-A_n|$
	\item Relative Error $= \frac{|A-A_n|}{|A|}$.
\end{itemize}

\begin{example}
	$A = 10^6$. $A_n = 10^6+1$.
	Error $=-1$.
	Absolute Error $=1$.
	Relative Error $=\frac1{10^6} = 10^{-6}$.
\end{example}
\begin{example}
	$A = 10^{-6}$. $A_n = 10^{-7}$.
	Absolute Error $=\frac9{10^7}$.
	Relative Error $=\frac9{10}$.
\end{example}

\newpage

Last time:
Given $A$ and approximations $A_h$ or $A_n$, defined error and absolute and relative error.
Convergence means $\lim_{h\ra0} A_n=A$ (implies $\lim_{h\ra0}\text{error}=0$) and $\lim_{n\ra\infty} A_n = A$ (implies $\lim_{n\ra\infty}\text{error}=0$).
Often we have 2 methods, both convergent, and we want to compare \say{rate or order} of convergence.

A good example to keep in mind: 
Approximate $A = \int_1^3 e^{x^2}\,dx$.
\ul{Method 1}: Use rectangles of width $h = \frac{3-1}{n} = \frac2n$ to get Riemann sums $A_h^1$ or $A_n^1$.
\ul{Method 2}: Use trapezoids of width $h$ to get sums $A_h^2$ or $A_n^2$.

\begin{definition}
	$A = A_h + \mc O(\beta(h))$ if there exists $C>0$, independent of $h$ such that $|A-A_h|\leq C\beta(h)$.
\end{definition}
\begin{note}
	If $\lim_{h\ra0}\beta(h)=0$, then $A_h\ra A$.
\end{note}
\begin{definition}
	$A = A_n + \mc O(\beta(n))$ if there exists $C>0$, independent of $n$, such that $|A-A_n|\leq C\beta(n)$.
\end{definition}
Sometimes consider $a_n$ which is \# flops or amount of memory.

\subsection*{Comparison of methods}

Suppose method 1 has $A^1\sim\mc O(\beta_1(h))$ and method 2 has $A^2\sim\mc O(\beta_2(h))$.
\begin{itemize}
	\item If $\displaystyle\lim_{h\ra0} \frac{\beta_1(h)}{\beta_2(h)}=0$, then method 1 converges faster.
	\item If $\displaystyle\lim_{h\ra0} \frac{\beta_1(h)}{\beta_2(h)}=M\in(0,\infty)$, then methods have same rate.
	\item If $\displaystyle\lim_{h\ra0} \frac{\beta_1(h)}{\beta_2(h)}=+\infty$, then method 2 converges faster.
\end{itemize}
\begin{example}
	Suppose $f$ is smooth on $[a,b]$, and fix $x\in(a,b)$.
	Set $A=f'(x)$, and $A_h = \frac{f(x+h)-f(x)}{h}$.
	Show $A=A_h + \mc O(h)$.
	By Taylor's Theorem, $f(x+h) = f(x) + hf'(x) + \frac12h^2f''(\xi)$, where $\xi$ is between $x$ and $x+h$.
	So $f'(x) = \frac{f(x+h)-f(x)}{h} -\frac12hf''(\xi)$.
	Thus $A = A_h - \frac12hf''(\xi)$. Set $C = \frac12\displaystyle\max_{a\leq\xi\leq b} |f''(\xi)|$.
	Then $|A-A_h|\leq Ch$.
\end{example}
\begin{example}
	Suppose $f$ smooth on $[a,b]$, and fix $x\in(a,b)$.
	$A = f'(x)$, $A_h = \frac{f(x+h)-f(x-h)}{2h}$.
	Show $A = A_h + \mc O(h^2)$. By Taylor's Theorem: $f(x+h) = f(x) + hf'(x) + \frac12h^2f''(x) + \frac16h^2f'''(\xi_1)$, where $\xi_1$ is between $x$ and $x+h$.
	Therefore $f(x-h) = f(x) - hf'(x) + \frac12h^2f''(x) - \frac16h^3f'''(\xi_2)$, where $\xi_2$ is between $x-h$ and $x$.
	So $$ A_h = \frac{2hf'(x) + \frac16h^3[f'''(\xi_1)+f'''(\xi_2)]}{2h} = f'(x) + \frac1{12}h^2[f'''(\xi_1)+f'''(\xi_2)].$$
	Let $C = \frac1{12}\displaystyle\max_{a\leq\xi\leq b}|f'''(\xi)|$.
	So $\displaystyle|A-A_h| \leq \frac1{12}h^2(C+C) = \frac16Ch^2$.
\end{example}

\subsection*{Computer Arithmetic}

Computers can only represent a finite amount of numbers using a fixed, finite number of digits.

\begin{definition}
	[Floating point numbers]
	$x = \sigma\times f\times\beta^{t-p}$, where $\sigma = \text{sgn}$ $(+,-)$, $f=$ fraction $ = .d_1d_2\dots d_m$, $\beta = $ base (typically $d \implies d_i\in{0,1}$ for all $i$), $t-p = $ exponent where $t$ variable, $p$ fixed (called the shift).
	
	\ul{Storage}: $x = \us{1\text{ bit}}{\sigma}\mid\us{N\text{ bits}}{t}\mid \us{M\text{ bits}}{f}$, so $M+N+1$ bits.
\end{definition}

\begin{example}
	$\beta = 2$, $p = 15$, $N=5$, $M=8$. $x = \pm 0.d_1d_2\dots d_8\times 2^{t-15}$.
	So $0\leq t\leq 11111_2 = 2^0+2^1+2^2+2^3+2^4 = 31$. So $-15\leq t-p\leq 16$ nearly equal range of positive and negative exponents.
	\begin{enumerate}
		\item $x = -0.25 = -[2^{-2}] = 0.01_2$. So $t-p = -1$ and $t=-1+5 = 14 = 2^3+2^2 + 2^1 = 1110_2$. Thus $x = \text{neg}\mid 01110\mid 10000000$
		\item $x = 50 = 2^5+2^4+2^1 = 110010._2$. So $t-p=6$ implies $t=6+15=21 = 2^4 + 2^2 + 2^0 = 10101_2$. Thus $t = \text{pos}\mid 10101\mid 11001000$.
	\end{enumerate}
\end{example}

\begin{note}
	[Biggest Number] all 1's: $+.\ub{M}{11\dots1}\times 2^{\ob{N}{11\dots1}-p}$
\end{note}
\begin{note}
	[Normailzed] $d_1 = \beta-1$.
\end{note}
\begin{note}
	[Overflow] exponent too large.
\end{note}
\begin{note}
	[Underflow] exponent too small.
\end{note}
\begin{definition}
	[Rounding Error] rounding a real number to a corresponding floating point number.
\end{definition}
2 methods: rounding and chopping (round down).
\begin{example}
	$\frac16+\frac1{10}$ base 10 with 3 digit arithmetic.
	\begin{enumerate}
		\item Exact: $\frac16+\frac1{10} = \frac{10+6}{60} = \frac4{15} = 0.2\conj6$
		\item Chopping: $\frac16 = 0.1\conj6\mapsto 0.166$, $\frac1{10} = 0.1$, add $= 0.266$, 
			
			relative error $\ds= \frac{|0.2\conj6-0.266|}{|0.2\conj6|} = \frac{|\frac4{15} - \frac{266}{1000}|}{4/15} = 0.0025$.
		\item Rounding: 0.666\ul{\ul{6}}, $\frac16\mapsto 0.167$, add $= 0.267$, 
			
			relative error $\ds= \frac{|0.2\conj6-0.267|}{0.2\conj6} = 0.00125$.
	\end{enumerate}
\end{example}

\begin{note}
	:
	\begin{enumerate}
		\item If changing the precision (single to double) drastically changes the output, then rounding errors are having a large effect.
		\item The order in which operations are performed makes a difference $(a+b)+c\neq a+(b+c)$ for some $a,b,c$.
		\item Subtraction can lead to a major loss of accuracy if the two numbers are nearly equal.
	\end{enumerate}
	$e^x = p(x) + R_x$ or $e^{-x} = $ alternating sign in polynomial or $= \frac1{e^x}$.
\end{note}

\ul{Goal}: Bound the relative error when using floating point arithmetic.
$a\circledast b \mapsto fl[rd(e)*rd(b)]$

\begin{definition}
	Machine epsilon is defined by $$\epsilon_{mach} = \max\setm{x\in\text{Computer \#s}}{1+x = 1\text{ in computer arithmetic}}.$$
\end{definition}
\begin{note}
	$\epsilon_{mach}$ is  not the smallest number in magnitude.
\end{note}
\begin{fact}
	$\ds\left|\frac{rd(x)-x}{x}\right| = \mc O(\epsilon_{mach}) \leq c\epsilon_{mach}$, $x$ is a real number, $rd(x)$ is $fl$.
\end{fact}

\begin{theorem}
	[Computer Arithmetic Error] Let $* = +,-,\times,\div$. Then, $$\ds\frac{|x*y-fl(x*y)|}{|x*y|} = \mc O(\epsilon_{mach}).$$
	\begin{enumerate}
		\item[$*$] single arithmetic calculations have bounded relative error.
	\end{enumerate}
\end{theorem}

\subsection*{\S1.5}

\ul{Goal}: Approximate $e^x$ and integrals involving $e^x$ using simple functions.

\ul{Book}: $\ds f(x) = \frac2{\sqrt\pi}\int_0^x e^{-t^2}\,dt\equiv\text{erf}(x)$.

\begin{example}
	$f(x) = \int_0^x t^2e^{-t^2}\,dt$. Approximate over $(0,2)$. Bound Error.
	
	Strategy: $t^2e^{-t^2}\approx p(t)$. Integrate $p(t)$ exactly.
	\begin{align*}
		e^t &= p_k(t) + R_k(t) 
			= 1+t+\frac12t^2+\dots+\frac1{k!}t^k \us{c_t\text{ between 0 and }t}{+ \frac{t^{k+1}}{(k+1)!}e^{c_t}} \\
		\implies t^2e^{-t^2} &= t^2 \bigg[ 1+(-t^2) + \frac12(-t^2)^2 + \dots + \frac1{k!}(-t^2)^k + \us{c_{-t^2}\text{ between }-t^2\text{ and }0}{\frac{(-t^2)^{k+1}}{(k+1)!} e^{c_{(-t^2)}}} \bigg] \\
			&= \ub{q_k(t)}{\sum_{i=0}^k(-1)^k\cdot\frac{t^{2k+2}}{k!}} + \ub{r_k(t)}{\frac{(-1)^{k+1}t^{2k+4}}{(k+1)!}e^c} \\
		f(x) &= \int_0^x t^2e^{-t^2}\,dt 
			 = \int_0^x \blr{q_k(t) + r_k(t)}\,dt 
			 = \sum_{i=0}^k(-1)^k\cdot \frac{t^{2k+3}}{(2k+3)k!}\bigg|_0^x + \int_0^x r_k(t)\,dt \\
			 &= \sum_{i=0}^k(-1)^k\frac{x^{2k+3}}{(2k+3)k!} + \ub{E_k(x)}{\int_0^x r_k(t) \, dt}.
	\end{align*}
	Bound $|E_k(x)|$: 
	Let $x$ between 0 and 2, $t$ between 0 and $x$, $c$ between 0 and $-t^2 \implies c\in[-x^2,0]$ with
	$$E_k(x) = \frac{(-1)^{k+1}}{(k+1)!} \int_0^x \ub{>0\text{ for all }t>0}{t^{2k+4}e^c}\, dt.$$
	$*$ Naive bound: $\int_0^x t^{2k+4}e^c\,dt \leq \max |t^{2k+4}|\cdot\max|e^c|\cdot|x-0| = (2^{2k+4})(e^0)(2)$.
	So $$|E_k(t)| \leq \frac{2\cdot2^{2k+4}}{(k+1)!}\ra0$$ as $k\ra\infty.$
	\begin{recall}
		$\int_a^b f(x)\, dx \leq \max|f(x)|\cdot(b-a)$.
	\end{recall}
	MVT, $\xi_x\in(-x^2,0)$, 
	\begin{align*}
		E_k(x) &= \frac{(-1)^{k+1}}{(k+1)!}e^{\xi_x}\int_0^x t^{2k+4}\,dt 
			   = \frac{(-1)^{k+1}}{(k+1)!}e^{\xi_x}\cdot\frac{t^{2k+5}}{2k+5}\bigg|_0^x 
			   = \ub{\delta_k(x)}{\frac{(-1)^{k+1}}{(2k+5)(k+1)!}x^{2k+5}}\ub{\us{\leq M}{\mid}}{e^{\xi_x}},
	\end{align*}
	$M$ based on original problem.
	For $x$ fixed; $\lim_{k\ra\infty}\delta_k(x)=0$.
	So 
	\begin{align*}
		|E_k(x)| &= \frac1{(k+1)!(2k+5)}\cdot|x|^{2k+5}\cdot e^{\xi_x} 
				 \leq \frac1{(k+1)!(2k+5)}|x|^{2k+5}M
	\end{align*}
	where $M=e^0=1$ because $\xi_x\in(-x^2,0) \implies \xi_x\in(-4,0)$ since $x<2$.
	For $x$ in $(0,2)$: $|x|^{2k+5} \leq 2^{2k+5}$.
	Thus $$|E_k(x)|\leq \frac{2^{2k+5}}{\ub{\text{fairly precise}}{(k+1)!(2k+5)}} \leq \frac{2^54^k}{7(k+1)!} = \frac{32}{7}\cdot\frac{4^k}{(k+1)!}\ (\forall k\geq1).$$
	$\therefore \forall x\in(0,2)$:
	\begin{align*}
		f(x) &= \int_0^xt^2e^{-t^2}\,dt = \sum_{i=0}^k\frac{(-1)^k}{k!(2k+3)}x^{2k+3} + \mc O\plr{\frac{4^k}{(k+1)!}} \\
		k = 10 &: |E_k(x)| \leq \frac{32}{7}\cdot \frac{4^{10}}{11!}\leq 0.12009 \\
		k = 20 &: |E_k(x)| \leq 0.84\times 10^{-8}
	\end{align*}
\end{example}

\begin{example}
	Find the order of accuracy when approximating $e^x$ with the rational function
	$$r(x) = \frac{1+\frac12x}{1-\frac12x}$$ for $-1\leq x\leq 0$. So $|e^x-r(x)|\leq$ ?
	Then
	\begin{align*}
		e^x-r(x) &= 1+x+\frac12x^2+\frac16x^3+\dots+\frac1{k!}x^k+\dots -\frac{1+\frac12x}{1-\frac12x} \\
				 &= \frac1{1-\frac12x}\blr{\plr{1-\frac12x}\plr{1+x+\frac12x^2+\frac16x^3 + \mc O(x^4)} - \plr{1+\frac12x}} \\
				 &= \frac1{1-\frac12x}\blr{1+x+\frac12x^2+\frac16x^3+\mc O(x^4) - \frac12x^2 - \frac14x^3 - 1 - \frac12x} \\
				 &= \ub{\text{Exact Error}}{\frac1{1-\frac12x}\bigg[\us{-\frac1{12}}{\plr{\frac16-\frac14}}x^3+\mc O(x^4)\bigg]}
	\end{align*}
	For $x\leq0$: $\frac1{1-\frac12x}\leq 1 \implies |e^x-r(x)| \leq \frac1{12}|x|^3 + \mc O(x^4) \implies \ul{e^x = r(x) + \mc O(|x|^3)}$,
	($x^4\leq|x|^3$ for $-1\leq x\leq0)$.

	\ul{? $x<-1$ ?}
	Then $e^x = 1+x+\frac12x^2+\os{\downarrow\text{won't cancel.}}{\frac16x^3e^\xi}$ for $\xi$ between 0 and $x$.
\end{example}

\subsection*{\S2.1 Horner's Rule}

\ul{Goal}: Compute polynomials efficinetly and more accurately.

\begin{align*}
	p(x) &= a_0+a_1x+a_2x^2 + \dots + a_nx^n \qquad \text{(Standard/expanded form)} \\
		 &= a_0 + x(a_1 + a_2x+\dots + a_nx^{n-1}) \\
		 &= a_0 + x\blr{a_1+x(a_2+\dots+a_nx^{n-2})} \\
		 & \qquad \vdots \\
	\text{nested form -- } &= a_0 + x(a_1 + x(a_2 + \dots + x(a_{n-1}+a_nx)\dots)).
\end{align*}
\begin{note}:
	\begin{enumerate}
		\item nested form $n+1$ multiplications, $n$ additions.
			
			standard - same + cost of computing $x^2,x^3,\dots,x^n$.
		\item typically more accurate.
		\item Most accurate and efficient is fully factored form:
			$p(x) = a_n(x-c_1)^{m_1}(x-c_2)^{m_2}\dots(x-c_k)^{m_k}$
			where $m_1+m_2+\dots+m_k = n$.
			Not known in general.
	\end{enumerate}
\end{note}

\begin{algorithm}
	\caption{Horner's Method}
	\begin{algorithmic}
		\Require $a_0,a-1,\dots,a_n$ and $x$
		\State $px\gets a_n$
		\For{$k=n-1$ to 0}
		\State $px\gets a_k + px\cdot x$
		\EndFor
		\State\Return $px = p(x)$
		\Comment{a slight modification outputs $p'(x)$.}
	\end{algorithmic}
\end{algorithm}
%Given $a_0,a_1,\dots,a_n$ and $x$.
%$px = a_n$
%for $k=n-1$ to 0
%	$px = a_k + px\cdot x$
%end
%output $px = p(x)$.
%$*$ A slight modification outputs $p'(x)$.


\begin{example}
		$p(x) = 1-x+\frac12x^2-\frac14x^5 
			 = 1+x(-1+\frac12x-\frac14x^4) 
			 = 1+x(-1+x(\frac12-\frac14x^3)) $.
\end{example}
\begin{example}
		$p(x) = 1+x^2 + \frac12x^4 
			 = 1+x^2(1+\frac12x^2)$.
\end{example}

\subsection*{\S1.6 Approximating natural log}

\begin{align*}
	x &= f\times \beta^{t-p} \\
	\ln(x) &= \ln(f\times\beta^{t-p})
		   = \ln(f) + (t-p)\ln(\beta)
\end{align*}
For $\beta=2$: $\frac12\leq f\leq 1$.
Assume $\ln(2)$ is known to high precision.
We only need to approximate $\ln(f)$ for $\frac12\leq f\leq 1$ to high precision for any $x\in(0,\infty)$.
This implies only need a Taylor's series on a small interval, which implies we can bound error uniformly.

\ul{Method in book}: $f_0 = \frac34$, $|R_n(z)|\leq\frac12\plr{\frac13}^n$ for all $\frac12\leq z\leq 1$.
Set $n=33$ in $p_n(z) \implies$ Error $\leq10^{-16}$.

\ul{Goal}: use $\log$ properties to improve the convergence rate. (based on problem 1.6.6)

\begin{enumerate}
	\item Find $w$ such that $\frac{1-w}{1+w}=z$ for $\frac12\leq z\leq 1$.
		This implies $w = \frac{1-z}{1+z}$ with $0\leq w\leq\frac13$ (shrunk domain size).
	\item $\ln(z) = \ln\plr{\frac{1-w}{1+w}} = \ln(1-w)-\ln(1+w)$.
	\item Taylor's expansion for $\ln(1-w),\ln(1+w)$ centered at $w_0=\frac16$ (midpoint for $w$).
\end{enumerate}
Integral form: 
%\begin{align*}
	$f(x) = p_n(x)+R_n(\xi_x)$, 
	$\ds p_n(x)	 = \sum_{k=0}^n \frac{(x-x_0)^k}{k!}f^{(k)}(x_0)$,
	$\ds R_n(\xi_x) = \int_{x_0}^x\frac{(x-t)^n}{n!}f^{(n+1)}(t)\,dt$.
%\end{align*}
Then
%\begin{align*}
	$y = \ln(x)$, 
	$y' = \frac1x = x\inv$, 
	$y'' = -x^{-2}$, 
	$y''' = 2x^{-3}$,
	$y^{(4)} = -6x^{-4}$,
	$\implies y^{(k)} = (-1)^{k-1}(k-1)! x^{-k}$,
%\end{align*}
with $k\geq 1$.
\begin{align*}
	\ln(1+w) = \ln(1+w_0) + \frac{w-w_0}{1+w_0} & + \sum_{k=2}^n \frac{(w-w_0)^k}{k!} (-1)^k (k+1)! (1+w_0)^{-k} \\
			  & + \int_{w_0}^w (w-t)^n\cdot(-1)^n(n+1-1)!(1+t)^{-n-1}\,dt \\
			 = \ln(1+w_0) + \frac{w-w_0}{1+w_0} & + \sum_{k=2}^n \frac{(-1)^{k-1}(w-w_0)^k}{k(1+w_0)^k} + \int_{w_0}^w (-1)^n(w-t)^n(1-t)^{-n-1}\,dt \\
	\ln(1-w) = \ln(1-w_0) - \frac{w-w_0}{1-w_0} & + \sum_{k=2}^n \frac{(-1)^k}k \frac{(w-w_0)^k}{(1-w_0)^k} + \int_{w_0}^w (-1)^{n+1}(w-t)^n(1-t)^{-n-1}\,dt
\end{align*}
\begin{enumerate}
	\item[4.] Bound $R_n(w)$ over $[0,\frac13]$.
		Note that $\ds\mlr{\int_a^b f(x)\,dx} \leq |b-a|\cdot\max_{a\leq x\leq b}|f(x)|$.
		So
		\begin{align*}
			\mlr{\int_{w_0}^w (-1)^n(w-t)^n(1+t)^{-n-1}\,dt}
				&\leq |w-w_0|\cdot\max_t\mlr{(-1)^n(w-t)^n(1+t)^{-n-1}} \\
				&\leq \frac16\cdot\max_t\mlr{\frac{(w-t)^n}{(1+t)^{n+1}}}
				\leq \frac16\cdot\max_t\mlr{\frac1{1+t}}\cdot\us{\us{\implies\text{ max at a point}}{\text{no critical points on }[0,\frac13]}}{\max_t\mlr{\frac{w-t}{1+t}}^n} \\
				&\leq \frac16 \cdot 1\cdot\max\set{\mlr{\frac{w-w_0}{1+w}}^n,\mlr{\frac{w-w_0}{1+w_0}}^n}
				= \frac16\cdot\max\mlr{\frac{w-w_0}{1+w_0}}^n \\
				&\leq \frac16\cdot\frac{(1/3)^n}{(2/6)^n} 
				= \frac16\cdot\plr{\frac17}^n
		\end{align*}
		$0\leq t\leq\frac13$ (lazy)
		\begin{align*}
			\mlr{\int_{w_0}^w (-1)^{n+1}(w-t)^n(1-t)^{-n-1}\,dt} 
			&\leq \frac16\cdot\max_t\mlr{\frac1{1-t}}\cdot\max_t\mlr{\frac{w-t}{1-t}}^n \\
			&\leq \frac16\cdot\frac32\cdot\mlr{\frac{1/6}{5/6}}^n
			= \frac14\plr{\frac15}^n.
		\end{align*}
		Combining:
		Total Error $\leq \frac16\plr{\frac17}^n + \us{\text{bigger}}{\frac14\plr{\frac15}^n} \leq \frac12\plr{\frac15}^n$.
		
		If error $\leq 10^{-16} \implies \frac12\plr{\frac15}^n\leq 10^{-16} \implies n\geq \log_2(\frac12\cdot10^{-16}) \approx 22.46$.
		
		Given $z$ in $[\frac12,1]$, $n\geq 23$ will give $10^{-16}$ accuracy for
		\begin{align*}
			\ln(z) &= \ln(1-w)-\ln(1+w) 
				   \approx p_n(1-w)-q_n(1+w) \qquad \text{for} \\
			p_n(x) &= \ln\plr{\frac56} + \sum_{k=1}^n \frac{(-1)^k}k\plr{\frac{x-\frac16}{5/6}}^k \\
			q_n(x) &= \ln\plr{\frac76} + \sum_{k=1}^n\frac{(-1)^{k-1}}k\plr{\frac{x-\frac16}{\frac76}}^k
		\end{align*}
		with $w = \frac{1-z}{1+z}$, need $\ln(2),\ln(\frac56),\ln(\frac76)$ to high precision.
\end{enumerate}

\subsection*{\S2.2 Difference Approximation of Derivatives}

$$ f'(x) = \lim_{h\ra0} \frac{f(x+h)-f(x)}h.$$
Forward difference approximation ($h\ra0^+$)
$$ D_h^+f(x) = \frac{f(x+h)-f(x)}h = f'(x) + \mc O(h)$$
Backward difference approximation ($h\ra0^-$)
$$ D_h^- f(x) = \frac{f(x)-f(x-h)}h = f'(x) + \mc O(h)$$
Central difference approximation
$$D_hf(x) = \frac{f(x+h)-f(x-h)}{2h} = f'(x) + \mc O(h^2) = \frac12 \blr{D_h^+f(x) + D_h^-f(x)} $$
\begin{note}:
	\begin{enumerate}
		\item Prone to cancellation error due to subtraction in the numerator, after threshold roundoff dominates.
		\item Using more nodes can increase the accuracy.
	\end{enumerate}
\end{note}
\begin{example}
	$\ds f'(x) = \frac{8f(x+h)-8f(x-h)-f(x+2h) + f(x-2h)}{12h} + \mc O(h^4)$.
\end{example}

\subsubsection*{Calculating Rates of Convergence}

Suppose $E = \mc (h^p)$ for some $p$. This implies $E\approx Ch^p$ for some constant $C$ when $h\ra0$.
Pick $h_1,h_2>0$ with $h_1\neq h_2$.
So $E_1\approx Ch_1^p$ and $E_2\approx Ch_2^p$ imply
\begin{align*}
	\frac{E_1}{E_2} &\approx \plr{\frac{h_1}{h_2}}^p, \\ %&\quad
	\log\plr{\frac{E_1}{E_2}} &\approx p\log\plr{\frac{h_1}{h_2}}, \\%&\quad
	p &\approx \frac{\log(E_1/E_2)}{\log(h_1/h_2)}.
\end{align*}
Approximating for several $h_i$ values lets us estimate $p$.
\ul{Book}: $h_1=2h_2 \implies \frac{E_1}{E_2} \approx 2^p$.
If $p=1$: $\frac{E_1}{E_2} \approx 2^1 = 2$ (error halves when $h\ra h/2$)
$p=2$: $\frac{E_1}{E_2} \approx 2^2 = 4$ (error $\ra$ error/4 as $h\ra h/2$)

\subsubsection*{Second derivatives}

Centered difference approximation $$\ds f''(x) = \frac{f(x+2h)-2f(x)+f(x-h)}{h^2} + \mc O(h^2) \equiv D_h^2 f(x) = D_h^- D_h^+ f(x) = D_h^+ D_h^- f(x).$$

\begin{proof}
	Observe, 
\begin{align*}
	D_h^+ D_h^- f(x) &= D_h^+ \blr{ \frac{f(x)-f(x+h)}h }
					 = \frac1h\blr{D_h^+f(x)-D_h^+f(x-h)} \\
					 &= \frac1h\blr{\frac{f(x+h)-f(x)}h-\frac{f(x-h+h)-f(x-h)}h} \\
					 &= \frac1{h^2}\blr{f(x+h)-f(x)-f(x)+f(x+h)}
					 = D_h^2f(x)
\end{align*}
\end{proof}
\begin{proof}
	[Accuracy] Goal: $f''(x) \la$ everything at $x$.
	\begin{align*}
		D_h^2f(x) &= \frac{f(x+h)-2f(x)+f(x-h)}{h^2} \\
				  &= \frac1{h^2} \bigg[ f(x) + hf'(x) + \frac12h^2f''(x) + \frac16h^3f''(x) + \frac1{24}h^4f^{(4)}(x) + \mc O(h^5) \\
				  &\quad - 2f(x) + f(x) - hf'(x) + \frac12h^2f''(x)-\frac16h^3f'''(x) + \frac1{24}h^4f^{(4)}(x) \bigg] \\
				  &= \frac1{h^2}\blr{h^2f''(x) + \frac1{12}h^4f^{(4)}(x)+\mc O(h^5)}
				  = f''(x) + \ub{\mc O(h^2)}{\frac1{12}h^2f^{(4)}(x)} + \mc O(h^5)
	\end{align*}
\end{proof}
\begin{note}:
	\begin{align*}
		\ds D_h D_hf(x) &= f''(x) + \mc O\plr{(2h)^2} = \frac{f(x+2h)-2f(x)+f(x-2h)}{4h^2}, \\
		%$D_h^2 f(x)$:	
		D_h^2f(x) &= f''(x) + \us{\uparrow f^{(4)}(x)}{\mc O(h^2)}.
	\end{align*}
	Error = 0 if $f(x)$ is cubic.
\end{note}

\subsection*{\S2.3 Application: Euler's Methods for Initial Value Problems}

\ul{Goal}: Approximate the unknown \ul{function} $y(t)$ over the interval $[t_0,T]$ such that $\begin{cases} y'(t) = f(t,y(t)) \\ y(t_0) = y_0 \end{cases}$ for $t_0 < t\leq T$ for some given function $f$.
\begin{example}
$\begin{cases} y'(t) = 4t \\ y(0)=1 \end{cases}$, so $y(t) = \int 4t\,dt = 2t^2+c$. $y(0)=1\implies 2(0)^2+c=1$ so $c=1$. \fbox{$y(t)=2t^2+1$}
\end{example}
\begin{example}
	$\begin{cases} y'(t) = y(t) \\ y(0)=2 \end{cases}$. $f(t,y)=y$.
	\ul{Guess}: $y(t)=e^t$ so $y'(t)=e^t=y(t) \checkmark$ but $y(0)=e^0 = 1\neq 2$.
	\ul{General solution}: $y(t) = ce^t = y'(t) \checkmark$ and $y(0)=2\implies \ul{2=c}$ \fbox{$y(t)=2e^t$}
\end{example}
\begin{example}
	$y'(t) = 3y(t)$, general solution: $y(t) = ce^{3t}$ and $y'(t)=3ce^{3t} = 3y(t)\checkmark$.
\end{example}

\subsubsection*{Approximation} $y'(t) = \frac{y(t+h)-y(t)}h + \frac12hy''(t_n)$ where $t_n$ between $t$ and $t+h$.
Plug into differential equation: $y'=f$
\begin{align*}
	\frac{y(t+h)-y(t)}h + \frac12hy''(t_n) = f(t,y(t))
\end{align*}
Solve for $y(t+h)$:
$$ y(t+h) = y(t) + hf(t,y(t)) - \ub{\mc O(h^2)}{\frac12h^2y''(t_n)}$$
\ul{Idea}: Given $y(t)$, can estimate $y(t+h)$ with $\mc O(h^2)$ accuracy,
given $y(t+h)$, can estimate $y(t+2h)$, etc ... $$t+Nh=T$$
\ul{Euler's Method}: $y_0 = y(t_0)$ [given], choose $h>0$, $t_j=t_0+jh$. For $n=0,1,2,\dots$ (stop at $t=T$). Then $$y_{n+1} = y_n+hf(t_n,y_n)$$

\begin{fact}
	Let $T=t_00+Nh$. Then $|y(T)-y_N|=\mc O(h)$. $h^2\mapsto h$ due to accumulation of error (without rounding).
\end{fact}
\begin{example}
	$y'+4y=t$, $y(0)=1$.
	Estimate $y(1)$ using $N=4$ steps, so $4=N=\frac{T-t_0}h = \frac1h\implies h=\frac14$.
	\begin{itemize}
		\item $t_0=0,t_1=\frac14,t_2=\frac12,t_3=\frac34,t_4=1=T$. code: $t_0+ih$, not $t=t+h$.
		\item $y_0=1$. $y' = t-4y \implies f(t,y) = t-4y$.
		\item $y_1 = y_0 + hf(t_0,y_0) = 1+\frac14[t_0+4y_0] = 1+\frac14[0-4\cdot1] = 0$.
		\item $y_2 = y_1 + \frac14[t_1-4y_1] = \frac1{16}$.
		\item $y_3 = \frac18$.
		\item $y_4 = \frac3{16}$.
	\end{itemize}
	$y(1) \approx y_4 = \frac3{16}$.
\end{example}

\subsection*{\S2.4 Linear Interpolation}

\ul{Goal}: Given data point $(x_k,y_k)$, where $x_i\neq x_j$ for $i\neq j$ and $y_k=f(x_k)$ for some function $f$, find a nice funciton $p$ such that
\begin{itemize}
	\item $p(x_k)=y_k=f(x_k)$ for all $k$
	\item $p(x) \approx f(x)$ otherwise.
\end{itemize}
\begin{enumerate}
	\item[$*$] Typically $f(x)$ unknown.
		Consider a function $f(x)$ whose values are known at $x_0<x_1<x_2<x_3<x_4$.

	\item[$*$] linear $\implies p(x)$ is piecewise linear. $p$ is linear on $[x_{k-1},x_k]$ for all $k$. $p(x) = p_k(x)$ if $x_{k-1}\leq x\leq x_k$.
\end{enumerate}

Let $x\in [x_{k-1},x_k]$. $p_k(x) = $ secant line connecting $f(x_{k-1})$ and $f(x_k)$.
Then $\ds m = \frac{f(x_k)-f(x_{k-1})}{x_k-x_{k-1}}$ and $y-f(x_{k-1}) = m (x-x_{k-1})$, so $\ds y = f(x_{k-1}) + \frac{f(x_k)-f(x_{k-1})}{x_k-x_{k-1}}(x-x_{k-1})$
(point slope form)
\begin{align*}
	\implies p_k(x) &= f(x_{k-1}) + \frac{f(x_k)-f(x_{k-1})}{x_k-x_{k-1}}(x-x_{k-1}) \\
					&= \frac{x_k-x}{x_k-x_{k-1}}f(x_{k-1}) + \frac{x-x_{k-1}}{x_k-x_{k-1}}f(x_t)
\end{align*}
for all $x\in[x_{k-1},x_k]$
(parameterized form).

\begin{theorem}
	Let $f\in C^2[a,b]$ and let $p(x)$ be the piecewise linear interpolant for the nodes $a=x_0<x_1<x_2<\dots<x_N=b$ for some $N>0$.
	Then for all $x\in[a,b]$,
	$$ |f(x)-p(x)| \leq \frac18(\max_k|x_k-x_{k-1}|)^2\max_{a\leq\zeta\leq b}|f''(\zeta)|. $$
\end{theorem}
\begin{note}:
	\begin{enumerate}
		\item If $f$ is linear, then $f''=0 \implies p(x)=f(x)$ for all $x$.
		\item The error only holds over $[a,b]$ where $x_0=a,x_N=b$.
	\end{enumerate}
\end{note}
\begin{theorem}
	[Linear Interpolation with $N=1$]
	Let $f\in C^2[x_0,x_1]$ and let $p_1(x)$ be the linear polynomial that interpolates $f$ at $x_0$ and $x_1$.
	Then, for all $x\in[x_0,x_1]$,
	$$|f(x)-p(x)|\leq\frac12|(x-x_0)(x-x_1)|\max_{x_0\leq\zeta\leq x_1}|f''(\zeta)|
		\leq \frac18 (x_1-x_0)^2 \max_{x_0\leq\zeta\leq x_1}|f''(\zeta)|. $$
\end{theorem}
\begin{proof}
	Let $E(x) = f(x)-p_1(x)$ (error at $x$).
	Define
	\begin{align*}
		\omega(x) &= (x-x_0)(x-x_1), \\
		G(x) &= E(x)-\frac{\omega(x)}{\omega(t)}\os{\text{aux. fun.}}{E(t)},
	\end{align*}
	where $x_0<t<x_1$.
	Then
	\begin{itemize}
		\item $G(x_0)=E(x_0)=0$
		\item $G(x_1)=E(x_1)=0$
		\item $G(t) = 0$.
	\end{itemize}
	Rolle's Theorem (MVT) $\implies$ there exists $\eta_0$ in $(x_0,t)$ and $\eta_1$ in $(t,x_1)$ such that $G'(\eta_0)=G'(\eta_1)=0$.
	Rolle's Theorem $\implies$ there exists $\xi_t$ in $(\eta_0,\eta_1)$ such that $G''(\xi_t)=0$.
	\begin{note}
		$G''(x) = f''(x) - \frac2{\omega(t)}E(t)$ and $p_1''(x)=0$ and $w''(x)=2$, $\omega$ is quadratic.
	\end{note}
	$G''(\xi_t)=0 \implies f''(\xi_t)-\frac2{\omega(t)}E(t)=0 \implies E(t) = \frac12\omega(t)\cdot f''(\xi_t) \implies f(t)-p(t) = \frac12(t-x_0)(t-x_1)f''(\xi_t)$.
	$*$ exact error for any $t$ in $(x_0,x_1)$.
	Thus $|E(x)| \leq \frac12 |\ub{\omega(x)\,\text{[quadratic]}\,x^2+\dots}{(x-x_0)(x-x_1)}|\cdot\ds\max_{x_0\leq\xi\leq x_1}|f''(\xi)|$. $\checkmark$
	
	Then $x_c = \frac{x_0+x_1}2$ and $|\omega(x)|$ is maximized over $[x_0,x_1]$ at the vertex $x_c$.
	So
	\begin{align*}
		|\omega(x_c)| &= \mlr{\omega\plr{\frac{x_0+x_1}2}}
					  = \mlr{\plr{\frac{x_0+x_1}2-x_0}\plr{\frac{x_0+x_1}2-x_1}} \\
					  &= \mlr{\frac{x_1-x_0}2\cdot\frac{x_0-x_1}2}
					  = \frac14|x_1-x_0|^2.
	\end{align*}
	implies $\ds |E(x)| \leq \frac12\cdot\frac14|x_1-x_0|^2\cdot\max_{x_0\leq\xi\leq x_1}|f''(\xi)|^2$.
\end{proof}

\subsection*{\S2.5 Application - The Trapezoidal Rule}

\ul{Goal}: Approximate $\int_a^bf(x)\,dx$.

Let $a=x_0<x_1<x_2<\dots<x_N=b$ (mesh), to divide $[a,b]$ into $N$ subintervals $[x_{k-1},x_k]$. Let $q_N(x)$ be the piecewise linear interpolant of $f(x)$ ($f(x_k)=q_N(x_k)$).

\begin{proposition}
	[Trapezoidal Rule ($N$-subintervals)]
	\begin{align*}
		\int_a^b f(x)\,dx = \sum_{k=1}^N\int_{x_{k-1}}^{x_k}f(x)\,dx 
						  \approx \sum_{k=1}^N \int_{x_{k-1}}^{x_k} p_k(x)\,dx
						  = \sum_{k=1}^N\frac12(x_k-x_{k-1}) [f(x_k)+f(x_{k-1})].
	\end{align*}
\end{proposition}
Suppose $x_k-x_{k-1}=h$ for all $k$ (uniform mesh).
Then 
\begin{align*}
	\int_a^b q_N(x)\,dx &= \frac h2\sum_{k=1}^N[f(x_{k-1})-f(x_k)] \\
						&= \frac h2[f(x_0)+2f(x_1)+2f(x_2)+\dots+2f(x_{N-1})+f(x_N)].
\end{align*}
\ul{Notation}: $T_N(f) = \int_a^b q_N(x)\,dx$, $I(f) = \int_a^bf(x)\,dx$.
\begin{example}
	$I = \int_0^1x^3\,dx = \frac14$. $f(x)=x^3$.
	Let $h=\frac14 \implies \frac{b-a}N=h \implies \frac1N=\frac14 \implies N=4$.
	\begin{align*}
		T_4 = \frac h2\blr{f(0) + 2f\plr{\frac14} + 2f\plr{\frac12} + 2f\plr{\frac34} + f(1)} 
			= \frac{17}{64} \approx 0.265625
	\end{align*}
\end{example}
\begin{proposition}
	[Accuracy] (uniform grid) $h=\frac{b-a}N$,
	$\ds I(f)-I_N(f) = -\frac{b-a}{12}\cdot h^2\cdot f''(\xi_h) $
	for some $\xi_h$ in $[a,b]$.
	Thus, $\ds|I(f)-I_N(f)|\leq \frac{b-a}{12}h^2 \max_{a\leq x\leq b}|f''(x)|$.
\end{proposition}
\begin{proof}
	$I(f)=I_N(f)+\mc O(h^2)$.
	Pick $k\in\set{1,2,\dots,N}$. (pick subinterval).
	\S2.4 $\implies \exists \xi_k$ in $[x_{k-1},x_k]$ such that $f(x)-p_k(x) = \frac12(x-x_{k-1})(x-x_k)f''(\xi_k)$ for all $x$ in $[x_{k-1},x_k]$.
	Thus,
	\begin{align*}
		|I(f)-I_N(f)| &= \mlr{\int_a^b[f(x)-q_N(x)]\,dx}
					  = \mlr{\sum_{k=1}^N\int_{x_{k-1}}^{x_k} [f(x)-p_k(x)]\,dx} \\
					  &\leq \sum_{k=1}^N\int_{x_{k-1}}^{x_k}|f(x)-p_k(x)|\,dx
					  = \sum_{k=1}^N\int_{x_{k-1}}^{x_k}|\frac12(x-x_{k-1})(x-x_k)f''(\xi_k)|\,dx \\
					  &\leq \frac12\max_{a\leq x\leq b}|f''(x)|\cdot\sum_{k=1}^N\int_{x_{k-1}}^{x_k}\us{\leq h}{|x-x_{k-1}|}\cdot\us{\leq h}{|x-x_k|}\,dx \\
					  &\leq \frac12\max_{a\leq x\leq b}|f''(x)|\cdot h^2\sum_{k=1}^N \ub{x_k-x_{k-1}=h}{\int_{x_{k-1}}^{x_k} 1\,dx}
					  = \frac12\max_{a\leq x\leq b}|f''(x)|\cdot h^3\sum_{k=1}^N1 \\
					  &= \frac12\max_{a\leq x\leq b}|f''(x)|\cdot h^3\cdot \ub{\frac{b-a}h}N
					  = \frac{b-a}2\cdot h^2\cdot\max{a\leq x\leq b}|f''(x)|.
	\end{align*}
	$*$ book: $\frac{b-a}2\mapsto \frac{b-a}{12}$ using MVT.
\end{proof}
\begin{remark}
	Globally, we have $\mc O(h^2)$ error and locally we have $\mc O(h^3)$ error.
\end{remark}
\begin{example}
	$I=\int_0^1x^3\,dx$. $h=\frac{b-a}N$.
	How small does $h$ need to be for $I_N$ to approximate $I$ with error $\leq10^{-6}$?
	\ul{Goal}: $\frac{b-a}{12}h^2\max_{a\leq x\leq b}|f''(x)|\leq10^{-6}$.
	\begin{align*}
		f(x) &= x^3 \\
		f'(x) &= 3x^2 \\
		f''(x) &= 6x \\
		\max_{0\leq x\leq 1}|f''(x)| &= 6\cdot1=6. \\
		\implies \frac{1-0}{12}\cdot h^2\cdot6 &\leq10^{-6} \\
		\fbox{$h\leq \sqrt2\cdot10^{-3}$.}
	\end{align*}
	More: $N = \frac{b-a}h = \frac1{\sqrt2\cdot10^{-3}} = \frac{10^3}{\sqrt2}\approx707.1$, so need $N\geq708$.
\end{example}

Exam 1: Monday 2/19, Ch. 1 and 2.

\subsection*{Solution of Tridiagonal Linear Systems}

\begin{example}
	Solve the system of equations
	\begin{align*}
		6x_1 + x_2 + 0 + 0 &= 8 \\
		2x_1 + 4x_2 + x_3 +0 &= 13 \\
		0	 + x_2 + 4x_3 + 2x_4 &= 22 \\
		0+0	 + x_3 + 6x_4 &= 27
	\end{align*}
	for $x_1,x_2,x_3,x_4$. In matrix form:
	$$ \us A{\mat{6&1&0&0\\2&4&1&0\\0&1&4&2\\0&0&1&6}}\us{\vx}{\mat{x_1\\x_2\\x_3\\x_4}} = \us{\vb}{\mat{8\\13\\22\\27}} $$
	\ul{Tridiagonal}:
	\begin{itemize}
		\item Equation for $x_i$ involves at most $x_{i-1}$ and $x_{i+1}$.
		\item $A\vx=\vb$ matrix form where $a_{ij}=0$ if $j>i+1$ or $j<i-1$.
		\item Only need to store $\vl,\vd,\vu$ for $\vl$ (length $n-1$) lower diagonal entries, $\vd$ (length $n$) diagonal entries, $\vu$ (length $n-1$) upper diagonal entries for $n$ unknowns $x_1,\dots,x_n$.
	\end{itemize}
	To solve we eliminate unkowns from various equations. Gaussian Elimination provides a strategic approach for matrix form.
	\begin{itemize}
		\item Eliminate $x_1$ from the second equation: subtract $\frac13$ of $1^\text{st}$ equation from second.
			\begin{align*}
				2x_1 + 4x_3 + x_3 = 13 \\
				-\frac13(6x_1+x_2 = 8)
			\end{align*}
			Becomes $\frac{11}3x_2+x_3=\frac{31}3$.
			Replace $2^\text{nd}$ equation with new equation:
			$$\begin{aligned}
				6x_1+x_2 &= 8 \\
				\frac{11}3x_2+ x_3 &= \frac{31}3 \\
				x_2 + 4x_3 + 2x_4 &= 22 \\
				x_3 + 6x_4 &= 27
			\end{aligned}, \qquad
			\us{\tilde A}{\mat{6&1&0&0\\0&\frac{11}3&1&0\\0&1&4&2\\0&0&1&6}}, \qquad \us{\tilde b}{\mat{8\\\frac{31}3\\22\\27}} $$
			$*$ same solution as $A\vx=\vb$.

		\item Eliminate $x_2$ from $3^\text{rd}$ equation: subtract $\frac3{11}$ of $2^\text{nd}$ equation, replace $3^\text{rd}$ equation with this.
			\begin{align*}
				x_2 + 4x_3 + 2x_4 &= 22 \\
				-\frac3{11}\bigg(\frac{11}3x_2 + x_3 &= \frac{31}3\bigg) \\
				%\hline \\
				\implies \frac{41}11 x_3 + 2x_4 &= \frac{211}{11}.
			\end{align*}
			$$\begin{aligned}
				6x_1 + x_2 &= 8 \\
				\frac{11}3 x_2 + x_3 &= \frac{31}3 \\
				\frac{41}{11}x_3 + 2x_4 &= \frac{211}{11} \\
				x_3+6x_4 &= 27
			\end{aligned}, \qquad
			\mat{6&1&0&0\\0&\frac{11}3&1&0\\0&0&\frac{41}{11}&2\\0&0&1&6}, \qquad \mat{8\\\frac{31}3\\\frac{211}{11}\\27} $$

		\item Eliminate $x_3$ from $4^\text{th}$ equation: subtract $\frac{11}{41}$ of $3^\text{rd}$ equation:
			\begin{align*}
				x_3 + 6x_4 &= 27 \\
				-\frac{11}{41}\bigg(\frac{41}{11}x_3+2x_4 &= \frac{211}{11}\bigg) \\
				%\hline \\
				\implies \frac{224}{41}x_4 &= \frac{896}{41}
			\end{align*}
			$$\begin{aligned}
				6x_1 + x_2 &= 8 \\
				\frac{11}3 x_2 + x_3 &= \frac{31}3 \\
				\frac{41}3 x_3 + 2x_4 &= \frac{211}{11} \\
				\frac{224}{41} x_4 &= \frac{896}{41}
			\end{aligned}, \qquad
			\us{\tilde A}{\mat{6&1&0&0\\0&\frac{11}3&1&0\\0&0&\frac{41}{11}&2\\0&0&0&\frac{224}{41}}},\qquad \us{\tilde b}{\mat{8\\\frac{31}3\\\frac{211}{11}\\\frac{896}{41}}} $$
	\end{itemize}
	\ul{Backward substitution}
	Given $x_{i+1},x_{i+2},\dots,x_n$, can find $x_i$ [$\tilde A$ upper triangular]
	\begin{align*}
		\frac{224}{41}x_4 = \frac{896}{41}, \qquad
		\fbox{$x_4 = 4$}
	\end{align*}
	\begin{align*}
		\frac{41}{11} x_3 + 2\ul{x_4} &= \frac{211}{11} \\
		\frac{41}{11} x_3 + 2(4) &= \frac{211}{11} \\
		\frac{41}{11} x_3 &= \frac{123}{11}, \qquad
		\fbox{$x_3 = 3$}
	\end{align*}
	\begin{align*}
		\frac{11}3 x_2 + \ul{x_3} &= \frac{31}3 \\
		\frac{11}3 x_2 + 3 &= \frac{31}3 \\
		\frac{11}3 x_2 &= \frac{22}3, \qquad
		\fbox{$x_2=2$}
	\end{align*}
	\begin{align*}
		6x_1 + \ul{x_2} &= 8 \\
		6x_1 + 2 &= 8 \\
		6x_1 &= 6, \qquad
		\fbox{$x_1=1$}
	\end{align*}
	Solution $=\vx=\mat{1&2&3&4}^T$
\end{example}

\begin{theorem}
	[Algorithm ($n$ equations, $n$ unknowns, \ul{tridiagonal})]
	$$ A\vx = \vf$$
	$\vec\ell, \vd,\vu$, 
	\# Eliminate ($A\mapsto\tilde A$ for $A= \text{tridiag}$)
\end{theorem}

fill in details from photo

\begin{theorem}
	If a tridiagonal matrix is \ul{diagonally dominant} [$d_i>|u_i|+|\ell_i|$ for all $i=1,\dots,n$] then the algorithm is guaranteed to complete.
\end{theorem}
\begin{note}:
	\begin{enumerate}
		\item diagonal dominance ensures $\tilde d_i$ (no division by zero)
		\item diagonal dominance is sufficient, \ul{\ul{not}} necessary.
		\item only error is due to roundoff.
	\end{enumerate}
\end{note}

\subsection*{\S2.7 Application: Simple Two-Point Boundary Value Problems}

\ul{Goal}: Approximate the unknown \ul{function} $u(x)$ over the interval $[a,b]$ such that
$$ \begin{cases} -u''(x) + u(x) = f(x), & a<x<b \\ u(a) = g_a \\ u(b) = g_b \end{cases} $$
for some function $f$ and $g_a,g_b)$ given.
\begin{example}
	$\begin{cases} -u''(x)+u(x)=2\sin(x), & 0<x<\pi \\ u(0)=u(\pi)=0 \end{cases} $
	
	Then $u(x) = \sin(x)$, $u'(x) = \cos(x)$, and $u''(x)=-\sin(x)$.
	
	So $-u''(x)+u(x) = -(-\sin(x))+\sin(x) = 2\sin(x) = f(x) \checkmark$.
	
	$u(0)=\sin(0)=0\checkmark$ and $u(\pi)=\sin(\pi)=0\checkmark$.
\end{example}
\ul{Idea} Divide $[a,b]$ into $n$ equal subintervals $[x_{k+1},x_k]$ and approximate $u(x_k)$.
Let $h=x_k-x_{k-1}$.
\begin{align*}
	f(x_k) = -\os{D_h^2u(x_k)}{u''(x_k)} + u(x_k) 
		   &= -\frac{u(x_k-h)-2u(x_k)+u(x_k+h)}{h^2} + u(x_k) + \frac1{12}h^2 u^{(4)}(\xi_k) \\
		   &\approx -\frac1{h^2}u(x_k-h) + \plr{\frac2{h^2}+1}u(x_k) - \frac1{h^2}u(x_k+h) \\
		   &= -\frac1{h^2}u(x_{k-1}) + \plr{\frac2{h^2}+1}u(x_k) - \frac1{h^2}u(x_{k+1}).
\end{align*}
Let $u_k \approx u(x_k)$ defined by (multiply by $h^2$)
$ -1\cdot U_{k-1} + (2+h^1)U_k-1\cdot U_{k+1} = h^2f(x_k) \implies U_k$ for $k=1,2,\dots,N-1$ defined by a tridiagonal system of linear equations with $U_0=u(x_0)=u(a)=g_a$, $U_N=u(x_N)=u(b)=g_b$.

$*$ we only consider $U_k$ an unknown for $k=1,2,\dots,N-1$.

\ul{$k=1$}: $-U_0+(2+h^2)U_1-U_2=h^2f(x_1)$, then $-U_0=-g_a$ (move to the right because it's known) $$\fbox{$(2+h^2)U_1-U_2=h^2f(x_1)+g_a$}$$

\ul{$k=2$}: \fbox{$-U_1 + (2+h^2) U_2 - U_3 = h^2 f(x_2)$}

\ul{$k=N-2$}: \fbox{$-U_{N-3}+(2+h^2)U_{N-2}-U_{N-1} = h^2f(x_{N-2})$}

\ul{$k=N-1$}: $-U_{N-2} + (2+h^2)U_{N-1} - U_N = h^2f(x_{N-1})$ with $U_N=g_b$.

$$\fbox{$-U_{N-2}+(2+h^2)U_{N-1} = h^2f(x_{N-1}) + g_b$}$$

Matrix form 
$$\mat{2+h^2 & -1 & 0 & 0 &\dots &0 & 0 \\ 
	-1 & 2+h^2 & -1 & 0 & \dots & 0 & 0 \\
	0 & -1 & 2+h^2 & -1 & \dots & 0 & 0 \\
	0 & 0 & -1 & 2+h^2 & \dots  & \vdots & \vdots \\
	\vdots & \vdots  & \vdots & \vdots & \ddots & 2+h^2 & -1 \\
0 & 0 & 0 & 0 & \dots & -1 & 2+h^2}\mat{U_1\\U_2\\U_3\\\vdots\\U_{N-2}\\U_{N-1}} = \mat{h^2f(x_1) + g_a \\ h^2f(x_2) \\ h^2f(x_3) \\ \vdots \\ h^2f(x_{N-2}) \\ h^2f(x_{N-1})+g_b} $$
\begin{fact}
	$|u(x_k)-U_k|\leq \mc O(h^2)$.
\end{fact}

\begin{recall}
	$\begin{cases} -u''+u = f(x) \\ u(a) = g_a \\ u(b) = g_b \end{cases}$,
	$-D_h^2u+u = f$ with $\ds D_h^2u(x) = \frac{u(x-h)-2u(x)+2u(x+h)}{h^2} \approx u''(x)+\mc O(h^2)$.
\end{recall}
\begin{example}
	$-u''(x) + u(x) = 2\sin(x)+1$ on $[0,\pi]$ with $u(0)=u(\pi)=1$. [solution: $u(x)=\sin(x)+1$].
	Using 6 intervals ($N=6$).
	$h = \frac{b-a}N = \frac\pi6$.
	So $x_0=0$, $x_1=\frac\pi6$, $x_2=\frac\pi3$, $x_3=\frac\pi2$, $x_4=\frac{2\pi}3$, $x_5=\frac{5\pi}6$, $x_6=\pi$. $u_0 = u(0) = 1$, $u_6=u(\pi)=1$, unknowns $u_k$ for $k=1,\dots,5$.
	$$ \mat{
		2+\frac{\pi^2}6 & -1 & 0 & 0 & 0 \\
		-1 & 2+\frac{\pi^2}6 & -1 & 0 & 0 \\
		0 & -1 & 2+\frac{\pi^2}6 & -1 & 0 \\
		0 & 0 & -1 & 2+\frac{\pi^2}6 & -1 \\
		0 & 0 & 0 & -1 & 2+\frac{\pi^2}6 }
		\mat{u_1\\u_2\\u_3\\u_4\\u_5} =
		\mat{h^2f(x_1)+g_a\\h^2f(x_2)\\h^2f(x_3)\\h^2f(x_4)\\h^2f(x_5)+g_b} =
		\mat{\frac{\pi^2}{36}\blr{2\sin\plr{\frac\pi3}+1}+1 \\
			\frac{\pi^2}{36}\blr{2\sin\plr{\frac\pi3}+1}+1 \\
			\frac{\pi^2}{36}\blr{2\sin\plr{\frac\pi2}+1} \\
			\frac{\pi^2}{36}\blr{2\sin\plr{\frac{2\pi}3}+1} \\
			\frac{\pi^2}{36}\blr{2\sin\plr{\frac{5\pi}6}+1} +1} = 
			\mat{\frac{\pi^2}{18}+1\\\frac{(\sqrt3+1)\pi^2}{36} \\ \frac{(\sqrt2+1)\pi^2}{36} \\ \frac{(\sqrt3+1)\pi^2}{36} \\ \frac{\pi^2}{18}+1}$$
	Run algorithm from \S2.6
	$$ \vu = \mat{1.4732\\1.8020\\1.8758\\1.8020\\1.4732}$$
	Graph of points with exact curve.
\end{example}

\section*{Ch.3 Root Finding}

\ul{Goal}: Given a function $f(x)$, find $\alpha$ such that $f(\alpha)=0$.

\ul{Idea}: Form a sequence $\set{x_k}_{k=0}^\infty$ such that $\lim_{x\ra\infty}x_k=\alpha$.

\ul{Global Method} $x_k\ra\alpha$ for any initial guess $x_0$.

\ul{Local Method} $x_k\ra\alpha$ \ul{only if} $x_0$ is close to $\alpha$.

\begin{itemize}
	\item global methods are typically slow (but reliable),
	\item local methods are fast \ul{when} they work.
	\item to improve reliability, use a global method first for a few steps and use the output as an initial guess for a faster method.
	\item double roots are hard to find.
\end{itemize}

$f(x) = 0$

\subsection*{3.1 Bisection Method}

\say{Global} method for finding zeros where $f(x)$ changes signs.
\ul{Idea} $f$ continuous.
Suppose $f(a)\cdot f(b)<0$ (different signs).
IVT $\implies \exists\alpha$ in $(a,b)$ such that $f(\alpha)=0$.
Let $c=\frac{a+b}2$.
\begin{itemize}
	\item If $f(c)f(a)<0$, then $\exists\tilde\alpha\in(a,b)$ such that $f(\tilde\alpha)=0$.
	\item If $f(c)=0$, then $\tilde\alpha=c$ and $f(\tilde\alpha)=0$ (Done).
	\item If $f(c)\cdot f(b)<0$, then $\exists\tilde\alpha\in(c,b)$ such that $f(\tilde\alpha)=0$.
\end{itemize}
Know $a<\alpha<b$ (width $b-a)$.
Know $a<\tilde\alpha\leq c$ \ul{or} $c<\tilde\alpha<b$ (width $\frac{b-a}2$).
If $f(c)\neq0$, let $\tilde a=a$, $\tilde b=c$, or $\tilde a =c$, $\tilde b=b$ and repeat.
Eventually, the process will converge to a root $\alpha^*\in(a^*,b^*]$ with $b^*-a^*$ arbitrarily small.
\ul{Bisection Method} Let $a_0=a,b_0=b$, where $f9a)\cdot f(b)<0$.
For $k=0,1,2,\dots$
$$ c_k = \frac{a_k+b_k}2 = a_k + \frac12(b_k-a_k) \qquad \text{[stable form]} $$
If $f(c_k)=0$: $\alpha=c_k$. break. verbatim if $(|c-0|<tol)$.

If $f(a_k)f(c_k)<0$, then $a_{k+1}=a_k$ and $b_{k+1}=c_k$, else $a_{k+1}=c_k,b_{k+1}=b_k$. Finally $k=k+1$, end.

\begin{theorem}
	Suppose $f$ is continuous on $[a,b]$ and $f(a)f(b)<0$.
	Let $x_n=c_n = \frac{a_{n-1}+b_{n-1}}2$.
	Then there exists \ul{a root} $\alpha\in[a,b]$ such that $|x_n-\alpha|\leq\plr{\frac12}^n(b-a)$.
	Let $\epsilon>0$. Then $|x_n-\alpha|\leq\epsilon$ if $n\geq\frac{\log(b-a)-\log\epsilon}{\log2}$.
\end{theorem}
\begin{note}:
	\begin{enumerate}
		\item The zero $\alpha$ is not known beforehand. The theorem guarantees $\alpha$ exists.
		\item This method is considered slow.
		\item The error may not decrease on each step. $x_k$ closer to $\alpha$ than $x_{k+1}$.
	\end{enumerate}
\end{note}

Want to solve $f(x)=0$.

\begin{example}
	$f(x)=x^3-3$, $[a,b]=[0,2]$.
	$\alpha = \nrt33 \approx 1.44225$.
	\begin{align*}
		x_0 = \frac{a+b}2 = 1. \\
		f(0) = -3, \qquad \ub{\text{sign change}}{f(1)=-2, \qquad f(2)=5}  \\
		a_1 = 1, \qquad b_1 = 2, \qquad x_1 = \frac{1+2}2 = \frac32 = 1.5. \\
		\ub{\,}{f(1) = -2, \qquad f\plr{\frac38},} \qquad f(2)=5 \\
		a_2 = 1, \qquad b_2=\frac32, \qquad x_2 = \frac{1+\frac32}2 = \frac54 = \us{\us{\text{estimate than }x_1}{\text{this is a worse}}}{\ul{1.25}} \\
		f(1) = -2, \qquad \ub{\,}{f(1.25) = - \frac{67}{64}, \qquad f\plr{\frac32} = \frac38} \\
		a_3 = 1.25, \qquad b_3 = \frac32, \qquad x_3 = \frac{11}8 = 1.375 \\
		f(1.25) = -\frac{67}{64}, \ub{\,}{f\plr{\frac{11}3} = -\frac{205}{512}, \qquad f\plr{\frac32}} \\
		a_4 = \frac{11}8, \qquad b_4 = \frac32, \qquad x_4 = \frac{23}{16} = 1.4375 \\
		f\plr{\frac{11}8} = -\frac{205}{512}, \qquad \ub{\,}{f\plr{\frac{23}{16}} = -\frac{121}{4096}, \qquad f\plr{32} = \frac38} \\
		a_5 = \frac{23}{16}, \qquad b_5 = \frac32, \qquad x_5 = \frac{47}{32} = 1.46875.
	\end{align*}
	After 5 iterations:
	\begin{itemize}
		\item $|x_5-\alpha| = 0.0265$.
		\item $f(x_5) = \frac{5519}{32768} \approx 0.16843$ (getting closer to zero).
	\end{itemize}
	\begin{align*}
		|x_N - \alpha| &\leq \epsilon = 10^{-3}. \\
		\implies N &\geq \frac{\log(b-a)-\log(10^{-3})}{\log2} \\
					&= \frac{\log(2) + 3\log(10)}{\log 2}
					\approx 10.966 \qquad \text{(is sufficient)}
	\end{align*}
\end{example}
\begin{note}
	Error \ul{does not} necessarily decrease with each iteration (upper bound does).
\end{note}

\subsection*{\S3.2 Newton's methods}

Derivation and Examples. 

\ul{Newton's method}: Given $x_0$, $$ x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)} $$ for all $n=0,1,2,\dots$.
\ul{Goal}: $x_n\ra\alpha$ as $n\ra\infty$, where $f(\alpha)=0$.
\begin{note}:
	\begin{enumerate}
		\item \ul{local} method. Behavior/success depends upon $x_0$ and $f$.
		\item converges fast when it works (mostly). Repeated roots slow down convergence.
	\end{enumerate}
\end{note}
\subsubsection*{Derivation}

\ul{Approach 1}: Taylor's Expansion. Let $x_n\approx\alpha$. Center at $x_n$.
$$ 0 = f(\alpha) = f(x_n) + (\alpha-x_n)f'(x_n) + \mc O(|\alpha-x_n|^2) $$
Solve for $x_n$:
$$ \alpha - x_n \approx - \frac{f(x_n)}{f'(x_n)} \implies \alpha \approx x_n - \frac{f(x_n)}{f'(x_n)}. $$

\ul{Approach 2}: Graphical. tangent line at $x_0$, tangent line at $x_1$.
\begin{itemize}
	\item Find the tangent line at $x_k$.
	\item Let $x_{k+1}$ be the $x$-intercept of \ul{the tangent line} (easy root finding problem).
	\item Repeat.
\end{itemize}
Tangent line:
\begin{align*}
	y - f(x_k) = f'(x_k)(x-x_k) \\
	y = f(x_k) + f'(x_k)(x-x_k) \\
	0 = f(x_k) + f'(x_k)(x-x_k) \\
	\implies x = x_k - \frac{f(x_k)}{f'(x_k)} \equiv x_{k+1}
\end{align*}
\begin{example}
	$f(x) = x^3-1$, $\alpha=1$.
	\begin{align*}
		x_0 &= 2. \\
		f'(x) &= 3x^2. \\
		x_{k+1} &= x_k - \frac{f(x_k)}{f'(x_k)} = x_k - \frac{x_k^3-1}{3x_k^2}. \\
		x_0 &= 2. \\
		x_1 &= 2-\frac{2^3-1}{3\cdot2^2} = \frac{17}{12} = 1.41\conj6 \\
		x_2 &= \frac{17}{12} - \frac{\plr{\frac{17}{12}}^3-1}{3\cdot\plr{\frac{17}{12}}^2} = \frac{5777}{5202} \approx 1.11053441. \\
		x_3 &\approx 1.0106367 \\
		x_4 &\approx 1.00011156.
	\end{align*}
\end{example}
\begin{example}
	$f(x) = \ln(x)$. $x_0=e$.
	\begin{align*}
		f'(x) &= \frac1x \\
		x_1 &= x_0 - \frac{f(x_0)}{f'(x_0)} = x_0 = \frac{\ln(x_0)}{1/x_0} = x_0 - x_0\ln(x_0) = e-e(1)=0. \\
		x_2 &= x_1 - x_1\ln(x_1), \qquad \ln(0) \text{ undefined}
	\end{align*}
\end{example}
\begin{example}
	$f(x)=x^2$, $x_0=1$, $\alpha=0$.
	\begin{align*}
		f'(x) &= 2x \\
		x - \frac{f(x)}{f'(x)} &= x-\frac{x^2}{2x} = x-\frac12x=\frac12x \qquad \text{for }x\neq0 \\
		x_0 &= 1. \\
		x_1 &= \frac12. \\
		x_2 &= \frac14. \\
		x_3 &= \frac18.
	\end{align*}
	$x_k = \plr{\frac12}^k\ra0$ as $k\ra\infty$.
\end{example}

Find $x$ such that $f(x)=0$, use $x_{n+1}=x_n-\frac{f(x_n)}{f'(x_n)}$ for $n=0,1,2,\dots$.
Goal: $x_n\ra\alpha$ for $f(\alpha)=0$.

\begin{example}
	Consider a function $f(x)$ with $f'(p)=0$ and $\alpha<p$ and an asymptote such that $f(x)\ra0^+$ as $x\ra\infty$. Then $x_0<p \implies x_n\ra\alpha$ and $x_0>p\implies x_n\ra\infty$ and for $x_0=p$ the method is undefined since $f'(p)=0$.
\end{example}
\begin{example}
	Consider a function with graph 2. Then $|x_0|<\beta \implies x_n\ra\alpha$ and $x_0=\beta\implies x_{2n+1}=-\beta,x_{2n}=\beta$ and $|x_0|>\beta\implies$ diverges.
\end{example}
\begin{example}
	Concave up function, $x_0>0\implies x_n\ra\alpha$ and $x_0<0\implies x_n\ra \alpha_{-1}$ and $x_0=0\implies x_1$ undefined.
\end{example}

\subsection*{\S3.3 How to stop Newton's Method}

\ul{Goal}: Decide criteria for choosing $N$ such that $x_N\approx\alpha$ for $\set{x_n}_{n=0}^N$ by Newton's Method.
\begin{enumerate}
	\item[] \ul{Assumption} $x_n\ra\alpha$ as $n\ra\infty$. 

	\item[] \ul{IF} the method converges, \ul{THEN} we can detect when $|\us{\text{error}}{x_N-\alpha}|<\text{TOL}$ and stop searching.

	\item[] \ul{Assumption} $f'(\alpha)\neq0$ and $f,f'$ are continuous on $[a,b]$ with $a<\alpha<b$.

	\item[$*$] we can choose an interval $[\tilde a,\tilde b]$ such that
		\begin{itemize}
			\item $\alpha\leq\tilde a<\alpha$, $\alpha<\tilde b\leq b$
			\item $C|f'(\alpha)| = M\geq |f'(x)|\geq m = \frac{|f'(\alpha)|}C$, $C>1$, for al $\tilde a<x<\tilde b$.
			\item $x_n\in[\tilde a,\tilde b]$ for all $n$ large enough. Convergence: $|x_n-\alpha|<\epsilon$ for all $n$ large.
		\end{itemize}
\end{enumerate}
\ul{Idea}: By the MVT, $\exists \xi_n$ between $x_n$ and $\alpha$ such that $f(x_n)-f(\alpha) = f'(\xi_n)(x_n-\alpha) \implies \fbox{$\ds(x_n-\alpha) = \frac{f(x_n)}{f'(x_n)}$}$

\ul{Case 1}: $|f(x_n)|$ small
\begin{align*}
	|x_n-\alpha| &= \frac{|f(x_n)|}{|f'(x_n)|} \leq \frac{|f(x_n)|}{\ds\min_{\tilde a\leq\xi\leq\tilde b}|f'(\xi)|} \\
				 &\leq \frac{|f(x_n)|}m = \frac1m|f(x_n)|\ <\text{TOL}
\end{align*}
Typically $<5\text{TOL}$. This implies STOP at $N$ if $|f(x_N)|\leq m\text{TOL}$.

\ul{Criteria 2}: $|x_m-x_n|$ small. $\ds x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)} \implies f(x_n) = f'(x_n) ( x_n-x_{n+1})$.
Last time:
\begin{align*}
	x_n - \alpha &= \frac{f(x_n)}{f'(x_n)} = \frac{f'(x_n)}{f'(\xi_n)}(x_n-x_{n+1}), \\
	|x_n-\alpha| &= \mlr{\frac{f'(x_n)}{f'(\xi_n)}}\cdot|x_n-x_{n+1}|
				 \leq \frac Mm|x_{n+1}-x_n|
\end{align*}
implies STOP at $N$ if $|x_N-x_{N-1}| < \frac mM\text{TOL} \leq \text{TOL}$.
\begin{note}:
	\begin{itemize}
		\item Both $|f(x_n)|$ and $|x_n-x_{n-1}|$ are computable and can be checked on each iteration.
		\item $N$ is not known before hand.
		\item Conservatively, we stop if $|f(x_n)|+|x_n-x_{n-1}|\leq \text{TOL}/5$.
		\item The test can lead to false conclusions if $x_n\not\ra\alpha$.
	\end{itemize}
\end{note}
\begin{enumerate}
	\item[$*$] Need \text{TOL} small enough to rule out false answer, but large enough to account for roundoff error.
\end{enumerate}

\subsection*{\S3.5 The Newton Error Formula}

\ul{Goal}: Classify how fast Newton's method converges \ul{when} $x_n\ra\alpha$.

\begin{theorem}
	Let $f\in C^2(I=[a,b])$ with $f(\alpha)=0$ for some $\alpha\in I$ (not an end point).
	If $x_n\in I$ and $\ds x_{n+1}=x_n-\frac{f(x_n)}{f'(x_n)}$ [Newton] then $\exists\xi_n$ between $\alpha$ and $x_n$ such that $\ds(\alpha-x_{n+1}) = -\frac12(\alpha-x_n)^2 \cdot \frac{f''(\xi_n)}{f'(x_n)}$.
\end{theorem}
\begin{note}:
	\begin{enumerate}
		\item Does not guarantee $x_{n+1}\in I$.
		\item $\ds \us{\text{error}}{|x_{n+1}-\alpha|} = \mlr{\frac{f''(\xi_n)}{2f'(x_n)}}\cdot \us{\text{old error.}}{|x_n-\alpha|^2}$
			
			Let $M_n= \mlr{\frac{f''(\xi_n)}{2f'(x_n)}}$.
			If $M_n$ is bounded for all $n$, then the new error $=\mc O(|\text{old error}|^2)$.
			If $|\text{old error}|<1$, then $|\text{new error}|$ much smaller $\implies$ fast convergence once $|x_n-\alpha|$ small enough.
	\end{enumerate}
\end{note}
\begin{proof}
	Taylor series, center at $x_n$: $0 = f(\alpha) = f(x_n) + (\alpha-x_n)f'(x_n) + \frac12(\alpha-x_n)^2 f''(\xi_n)$ for some $\xi_n$ between $\alpha$ and $x_n$.
	\begin{align*}
		(x_n-\alpha)f'(x_n)-f(x_n) &= \frac12(\alpha-x_n)^2f''(\xi_n). \\
		(x_1-\alpha) - \frac{f(x_n)}{f'(x_n)} &= \frac12(\alpha-x_n)^2\frac{f''(\xi_n)}{f'(x_n)} \qquad \text{RHS}\checkmark \\
		\text{LHS} &= x_n - \alpha -\frac{f(x_n)}{f'(x_n)}
				   = x_{n+1} - \alpha \qquad \checkmark
	\end{align*}
\end{proof}
\begin{example}
	Suppose for some $f$
	\begin{itemize}
		\item $|f''(x)|\leq 10$ for all $x$,
		\item $|f'(x)| \geq 3$ for all $x$,
		\item $|x-\alpha|<\frac12$.
	\end{itemize}
	\begin{enumerate}
		\item[a)] Bound the error in the $1^\text{st}$ three steps of Newton's methods.
			\begin{align*}
				|x_1-\alpha| &\leq \frac{|f''(\xi_1)|}{2|f'(x_0)|}\cdot|x_0-\alpha|^2, \qquad |f'(x)|\geq 3\implies \frac1{|f'(x_0)|} \leq\frac13 \\
							 &\leq \frac{10}{2\cdot3}\cdot\plr{\frac12}^2
							 = \frac5{12} = 0.41\conj 6. \\
				|x_2 - \alpha| &\leq \frac{10}{2\cdot3}|x_1-\alpha|^2 \\
							   &\leq \frac{10}6\cdot\plr{\frac5{12}}^2
							   = \frac{125}{432} = 0.2893\conj{518} \\
				|x_3-\alpha| &\leq \frac{10}6\cdot\plr{\frac{125}{432}}^2 \approx 0.13954 \\
				\vdots\quad & \\
				|x_6-\alpha| &\leq 5.135\times10^{-6} \\
				\vdots\quad & \\
				|x_8-\alpha| &\leq 3.219\times10^{-21}
			\end{align*}
		\item[b)] How small does the initial error need to be to \ul{guarantee} convergence? $|x_n-\alpha|\ra0$.
			Need $|x_0-\alpha|<\frac6{10}$.
			\begin{note}
				The method \ul{may} converge for $x_0$ further from $\alpha$. It \ul{will} converge for $|x_0-\alpha|<\frac6{10}$.
			\end{note}
			Let $\mlr{\frac{f''(\xi)}{2f'(x)}}\leq M$, $e_n=|x_n-\alpha|$.
			Relate $|x_0-\alpha|=e_0$ to $M$.
			\begin{align*}
				e_1 &\leq Me_0^2 = M\inv(Me_0)^2 \\
				e_2 &\leq Me_1^2 = M\cdot(M\inv)^2 \plr{(Me_0)^2}^2 = M\inv(Me_0)^4 \\
				e_3 &\leq M\inv(Me_0)^8.
			\end{align*}
			In general, \fbox{$e_n\leq M\inv(Me_0)^{2^n}$} for all l$n\geq0$.
			Then $\lim_{n\ra\infty}e_n=0$ if \fbox{$|Me_0|<1$}.
			In the example, $M=\frac{10}6$, so $$\frac{10}6e_0<1 \implies e_0<\frac6{10}, \qquad \fbox{$|x_0-\alpha|<\frac6{10}$}$$
			If $x_0\in(\alpha-\frac6{10},\alpha_\frac6{10})$, then $x_n\ra\alpha$ as $n\ra\infty$.
	\end{enumerate}
\end{example}
%$$ (\alpha-x_{n+1}) = -\frac12 (\alpha-x_n)^2\frac{f''(\xi_n)}{f'(x_n)} $$
%\begin{example}
%	$|f''(x)|\leq10$, $|f'(x)|\geq2$, $|x-\alpha|\leq\frac12$.
%\end{example}
\begin{note}
	The methods \ul{may} converg for $x_0$ further from $\alpha$. It \ul{will} converge for $M|x_0-\alpha|<1$.
\end{note}

\begin{definition}
	Let $x_n\ra\alpha$ such that $$\lim_{n\ra\infty} \frac{|\alpha-x_{n+1}|}{|\alpha-x_n|^p}=C$$ for some nonzero (finite) constant $C$ and some $p$.
	The number $p$ is the \ul{order} of the convergence for the sequence.
\end{definition}
\begin{example}
	[Newton's] $\ds \frac{|\alpha-x_{n+1}|}{|\alpha-x_n|^2} = \frac12\mlr{\frac{f''(\xi_n)}{f'(x_n)}}$.
	Suppose $f'(\alpha)\neq0$ and $f''$ is bounded. $f'$ constant $\implies f'(x_n\ra f'(\alpha)$. $|f''|$ bounded. $f''$ constant $\implies f''(\xi_n)\ra f''(\alpha)$.
	$\xi_n$ between $x_n$ and $\alpha$. Limit holds by squeeze theorem.
	$f''(x_n)\ra f''(\alpha) \implies f''(\xi_n)\ra f''(\alpha)$. ($\xi_n\ra\alpha$).
	Thus, 
	\begin{align*}
		\lim_{n\ra\infty} \frac{|\alpha-x_{n+1}|}{|\alpha-x_n|^2} &= \frac12\mlr{\frac{f''(\alpha)}{f'(\alpha)}} = C\in(0,\infty) \\
																  &\leq\frac12\frac{\max}{|f'(\alpha)|}.
	\end{align*}
	If $f''$ constant \ul{and} $f'(\alpha)\neq0,f''(\alpha)\neq0$, then \ul{$p=2$}.
\end{example}
\begin{example}
	For $f(x)=x^2$, Newton's method converges for all $x_0$ with $p=1$.
\end{example}

\subsection*{\S3.6 Newton's Method: Theory and Convergence}

\ul{Goal}: Develop sufficient conditions that guarantee Newton's $\ra\alpha$.

\begin{theorem}
	[Optimistic] Assume $f$ is defined and twice continuously differentiable for all $x\in(-\infty,\infty)$, with $f(\alpha)=0$ for some $\alpha$.
	Define the ratio
	$$ M = \frac{\ds\sup_{x\in\mb R}|f''(x)|}{2\ds\inf_{x\in\mb R}|f'(x)|}. \qquad \text{book} : \frac\max\min, \text{correcting for asymptotes} $$
	and assume that $M<\infty$. Then for any $x_0$ such that $M|\alpha-x_0|<1$, Newton's method converges.
	Moreover, $|\alpha-x_n|\leq M\inv(M|\alpha-x_0|)^{2^n}$.
	\begin{enumerate}
		\item[$*$] Need $x_0$ close enough to have $M|\alpha-x_0|<1$ (only need $M|\alpha-x_k|<1$ for some $k\geq0$).
	\end{enumerate}
	Optimistic $\implies$ only need local estimates for $M$, not global.
\end{theorem}
\begin{observe}
	With such strong assumptions, we can find how big $n$ needs to be. Goal: $|x_N-\alpha|\leq\epsilon$. Observe
	\begin{align*}
		M\inv(M|\alpha-x_0|)^{2^n} &\leq \epsilon \\
		(M|\alpha-x_0|)^{2^n} &\leq M\epsilon \\
		2^n \ub{<0}{\log_2(\ub{<1}{M|\alpha-x_0|})} &\leq \log_2(M\epsilon) \\
		2^n &\geq \frac{\log_2(M\epsilon)}{\log_2(M|\alpha-x_0|)}
	\end{align*}
	$$\fbox{$\ds n\geq \log_2\plr{\frac{\log_2(M\epsilon)}{\log_2(M|\alpha-x_0|)}}. $}$$
\end{observe}

\begin{example}
	Consider $f$ such that
	\begin{itemize}
		\item $f(a)=0$ for $\alpha\in[2,3]$,
		\item $f'(x)\geq3$ and $0\leq f''(x)\leq5$ for all $x\in\mb R$.
	\end{itemize}
	Show the method converges for $y_0=\frac52$.
	How many iterations to get $10^{-4}$ accuracy.
	$$ M = \frac{\sup|f''|}{2\inf|f'|} $$
	$ M \leq \frac 5{2\cdot3} = \frac56$.
	Then $|x_0-\alpha| \leq \frac12$ ($x_0$ midpoint of $[2,3]$).
	$M|x_0-\alpha| \leq \frac56\cdot\frac12 = \frac5{12}<1$ implies convergence.
	Need $$ n\geq\log_2\plr{\frac{\log_2(\frac56\cdot10^{-4})}{\log_2(5/12)}} \approx 3.4234 $$
	Need $n\geq4$ iterations.
	\begin{align*}
		M|x_0-\alpha| &> 1 \\
		\frac56\cdot|x_0-\alpha| &> 1 \\
		|x_0-\alpha| &> \frac65.
	\end{align*}
	$\alpha$ in $[0,4]$, $x_0=2$. $|x_0-\alpha|\leq2$.
	Run bisection with $\text{tol}=\frac65$. $\ra x_N$ with $|x_N-\alpha|<\frac56$. Use $x_N$ for Newton, this is for guaranteed convergence.
\end{example}
\begin{example}
	$f$ smooth and monotone with root $x=\alpha$. Newton's method may not converge for all $x_0$.
	Let 
	\begin{align*}
		f(x) &=\tan\inv(x) \\
		f'(x) &= \frac1{1+x^2}>0 \qquad \forall t \\
		f(0) &=0 \\
		f''(x) &= -\frac{2x}{(1+x^2)^2} \qquad \text{(bounded)}
	\end{align*}
	($M=\infty$ since $\ds\inf_{x\in\mb R}|f'(x)|=0$ and $\ds0<\sup_{x\in\mb R}|f''(x)|<\infty$).
	If $|x_0|$ is large enough, then $x_n\not\ra0$.
\end{example}
\begin{proof}
	\begin{align*}
		x_{n+1} &= x_n-\frac{f(x_n)}{f'(x_n)} \\
				&= x_n-\frac{\tan\inv(x_n)}{1/(x^2+1)} \\
				&= x_n-(x_n^2+1)\tan\inv(x).
	\end{align*}
	Suppose $\tan\inv(|x_n|)\geq1$. ($|a-b|\geq|a|-|b|$ inverse triangle inequality). Then,
	\begin{align*}
		|x_{n+1}| &= |x_n-(x_n^2+1)\tan\inv(x_n)| \\
				  &\geq |(x_n^2+1)\tan\inv(x_n)|-|x_n| \\
				  &\geq |x_n|^2+1-|x_n| \\
				  &\geq |x_n|^2-|x_n| \\
				  &= |x_n|(|x_n|-1).
	\end{align*}
	If $|x_{n+1}|\geq |x_n|$, the method will not converge.
	Solving:
	\begin{align*}
		|x_n|\cdot(|x_n|-1) &\geq \ub{>0}{|x_n|} \\
		|x_n|-1 &\geq 1 \\
		|x_n| &\geq2.
	\end{align*}
	For $|x_n|\geq2$ and $|\tan\inv(x_n)|\geq1$, we have $|x_{n+1}|\geq|x_n|$ ($\geq\dots\geq|x_0|$).
	Pick $x_0$ such that $|x_0|\geq2$ and $|\tan\inv(x_0)|\geq1$.
\end{proof}

\begin{theorem}
	[Local Result]
	Let $f\in C^2(I)$, where $\alpha\in I=[a,b]\subset\mb R$ is a root.
	Assume $f'(\alpha)\neq0$, and let $\ds x_{n+1}=x_n-\frac{f(x_n)}{f'(x_n)}$.
	For $x_0$ sufficiently close to $\alpha$, we have $\ds\lim_{n\ra\infty}x_n=\alpha$ and $\ds\lim_{n\ra\infty} \frac{|\alpha-x_{n+1}|}{|\alpha-x_n|^2} = -\frac{f''(\alpha)}{2f'(\alpha)}$.
\end{theorem}
\begin{proof}
	[Proof idea] There exists $\tilde I=[\tilde a,\tilde b]\subseteq[a,b]$ such that
	\begin{itemize}
		\item $\alpha\in\tilde I$,
		\item $\ds M = \frac{\ds\max_{x\in\tilde I}|f''(x)|}{\ds\min_{x\in\tilde I}|f'(x)|} < \infty$,
		\item $x_{n+1}$ is in $\tilde I$ for $x_n$ in $\tilde I$,
		\item $M|\alpha-x_0|<1$ for $x_0\in\tilde I$.
	\end{itemize}
	$M<\infty$: $f''$ continuous on $\tilde I$ (bounded), $f'(\alpha)\neq0\implies |f'|\geq \frac{|f'(\alpha)|}2$ on $\tilde I$.
	Error $$|\alpha-x_{n+1}|\sim M\inv(M|\alpha-x_n|)^2 \leq |x_0-\alpha|$$
	\begin{itemize}
		\item small enogh for $M|\alpha-x_n|<1$
		\item small enough for $|x_{n+1}-\alpha|\leq|x_n-\alpha| \implies x_{n+1}$ in $\tilde I$.
	\end{itemize}
	Key: only need $M$ for $x$-values \ul{near} $\alpha$, implies need to start closer to $\alpha$. (May converge otherwise. This is for a guarantee).
\end{proof}

\subsection*{\S3.8 The Secant Method}

\ul{Goal}: Approximate $f'(x_n)$ in Newton's method to avoid calculating $f'$.

\ul{Idea}: For $x_n\ra\alpha$, $x_{n-1}$ is close to $x_n$.
Let $h_n=x_{n-1}-x_n$. Then, 
\begin{align*}
	f'(x_n) = \lim_{h\ra0}\frac{f(x+h)-f(x)}h
	\approx \frac{f(x_n+h_n)-f(x_n)}{h_n}
	= \frac{f(x_{n-1})-f(x_n)}{x_{n-1}-x_n}
\end{align*}
\ul{Secant Method}: $\ds x_{n+1} = x_n - f(x_n)\blr{\frac{x_n-x_{n-1}}{f(x_n)-f(x_n)-f(x_{n-1})}} $ for $x_0,x_1$ given.
\begin{note}
	Reuses the values $f(x_n),f(x_{n-1})$ to avoid additional overhead in approximating $f'$.
\end{note}
\ul{Geometrically} secant method uses the secant line for $x_{n-1},x_n$ to find $x_{n+1}$ instead of tangent line at $x_n$.

\ul{Stopping Criteria}
\begin{itemize}
	\item $|f(x_n)|$ small
	\item $|x_n-x_{n-1}|$ small

		$\alpha - x_n = c_n(x_n-x_{n-1})$ \ul{and} $c_n\ra1$ as $n\ra\infty$ \ul{if} $x_n\ra\alpha$.
\end{itemize}
\ul{Error Formula}
$\ds\alpha - x_{n+1} = -\frac12(\alpha - x_n)(\alpha - x_{n-1}) \frac{f''(\xi_n)}{f'(\eta_n)}$ for some $\xi_n,\eta_n$ such that $$\min\set{\alpha,x_n,x_{n-1}} \leq \xi_n,\eta_n \leq \max\set{\alpha,x_n,x_{n-1}}.$$

\ul{Rate of convergence} $p = \frac{1+\sqrt5}2 \approx 1.618$.

\ul{(Optimistic) convergence} $\ds M = \frac{\ds\sup_{x\in\mb R}|f''(x)|}{\ds2\inf_{x\in\mb R}|f'(x)|}\in[0,\infty)$.
Then $x_n\ra\alpha$ if $M(\max\set{|x_0-\alpha|,|x_1-\alpha|}) < 1$.
\S3.11.3 proofs + local analysis.
$*$ convergence for $f$ nice and $x_0,x_1$ are close enough to $\alpha$. Slower rate than Newton's, but less work per step.

\begin{example}
	$f(x) = 2^x-4$, ($\alpha = 2$). $x_0=0$, $x_1=1$. Find $x_3$. 
	\begin{align*}
		x_2 &= x_1-f(x_1)\cdot \frac{x_1-x_0}{f(x_1)-f(x_0)}
			= 1-(-2)\cdot\frac1{-2-(-3)} = 1+2(1) = 3. \\
		x_3 &= x_2 - f(x_2) \cdot \frac{x_2-x_1}{f(x_2)-f(x_1)}
			= 3-(4)\cdot\frac{3-1}{4-(-2)} = \frac53.
	\end{align*}
	$|\alpha-x_7|\approx 2.091\times10^{-6}$. $|\alpha-x_8| \approx 3.422\times10^{-10}$.
\end{example}
\begin{example}
	$f(x)$ such that
	\begin{itemize}
		\item $f(a)=0$ for $0<\alpha<8$, $f'(\alpha)\neq0$, $f(0)f(8)<0$.
		\item $|f''(x)| \leq 6$
		\item $|f'(x)|\geq 2$.
	\end{itemize}
	How many steps of the bisection method are needed to generate $x_0,x_1$ such that secant method converges?
	$ M\leq\frac6{2\cdot2} = \frac32$. Need $\frac32|x_i-\alpha|<1$ so $|x_i-\alpha|<\frac23$ for $i=0,1$.
	Not possible on $[0,8]$ without more information.
	\ul{Bisection}: $|x_N-\alpha| < \frac23$ for $N\geq \frac{\log(b-a)-\log(\frac23)}{\log 2} \approx 3.58$ for $b-a=8$.
	$\tilde x_0$ requires/based on 4 iterations of bisection.
	$\tilde x_1$ requires 5 iterations.
	Need 5 iterations of Bisection to generate $\tilde x_0,\tilde x_1$ for secant method.
\end{example}

\subsection*{\S3.9 Fixed-Point Iterations}

\ul{Goal}: Develop abstract convergence tool for root-finding methods.
\begin{definition}
	$x = \beta$ is a fixed point of $g(x)$ \ul{if} $g(\beta)=\beta$.
\end{definition}
\ul{Idea}: solve $g(x)=x$ for $x=\beta$ i.e., solve $\ub{\text{root-finding}}{g(x)-x=0}$.
\begin{example}
	$f(x)=x^2$, $\alpha-0$ \ul{root}.
	\begin{align*}
		f(x) &=x \\
		x^2 &=x \\
		x(x-1) &=0 \\
		x &= 0,1
	\end{align*}
	$\beta=0,1$ fixed points.
\end{example}

Relationship to root-finding:
\begin{align*}
	f(x) &=0 \\
	f(x)+x-x &= 0 \\
	\ub{g(x)}{f(x)+x)} &= x
\end{align*}
$x=\alpha$ is a zero of $f(x)$ if and only if $x=\alpha$ is a fixed point of $g(x) = f(x)+x$.
Can go from $f(x)=0$ to $g(x)=\alpha$.
Non-unique:
\begin{example}
	$f(x) = 3x^2-2x=0$. $\ub{g(x)}{3x^3-x}=x$.
	\begin{align*}
		3x^3 &= 2x, \qquad
		x^3 = \frac23x, \qquad
		x = \ub{g(x)}{\nrt3{\frac23x}}
	\end{align*}
\end{example}

Fixed-Point Iteration/Solver:

Want $x=g(x)$

Choose $x_0$.

Let $x_{n+1} = g(x_n)$ for $n=0,1,2,\dots$

(hopefully $x_n\ra\beta$)

\begin{example}
	$f(x)=0$. Apply Newton's Method $$x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}.$$
	This is a fixed-point iteration for $g(x) = x-\frac{f(x)}{f'(x)}$.
	Observe; $x = x-\frac{f(x)}{f'(x)}$, $0 = -\frac{f(x)}{f'(x)} \implies f(x)=0$ \ul{if} $f'(x)\neq0$.
	\begin{enumerate}
		\item[$*$] We can analyse root-finding methods with the form $x_{n+1}=g(x_n)$.
	\end{enumerate}
\end{example}
\begin{theorem}
	Let $g\in \Cpb{}ab$ with $a\leq g(x)\leq b$ for all $a\leq x\leq b$. Then
	\begin{enumerate}
		\item $g$ has at least one fixed-point in $[a,b]$.
		\item If there exists $\gamma<1$ such that $|g(x)-g(y)|\leq \gamma|x-y|$ for all $x,y$ in $[a,b]$, then
			\begin{enumerate}
				\item[a)] the fixed-point $\alpha$ is unique
				\item[b)] The iteration $x_{n+1}=g(x_n)$ converges to $\alpha$ for any $x_0$ in $[a,b]$.
				\item[c)] $|\alpha-x_n|\leq \frac{\gamma^n}{1-\gamma}|x_1-x_0|$.
			\end{enumerate}
		\item If $g\in\Cpb1ab$ with $x\in\max_{[a,b]}|g'(x)| = \gamma<1$. Then,
			\begin{enumerate}
				\item[a)] $\alpha$ is unique
				\item[b)] $x_{n+1} = g(x_n) \ra\alpha$ for any $x_0\in[a,b]$.
				\item[c)] $|\alpha-x_n| \leq \frac{\gamma^n}{1-\gamma} |x_1-x_0|$.
				\item[d)] $\lim_{n\ra\infty} \frac{\alpha-x_{n+1}}{\alpha-x_n} = g'(\alpha)$.
			\end{enumerate}
		\item[$*$] Yields a local convergence theorem.
			\begin{itemize}
				\item verify $a\leq g(x)\leq b$ for all $x\in[a,b]$ where $\alpha\in[a,b]$ for some $a,b$.
				\item Verify $|g'(x)|\leq\gamma<1$ for all $a\leq x\leq b$.
			\end{itemize}
			(Find $a,b$)
	\end{enumerate}
\end{theorem}
\begin{example}
	Let $g(x) = \frac12\cos(x)$, $[a,b] = [0,\frac12]$.
	$\alpha = \frac12\cos(\alpha)$.
	\begin{itemize}
		\item $g(x) = \frac12\cos(x)$, $0\leq\frac12\cos(x)\leq\frac12$ for $0\leq x\leq\frac12$. $\checkmark$
		\item $g'(x) = -\frac12\sin(x)$. $|g'(x)|\leq\frac12=\gamma<1$. $\checkmark$
	\end{itemize}
	$\implies \alpha$ exists such that $\alpha = g(\alpha) = \frac12\cos(\alpha)$.
	Letting $x_{n+1}=g(x_n)$ and $x_0\in[0,\frac12]$, we have $x_n\ra\alpha\approx 0.450184$.
	How many iterations are needed to guarantee $|x_n-\alpha|<\text{TOL}$ for $x_0=0$?
	\begin{align*}
		|\alpha - x_n| \leq \frac{\gamma^n}{1-\gamma}|x_1-x_0|.
	\end{align*}
	$\gamma=\frac12$. $x_1 = g(x_0) = \frac12\cos(0) = \frac12$.
	$$ \frac{\gamma^n}{1-\gamma}|x_1-x_0| = \frac{\plr{\frac12}^n}{1/2}\frac12 = \frac1{2^n}.$$
	$\frac1{2^n} < \text{TOL}$, $2^n > \frac1{\text{TOL}}$, $n>\log_2\plr{\frac1{\text{TOL}}}$.
	$ x-\frac12\cos x=0$.
\end{example}

\begin{theorem}
	[rate] $x_{n+1} = g(x_n)$.
	Suppose $g\in\Cpb pab$ with $\alpha\in[a,b]$, $g(\alpha)=\alpha$.
	If $g'(\alpha)=g''(\alpha)=\dots=g^{(p-1)}(\alpha)=0$ but $g^{(p)}(\alpha)\neq0$, then $x_n\ra\alpha$ with order $p$ for $x_0$ sufficiently close to $\alpha$.
\end{theorem}

\begin{example}
	Show Newton's method has rate at least 2 for $f'(\alpha)\neq0$, $f(\alpha)=0$.
	$$ g(x) = x-\frac{f(x)}{f'(x)} $$
	Then \begin{align*}
		g'(x) &= 1- \frac{[f'(x)]^2-f(x)f''(x)}{[f'(x)]^2}
			  = \frac{[f'(x)]^2-[f'(x)]^2+f(x)f''(x)}{[f'(x)]^2}
			  = \frac{f(x)f''(x)}{[f'(x)]^2}. \\
		g'(\alpha) &= \frac{f(\alpha)f''(\alpha)}{[f'(\alpha)]^2} = 0, \qquad \text{(assuming $f''$ okay)}
	\end{align*}
\end{example}

\ul{Review}:
Solve $f(x)=0$ -- root finding.
or solve $g(x)=x$ -- fixed point problem.
To relate the two ideas, $g(x)=x+f(x)$.
Start with $x_0$: update $x_{n+1} = g(x_n)$.
$g(x) = x-\frac{f(x)}{f'(x)}$.
\begin{theorem}
	Let $g\in \Cb{}ab$, $a\leq g(x)\leq b$ for all $x\in[a,b]$.
	There is a $\gamma<1$ such that $$|g(x)-g(y)|\leq\gamma|x-y|.$$
	\begin{enumerate}
		\item unique $\alpha$ such that $g(\alpha)=\alpha$.
		\item The iteration of $x_{n+1}=g(x_n)$ converges to $\alpha$ for any $x_0[a,b]$.
		\item $|\alpha-x_n|\leq \frac{\gamma^n}{1-\gamma}|x_1-x_0|$.
	\end{enumerate}
	If $g\in\Cb1ab$ with $\max_x|g'(x)| = \gamma<1$ then
	$$ \begin{rcases} \alpha \text{ is unique} \\ \text{convergence} \\ \text{distance} \end{rcases} + \lim_{n\ra\infty} \frac{\alpha-x_{n+1}}{\alpha-x_n} = g'(\alpha) $$
\end{theorem}

\subsection*{\S3.11 Special Topics}
\subsubsection*{\S3.11.4 Multiple Roots}
Suppose $f'(\alpha)=0$

\begin{lemma}
	If $f\in\Cb kab$ (derivatives up to $k-1$) are all 0 at $x$, $f(\alpha) = f'(\alpha) = \dots = f^{(k)}(\alpha) = 0$ and $f^{(n)}(\alpha)\neq0$, then $f(x) = (x-\alpha)^k F(x)$, $F(\alpha)\neq0$.
\end{lemma}
\begin{example}
	Suppose $f(x) = (x-\alpha)^2F(x)$, $F(\alpha)\neq0$, with 
	\begin{align*}
		g(x) &= x-\frac{f(x)}{f'(x)} \\
			 &= x-\frac{(x-\alpha)^2f(x)}{2(x-\alpha)F(x) + (x-\alpha)^2F'(x)} \\
			 &= x-\frac{(x-\alpha)F(x)}{2F(x)+(x-\alpha)F'(x)} \\
		g'(x) &= 1- \frac{[f(x)-(x-\alpha)F'(x)][\text{Denom}] - (x-\alpha)F(x)[\text{Denom}]'}{[\text{Denom}]^2} \\
			  &= 1- \frac{F(\alpha) \text{Denom}[\alpha]}{\text{Denom}[\alpha]^2} \\
			  &= 1- \frac{F(\alpha)}{\text{Denom}[\alpha]} = 1 - \frac{F(\alpha)}{2F(\alpha)} = 1-\frac12 = \fbox{$\frac12$} \neq 0
	\end{align*}
\end{example}

\ul{Modified Newton's Method}

Let $u(x) = f(x)/f'(x)$.
$u(x)$ has root $\alpha$ with multiplicity 1.
Apply Newton's method to $u(x)$ to find $\alpha$.
\begin{align*}
	f(x) &= (x-\alpha)^k F(x) \\
	u(x) &= \frac{(x-\alpha)^k F(x)}{k(x-\alpha)^{k-1}F(x) + (x-\alpha)^kF'(x)} \\
		 &= \frac{x-\alpha)F(x)}{kF(x) + (x-\alpha)F'(x)} \\
	u(\alpha) &= \frac0{kF(\alpha)} = 0 \\
	u'(x) &= \text{(same thing as above)} \\
	u'(\alpha) &= \frac{F(\alpha)}{\text{Denom}(\alpha)} = \frac{F(\alpha)}{kF(\alpha)} = \frac1k \neq0 \\
	x_{n+1} &= x-\frac{u(x_n)}{u'(x_n)} \qquad \text{converges to }\alpha
\end{align*}

\subsubsection*{\S3.11.5 Hybrid Algorithm}

\ul{Goal}: Combine the Bisection Method with the Secant Method.

\ul{Idea}: $k$th interval $[a_k,b_k]$ either $x_k=a_k$ or $b_k$, and $x_{k-1} = b_k$ or $a_k$.

Let $f(a_1)f(b_1)<0$. Set $x_0=a_1$ and $x_1=b_1$.
For $k=1,2,3,\dots$ \ul{Secant Method}.
$$ c = x_k - f(x_k)\cdot\frac{x_k-x_{k-1}}{f(x_k)-f(x_{k-1})} $$

If $a_k\leq c\leq b_k$ then update brackets.

If $f(a_k)f(c)<0 \implies \begin{cases} a_{k+1} &= a_k \\ b_{k+1} &=c \end{cases}$ otherwise $[f(b_k)f(c)<0] \implies \begin{cases} a_{k+1} &= c \\ b_{k+1} &= b_k \end{cases}$, $x_{k+1}=c$.

If secant method doesn't work, use bisection.
($c$ is outside the interval) $c = a_k+\frac12(b_k-a_k)$.

If $f(a_k)f(c) < 0 \implies \begin{cases} a_{k+1} &= a_k \\ b_{k+1} &= c \end{cases}$ otherwise $\implies \begin{cases} a_{k+1} &= c \\ b_{k+1} &= b_k \end{cases}$, $x_{k+1}=c$.

\section*{Interpolation + Approximation}

\ul{Goal}: find a simpler function $p(x)$ to approximate $f(x)$.
Set of data, $x_i, y_i=f(x_i)$, info about derivatives.

\subsection*{\S4.1 Lagrange Interpolation}

\ul{Goal}: Given $n+1$ data values $y_i = f(x_i)$ for distinct nodes $\set{x_i}_{i=1}^n$, find a polynomial of degree $\leq n$ such that $p_n(x_i)=y_i$ for all $i=0,1,\dots,n$.

\begin{example}
	$f(x) = |x|$. $x_0=-1$, $x_1=0$, $x_2=1$. Find $p_2(x)$.
	Guess $p_2(x) = x^2$.
	\begin{align*}
		p_2(x) &= a_0 + a_1x + a_2x^2 \\
		1 &= f(-1) = p_2(-1) = a_0 - a_1 + a_2 \\
		0 &= f(0) = p_2(0) = a_0 \\
		1 &= f(1) = p_2(1) = a_0 + a_1 + a_2
	\end{align*}
	So \ul{$a_0 = 0$},
	\begin{align*}
		- a_1 + a_2 &= 1 \\
		+ a_1 + a_2 &= 1 \\
		\implies 2a_2 &= 2
	\end{align*}
	\ul{$a_2=1$},
	\begin{align*}
		1 &= 0 + a_1 + 1 \\
		1 &= a_1 + 1
	\end{align*}
	\ul{$a_1 = 0$}.
	$\implies p_2(x) = x^2$.	
\end{example}

\ul{Alternative Construction}: Use Lagrange Basis Functions.
$ L_i^{(n)}$ for $p_n$ so that $$ p_n(x) = \sum_{i=0}^n \us{\us{\text{polynomial}}{\text{degree }n}}{L_i^{(n)}(x)} \cdot \us{\us{\text{given}}{\text{constants}}}{f(x_i)} $$ $\implies p_n$ degree at most $n$,
where 
\begin{align*}
	L_i^{(n)}(x) &= \prod_{\us{k\neq i}{k=0}}^n \frac{x-x_k}{x_i-x_k}
				 = \frac{x-x_0}{x_i-x_0}\cdot\frac{x-x_1}{x_i-x_1}\cdot \dots \cdot \frac{x-x_{i-1}}{x_i-x_{i-1}} \cdot \frac{x-x_{i+1}}{x_i-x_{i+1}} \cdot \dots \cdot \frac{x-x_n}{x_i-x_n}
\end{align*}
product of $n$ linear terms $\la x_i\neq x_k$ don't divide by zero.
\begin{observe}
	\begin{align*}
		L_i^{(n)}(x_i) &=\prod_{\us{k\neq i}{k=0}}^n \frac{x_i-x_k}{x_i-x_k} = 1^n = 1.\\
		\us{j\neq i}{L_i^{(n)}(x_j)} &= \prod_{\us{k\neq i}{k=0}}^n \frac{x_j-x_k}{x_i-x_k} = \frac{()}{()} \cdot \dots \cdot \frac{()}{()} \cdot \frac{x_j-x_j}{x_i-x_j} \cdot \frac{()}{()} \cdot \dots \cdot \frac{()}{()} = 0.
	\end{align*}
	Thus, check: $p_n(x_k) = f(x_k)$
	\begin{align*}
		p_n(x_k) &= \sum_{i=0}^n L_i^{(n)} (x_k) f(x_i) \\
				 &= L_k^{(n)}(x_k) f(x_k) \qquad (0 \text{ for }i\neq k) \\
				 &= 1\cdot f(x_k)
				 = f(x_k) \ \checkmark
	\end{align*}
\end{observe}

In the example:
\begin{align*}
	L_0^{(2)}(x) &= \prod_{k=1}^2 \frac{x-x_k}{x_0-x_k} = \frac{x-x_1}{x_0-x_1}\cdot \frac{x-x_2}{x_0-x_2}
				 = \frac{x-0}{-1-0} \cdot \frac{x-1}{-1-1}
				 = \frac{x(x-1)}{(-1)(-2)} = \frac12x(x-1). \\
	L_1^{(2)} &= \prod_{k=0,2} \frac{x-x_k}{x_1-x_k}
			  = \frac{x-x_0}{x_1-x_0}\cdot \frac{x-x_2}{x_1-x_2}
			  = \frac{x+1}{1-0}\cdot \frac{x-1}{0-1}
			  = \frac{(x+1)(x-1)}{-1}
			  = -(x+1)(x-1) \\
\end{align*}
Check: $L_0^{(2)}(\os{x_0}{-1}) = 1$, $L_0^{(2)}(x_1)=0$, $L_0^{(2)} (x_2) = 0$.

Cheat: $L_2^{(2)}(x) = a(x+1)x$. Need $1=L_2^{(2)}(\os{x_2}1) = a(2)(1)$, so $a = \frac12$.
Then $L_2^{(2)}(x) = \frac12 x(x+1)$.
\begin{align*}
	p_2(x) &= \sum_{i=0}^2 L_i^{(2)}(x)f(x_i)
		   = +1 \cdot L_0^{(2)}(x) + 0\cdot L_1^{(2)}(x) + 1\cdot L_2^{(2)}(x) \\
		   &= \frac12x(x-1)+\frac12x(x+1)
		   = \frac12x^2-\frac12x + \frac12 x^2 + \frac12 x
		   = x^2.
\end{align*}

\begin{theorem}
	[Lagrange Interpolation Existence + Uniqueness]
	Let $x_i\in I$, $0\leq i\leq n$, with $x_i\neq x_j$ for all $i\neq j$.
	There exists a unique polynomial of degree $\leq n$ such that $p_n(x_i)=f(x_i)$ for a given fun. $f\in C(I)$.
\end{theorem}
\begin{proof}
	\ul{Existence}: $p_n(x) = \sum_{i=0}^n L_i^{(n)}(x)f(x_i)$.

	\ul{Uniqueness}: Assume there exists another polynomial $q(x)$ such that $q(x_i) = f(x_i)$ for all $i=0,\dots,n$.
	Let $r(x) = \ub{\text{deg}\leq n}{p_n(x)} - \ub{\text{deg}\leq n}{q(x)} \implies r$ has degree $\leq n$.
	Then $r$ has degree $\leq n$ \ul{\ul{and}}
	\begin{align*}
		r(x_i) &= p_n(x_i) - q(x_i)
			   = f(x_i) - f(x_i)
			   = 0
	\end{align*}
	for all $i=0,1,\dots,n \implies r(x)$ has $n+1$ zeros.
	Therefore $r(x)=0$ since degree $n$ polynomials have at most $n$ distinct zeros unless they are the zero for (Fundamental Theorem of Algebra)
	$r(x) = 0 \implies p_n(x) - q(x) = 0$ implies $p_n(x) = q(x) \implies p_n$ is unique.
\end{proof}
\begin{note}:
	\begin{enumerate}
		\item For $n$ large, $p_n(x)$ is hard to compute numerically.
		\item $n\ra\infty$ does not guarantee $p_n(x)\approx f(x)$ even for some \say{nice} functions $f(x)$.
	\end{enumerate}
\end{note}

\subsubsection*{\S4.2 Newton Interpolation and Divided Difference}

\ul{Goal}: Efficiently construct Lagrange Interpolation polynomials.
The method allows a partial factoring similar to Horner's method.

\begin{theorem}
	Let $p_n(x)$ interpolate $f$ at the nodes $x_i$, $i=0,1,2,\dots,n$.
	$p_{n+1}(x)$ interpolate $f$ at $x_i$, $i=0,1,2,\dots,n+1$. $x_i\neq x_j$.
	Then $p_{n+1}(x) = p_n(x) + a_{n+1}\omega_n(x)$, where
	\begin{align*}
		\omega_n(x) &= \prod_{i=0}^n (x-x_i) \\
		a_{n+1}(x) &= \frac{f(x_{n+1})-p_n(x)}{\omega_{n+1}(x)} \\
		p_0(x) &= a_0 = f(x_0)
	\end{align*}
	\begin{enumerate}
		\item[$*$] constructs $p_{n+1}$ given $p_n$ when 1 extra node is added.
		\item[$*$] can be implemented recursively.
		\item[$*$] avoids repetitive computations: $w_{n+1} = (x-x_{n+1})\omega_n(x)$.
	\end{enumerate}
\end{theorem}
\begin{corollary}
	Given $\set{a_k}$ and $\omega_n$,
	\begin{align*}
		p_n(x) &= a_0 + a_1(x-x_0) + a_2(x-x_0)(x-x_1) + \dots + a_n(x-x_0)(x-x_1)\dots(x-x_{n-1}) \\
			   &= \sum_{k=0}^n a_k\omega_{k-1}(x).
	\end{align*}
	for $\omega_{-1}(x)\equiv1$.
	\begin{align*}
		p_n(x) = a_0 + (x-x_0)\blr{a_1 + (x-x_1)\blr{a_2 + \dots + (x-x_{n-1})a_n}} \sim \text{Horner's}
	\end{align*}
	\begin{enumerate}
		\item[$*$] $\set{a_k}_{k=0}^n$ called \ul{divided differences} and are used to evaluate $p_n(x)$.
	\end{enumerate}
\end{corollary}
Alg. 4.1 constructs $\set{a_k}$.

Alg. 4.2 evaluated $p_n(xx)$ given $\set{x_k}$.
$\omega_n$ built/included in the algorithm using the partial factoring/Horner's approach.
$X = \set{x_i}_{i=0}^n$ fixed nodes for defining $p_n$.

\newpage
\begin{example}
	[Constructing $p_n(x)$ by hand] via divided difference tables.
	Find $p_2(x)$ for $f(x)=\log_2x$, $x_0=1,x_2=2,x_3=4$.
\end{example}

	Define for $j\geq1$: 
	\begin{align*}
		\us{\fbox{$(j,k)$}}{f_j(x_k)} &\equiv \frac{f_{j-1}(x_{k+1})-f_{j-1}(x_k)}{x_{k+j}-x_k}
	\end{align*}

	Insert Table from class!!!

	\begin{align*}
		f_1(x_0) &= \frac{f_0(x_1)-f_0(x_0)}{x_1-x_0} = \frac{f(2)-f(1)}{2-1} = \frac11 = 1. \\
		f_1(x_1) &= \frac{f_0(x_2)-f_0(x_1)}{x_2-x_1} = \frac{f(4)-f(2)}{4-2} = \frac{2-1}2 = \frac12. \\
		f_2(x_0) &= \frac{f_1(x_1)-f_1(x_0)}{x_2-x_0} = \frac{\frac12-1}{4-1} =-\frac16.
	\end{align*}
	$f_2(x_1)$ not defined. So $x_{2+1}=x_3$.
	\begin{align*}
		p_2(x) &= \ul{\ul{f_0(x_0)}} + \ul{\ul{f_1(x_0)}}(x-x_0) + \ul{\ul{f_2(x_0)}}(x-x_0)(x-x_1) \\
			   &= 0 + 1\cdot(x-1)+ \plr{-\frac16}(x-1)(x-2) \\
			   &= (x-1)\blr{1+(x-2)\plr{-\frac16}} \qquad \text{evaluate} \\
			   &= -\frac16(x-1)(x-2) \\
		p_2(x) &= f_0(x_2) + f_1(x_1)(x-x_2) + f_2(x_0)(x-x_2)(x-x_1) \\
			   &= 2 + \frac12(x-4) - \frac16(x-4)(x-2) \\
			   &= -\frac16(x-1)(x-8).
	\end{align*}
\begin{example}
	[continued] Now interpolate at $x_0=1,x_1=2,x_2=4,x_3=8$.
	\begin{align*}
		f_1(x_2) &= \frac{f_0(x_3)-f_0(x_2)}{x_3-x_2} = \frac{3-2}{8-4} = \frac14. \\
		f_2(x_1) &= \frac{f_1(x_2)-f_1(x_1)}{x_3-x_1} = \frac{\frac14-\frac12}{8-2} = -\frac1{24}. \\
		f_3(x_0) &= \frac{f_2(x_1)-f_2(x_0)}{x_3-x_0} = \frac{-\frac1{24}+\frac16}{8-1} = \frac1{56}. \\
		p_3(x) &= f_0(x_0) + f_1(x_0)(x-x_0) + f_2(x_0)(x-x_0)(x-x_1) + f_3(x_0)(x-x_0)(x-x_1)(x-x_2) \\
			   &= p_2(x) + f_3(x_0)(x-x_0)(x-x_1)(x-x_2) \\
			   &= p_2(x) + \frac1{56}(x-1)(x-2)(x-4) \\
			   &= \frac1{168}(x-1)(3x^2-46x+248) \\
			   &= \frac1{168}(x-1)(248+x(3x-46)) \\
			   &= \frac1{56}x^3 - \frac7{24}x^2 + \frac74x - \frac{31}{21}.
	\end{align*}
\end{example}

\subsection*{\S4.3 Interpolation Error}

\ul{Goal}: Bound the error over the entire interval.

\begin{theorem}
	Let $f\in C^{n+1}([a,b])$, and let the nodes $x_k\in[a,b]$ for $0\leq k\leq n$ with $x_i\neq x_j$.
	Then for each $x\in[a,b]$, there exists $\xi_x$ in $[a,b]$ such that $f(x)-p_n(x) = \frac{\omega_n(x)}{(n+1)!}f^{(n+1)}(\xi_x)$, where $\omega_n(x) = \prod_{k=0}^n (x-x_k)$.
\end{theorem}
\begin{corollary}
	$\ds \max_{x\in[a,b]} |f(x)-p_n(x)| \leq \frac{\ds\max_{x\in[a,b]}|\omega_n(x)|}{(n+1)!} \max_{x\in[a,b]} |f^{(n+1)}(x)|$.
\end{corollary}
\begin{note}:
	\begin{enumerate}
		\item The error can blow-up since $f^{(n+1)}(x)$ may become unbounded (especially for $n$ large).
		\item Chebyshev Interpolation: choose $x_k$ to minimize $|\omega_n(x)|$.
			The nodes are non-uniformly spaced.
		\item $|\omega_n(x)| \leq (b-a)^{n+1}$ since $x,x_k\in[a,b]$.
			Fact: $\lim_{n\ra\infty} \frac{c^n}{n!} = 0$ for any fixed real number $c$.
	\end{enumerate}
\end{note}
\begin{example}
	What is the error for cubic interpolation to $f(x)=\sqrt x$ on the interval $[1,4]$ using equally spaced nodes?
	\begin{align*}
		\omega_n(x) &= (x-x_0)(x-x_1)(x-x_2)(x-x_3) \\
					&= (x-1)(x-2)(x-3)(x-4) \\
		\max_{1\leq x\leq 4} \omega_3(x) &= \frac9{16} \qquad\text{at }x=\frac52 \\
		\min_{1\leq x\leq 4} \omega_3(x) &= -1 \qquad \text{at } x=\frac52\pm\frac{\sqrt5}2.\\
		\implies \max_{1\leq x\leq 4} |\omega_3(x)| &= +1 \\
		f^{(4)}(x) &= -\frac{15}{16} \cdot \frac1{\sqrt{x^7}} \\
		\max_{1\leq x\leq 4} |f^{(4)}(x)| &= +\frac{15}{16}\cdot\frac1{\sqrt{1^7}} = \frac{15}{16}. \\
		\implies f(x)-p_3(x) &\leq \ob{|\omega_3|}{\frac1{4!}}\cdot\ob{f^{(4)}}{\frac{15}{16}} = \frac5{128} \approx 0.039.
	\end{align*}
\end{example}
\begin{example}
	If we want to use a table of values to interpolate the sine function on the interval $[0,\pi]$, how many points are needed for $10^{-6}$ accuracy with linear interpolatoin (piecewise).
	$f^{(2)}(x) = -\sin(x) \implies |f^{(2)}(x)| \leq 1$.
	On the $k^\text{th}$ subinterval:
	\begin{align*}
		f(x)-p_1(x) &= \frac{\omega_1(x)}{2!} f^{(2)}(\xi_x) \\
		\omega_1(x) &= (x-x_{k-1})(x-x_1)
	\end{align*}
	On the $k^\text{th}$ subinterval $[x_{k-1},x_k]$:
	\begin{align*}
		\omega_1(x) &= (x-x_{k-1})(x-x_k) \\
					&\leq \plr{\frac{x_{k-1}+x_k}2-x_{k-1}}\mlr{\frac{x_{k-1}+x_k}2-x_k}\\
					&= \plr{\frac{x_k-x_{k-1}}2}\mlr{\frac{x_{k-1}-x_k}2} = \frac{h^2}5\\
		\implies |f(x)-p_1(x)| &\leq \frac{h^2}4\cdot\frac1{2!} = \frac{h^2}8.
	\end{align*}
	Want $\frac{h^2}8 \leq \text{TOL} = 10^{-6}$
	\begin{align*}
		h &\leq \sqrt{8\cdot10^{-6}} = \frac1{250\sqrt2} \approx 0.00283.\\
		\pi = nh \implies n &= \frac\pi h \geq \frac\pi{(1/250\sqrt2)} = 250\pi\sqrt2 \approx 1110.72.
	\end{align*}
	Need at least 1,111 points.
\end{example}
\begin{example}
	[continued] cubic interpolation?
	\begin{align*}
		\omega_3(x) &= (x-x_k)(x-x_{k+1})(x-x_{k+2})(x-x_{k+3}) \\
		\max \omega_3\plr{\frac{x_k+x_{k+3}}2} &= \plr{\frac{3h}2}\plr{\frac h2}\plr{-\frac h2}\plr{-\frac{3h}2} = \frac9{16}h^4 \\
		\max \omega_3\plr{\frac{x_k+x_{k+1}}2} &= \plr{\frac h2}\plr{-\frac h2}\plr{-\frac{3h}2}\plr{-\frac{5h}2} = -\frac{15}{16}h^4.
		\frac{5h^4}{128} \leq 10^{-6} \\\implies h^4 &\leq \frac{128}510^{-6} \\
		h &\leq \nrt4{\frac{128}{5\cdot10^6}} = \frac{\nrt4{10}}{25} \approx 0.07113 \\
		\pi = nh \implies n &= \frac\pi h\geq \frac{25\pi}{4\sqrt{10}} \approx 44.17
	\end{align*}
	Need at least 45 points.
\end{example}

\subsection*{\$4.6 Hermite Interpolation}

\ul{Goal}: Interpolate derivative data in addition to function values.

\begin{theorem}
	Given the nodes $x_i$, $1\leq i\leq n$, and a differentiable function $f(x)$, if the nodes are distinct, then there exists a unique polynomial $H_n$ of degree $\leq 2n-1$ such that $H_n(x_i) = f(x_i)$, $H'_n(x_i) = f'(x_i)$, $1\leq i\leq n$.
\end{theorem}
\begin{proof}
	(idea) Define basis functions $h_k,\tilde h_k$ polynomials with degree $2n-1$ such that
	\begin{align*}
		\begin{cases} h_k(x_j) = \begin{cases} 1 & \text{if }j=k \\ 0 & \text{if }j\neq k \end{cases} \\ h_k'(x_j) = 0 \end{cases}, &\qquad
		\begin{cases} \tilde h_k(x_j) = 0 \\ \tilde h_k'(x_j) = \begin{cases} 1 & \text{if }j=k \\ 0 & \text{if } j\neq k \end{cases} \end{cases}.
	\end{align*}
	Then, $\ds H_n(x) = \sum_{k=1}^n \blr{f(x_k) h_k(x) + f'(x_k)\tilde h_k(x)}$.
\end{proof}
\ul{HW}: $h_k(x) = \set{1-2\blr{L_k^{(n)}}(x_k)(x-x_k)}\blr{L_k^{(n)}(x)}^2$, $\tilde h_k(x) = (x-x_k)\blr{L_k^{(n)}(x)}^2$.

\begin{theorem}
	[Error] Let $f\in\Cpb{2n}ab$ and let the nodes $x_k\in[a,b]$ for all $k$, $1\leq k\leq n$, with $x_i\neq x_j$ for $i\neq j$.
	Then, for each $x\in[a,b]$, $\exists\xi_x\in[a,b]$ such that $$f(x) - H_n(x) = \frac{\psi_n(x)}{(2n)!}f^{(2n)}{\xi_x}$$ where $$\psi_n(x) = \prod_{k=1}^n (x-x_k)^2.$$
\end{theorem}
\begin{note}:
	\begin{enumerate}
		\item If $|f^{(m)}|$ bounded for all $M$, error $\ra0$ as $n\ra\infty$. Otherwise the error may diverge similar to Lagrange interpolation.
		\item Fitting $2n$ data points: $f(x_k)$ and $f'(x_k) \implies$ need a degree $2n-1$ poly. (no node $x_0$ unlike Lagrange).
		\item Can be generalized to fit higher order derivatives. \# derivatives at each point can vary.
		\item Given 2 nodes $x_1=a,x_2=b$, can fit a cubic polynomial $H_2$ with
			$$ |f(x)-H_2(x)| \leq \frac{(b-a)^4}{384} \max_{a\leq\xi\leq b} |f^{(4)}(\xi)|$$ for all $a\leq x\leq b$.
	\end{enumerate}
\end{note}
\begin{example}
	Construct $H_2(x)$ for $f(x) = \cos(x)$ on the interval $[0,\pi]$.
	Idea: use divided differences. Count each node twice.	
	[Difference table]
	\begin{align*}
		f_0(x_k) &= f(x_k) = \cos(x_k) \\
		f_1(x_k) &= f'(x_k) = -\sin(x_k) \\
		f_1(x_{1,2}) &= \frac{f(x_2)-f(x_1)}{x_2-x_1} = \frac{\cos(\pi)-\cos(0)}{\pi-0} = \frac{-2}\pi \\
		f_2(x_1) &= \frac{f_1(x_2)-f'(x_1)}{x_2-x_1} = \frac{-\frac2\pi+\sin(x_1)}\pi = -\frac2{\pi^2} \\
		f_2(x_2) &= \frac{f'(x_2)-f_1(x_{1,2})}{x_2-x_1} = \frac2{\pi^2} \\
		f_2(x_{1,2}) &= \frac{f_2(x_2)-f_2(x_1)}{x_2-x_1} = \frac{\frac2{\pi^2}-\plr{-\frac2{\pi^2}}}\pi = \frac4{\pi^3}. \\
		H_2(x) &= f_0(x_1) + f_1(x_1)\cdot(x-x_1) + f_2(x_1)(x-x_1)^2 + f_3(x_{1,2})(x-x_1)^2(x-x_2) \\
		&= 1 + 0(x-0) + \plr{-\frac2{\pi^2}}(x-0)^2 + \plr{\frac4{\pi^2}}(x-0)^2(x-\pi) \\
		&= 1-\frac2\pi x^2 + \frac4{\pi^2}x^2(x-\pi).
	\end{align*}
	\begin{remark}
		This approach is not used for Finite Element.
	\end{remark}
	\ul{Max Error} (Theorem) $f^{(4)}(x) = \sin(x)$.
	$\ds \frac{(\pi-0)^4}{384} \max_{0\leq\xi\leq\pi} |f^{(4)}(\xi)| = \frac{\pi^4}{384} \approx 0.0257\times\pi^2$.

	\ul{Max Error} (Actual) $\approx 0.020017$ at $x\approx 2.26642$.
	$H_2(0) = 1,H'_2(0) = 0$ and $H_2(\pi)=-1,H_2'(\pi)=0$.
\end{example}

Cubic Lagrange is $C^0$, cubic Hermite is $C^1$.
Spline: Glue together piecewise cubic so that the global function is $C^2$.
For a spline $s(x_k) = f(x_k)$ and $s'(x_k)\neq f'(x_k)$, but $s''(x_k)$ is continuous.

\subsection*{\S4.7 Piecewise Polynomial Interpolation}

\ul{Goal}: Use piecewise polynomials of low degree to avoid needing to bound $|f^{(n)}(\xi)|$ for $n$ large in the error result.
Avoids large oscillations caused by high degree polynomials.
\ul{Idea}: piecewise linear: one line on each interval $[x_i,x_{i+1}]$.
piecewise quadratic: one function over two intervals.

\begin{enumerate}
	\item[$*$] for piecewise degree $d$ polynomials, we need $n=dm$ for some integer $M$.
		Each subinterval is width $dh$ and defines $a$ polynimal with degree $\leq d$.
	\item[$*$] piecewise Lagrange: the interpolating function is continuous.
	\item[$*$] piecewise Hermite: $\implies$ interpolant function is $C^1$ (differentiable since avoids corners when gluing pieces together).
\end{enumerate}

\begin{theorem}
	[Error for piecewise degree $d$]
	Let $f\in\Cpb{d+1}ab$ and let $q_d$ be the piecewise polynomial interpolant of degree $d$ to $f$ on $[a,b]$ using $n+1$ equally spaced nodes, where $x_k$, $0\leq k\leq n$, $x_i\neq x_j$ for $i\neq j$, $n=Md$.
	[$q_d(x_k) = f(x_k)$ for all $k$, $q_d$ has degree $\leq d$ on each subinterval].
	Then $$ |f(x)-q_x(x)| \leq C_d h^{d+1} \max_{a\leq\xi\leq b}|f^{(d+1)}(\xi)| $$
	for all $a\leq x\leq b$, where $h=x_k-x_{k-1}$.
\end{theorem}
\begin{fact}
	$C_1 = \frac18$, $C_2 = \frac1{9\sqrt3}$, $C_3 = \frac1{24}$.
\end{fact}
\begin{proof}
	For $x\in[x_{\ell-1},x_\ell]$, $\sim p_k$ interpolates $f$ on $k^\text{th}$ subinterval containing $[x_{\ell-1},x_\ell]$ 
	\begin{align*}
		|f(x)-q_d(x)| &= |f(x)-p_k(x)| \\
					  &= \frac{|\omega_d(x)|}{(d+1)!}|f^{(d+1)}(\xi)| \\
					  &\leq C_d h^{d+1} \max_{a\leq\xi\leq b} |f^{(d+1)}(\xi)|
	\end{align*}
	where $\omega_d = \prod_{k=0}^d(x-x_k) \leq Ch^{d+1}$ for some constant $C$.
\end{proof}
\begin{enumerate}
	\item[$*$] Letting $n\ra\infty$ ($m\ra\infty$ for $n=md$)
		$\implies h\ra0^+$ with $d$ fixed
		$\implies$ error $\ra0$ if $|f^{(d+1)}|$ bounded.
\end{enumerate}

\begin{example}
	Construct the piecewise cubic interpolating polynomial for $f(x) = \log_2(x)$ on the interval $[\frac18,8]$ using the nodes
	$x_0=\frac18,x_1=\frac14,x_2=\frac12,x_3=1,x_4=2,x_5=4,x_6=8$.
	\begin{note}
		$6=n=3\cdot2$, $d=3$, $M=2$. two pieces.
	\end{note}
	\begin{align*}
		p_1(x) &= \frac{64}7x^3 - \frac{56}3x^2 + 14x - \frac{94}{21} \\
		p_2(x) &= \frac1{56}x^3 - \frac7{24}x^2 + \frac74x - \frac{31}{21}.
	\end{align*}
\end{example}
\begin{example}
	Using piecewise cubic interpolation, how many nodes are needed to approximate $\ln(x)$ over $[1,10]$ within $10^{-8}$ error using equally spaced nodes.
	$f(x) = \ln(x)$, $$ f^{(4)}(x) = - \frac6{x^4} \implies \max_{1\leq\xi\leq 10} |f^{(4)}(\xi)| = 6.$$
	\begin{align*}
		C_dh^4\cdot 6 &\leq 10^{-8} \\
		\frac1{24}h^4\cdot6 &\leq 10^{-8} \\
		h^4 &\leq 4\times10^{-8} \\
		h &\leq \frac1{30\sqrt2} \approx 0.01414. \\
		n &= md. \\
		h &= \frac{b-a}n = \frac{10-1}{m3} = \frac3m. \\
		M &\geq \frac3h = 150\sqrt2 \approx 212.13. \\
		M &= 213.
	\end{align*}
	\fbox{$n = 3M = 639$}.
\end{example}

\subsection*{\S4.8 An introduction to splines}

\ul{Goal}: Construct \ul{smooth} piecewise polynomial interpolation functions using only given function values (no derivative information). 

\begin{definition}
	$q_d$ is a spline of degree $d$ if:
	\begin{itemize}
		\item each piece (defined on $[x_{k-1},x_k]$) is a polynomial of degree $\leq d$
		\item $q_d(x_k) - f(x_k)$ for aall $k=0,1,\dots,n$, $x_j\neq x_i$ for $i\neq j$. (interpolation)
		\item $\ds\lim_{x\ra x_k^-} q_d^{(i)}(x) = \lim_{x\ra x_k^+} q_d^{(i)}(x)$ for all $i=0,1,\dots,M$. (smoothness) 
			
			$M$ is the degree of smoothness.
	\end{itemize}
	In practice;
	\begin{enumerate}
		\item $M=d-1$ (cubic $\implies 2$ continuous derivatives) $\sim$ existence
		\item $d$ is odd $\implies$ conditions can be added at \ul{both} $x_0,x_n$ to ensure existence and \ul{uniqueness} of the spline.
	\end{enumerate}
\end{definition}
\begin{example}
	For what value of $k$ is the following a spline function?
	$$ q(x) = \begin{cases} x^2-x^2+kx+1, & 0\leq x\leq1 \\ -x^3+(k+2)x^2-kx+3, & q\leq x\leq2 \end{cases} $$
	$q$ is piecewise cubic with 2 pieces.
	\begin{align*}
		\lim_{x\ra1^-} q(x) &= 1-1+k+1 = k+1. \\
		\lim_{x\ra1^+} q(x) &= -1 (k+2)-k+3 = 4. \\
							&\implies k+1 = 4 \qquad \text{for continuity} \\
							&\implies \fbox{$k = 3$}
	\end{align*}
	Check $q'$ and $q''$ (since $d=3$, $M=2$)
	\begin{align*}
		\lim_{x\ra1^-} q'(x) &= \lim_{x\ra1^-} \blr{ 3x^2-2x+k} = 3-2+k = k+1. \\
		\lim_{x\ra1^+} q'(x) &= \lim_{x\ra1^+} \blr{-3x^2+2(k+2)x-k}=-3+2(k+2)-k=k+1 \\
		\lim_{x\ra1^-}q''(x) &= \lim_{x\ra1^-} \blr{ 6x-2} = 6-2=4. \\
		\lim_{x\ra1^+}q''(x) &= \lim_{x\ra1^+} \blr{-6x+2(k+2)}=-6+2k+4=-2+2k. \\
							 & k=3 \implies -2+2(3) = -2+6 = 4. \\
							 & \fbox{$k=3$}
	\end{align*}
	Then $q\in\Cpb202$ and $q$ is cubic on $[0,1]$ and $[1,2]$.
\end{example}
\begin{example}
	Construct a quadratic spline interpolating $(-1,0)$, $(0,1)$, and $(1,3)$.
	$x_0=-1,x_1=0,x_2=1$. 
	$\implies 2$ pieces, $p_1$ on $[-1,0]$ and $p_2$ on $[0,1]$.
	$$ q_2(x) = \begin{cases} p_1(x) = a_1+b_1x+c_1x^2, & -1\leq x<0, \\ p_2(x) = a_2+b_2x+c_2x^2, & 0\leq x\leq1. \end{cases} $$
	\begin{align*}
		p_1(-1) &= 0 \implies 1_1-b_1+c_1 = 0 \\
		p_1(0) &= 1 \implies a_1 = 1 \\
		p_2(0) &= 1 \implies a_2 = 1 \\
		p_2(1) &= 3 \implies a_2+b_2+c_2 = 3
	\end{align*}
	4 equations, 6 unknowns. $a_1=1,a_2=1$.
	\begin{align*}
		p_1'(0) &= p_2'(0) \\
		b_1+2c_1x\big|_{x=0} &= b_2 + 2c_2x\big|_{x=0} \\
		b_1 &= b_2
	\end{align*}
	$\begin{cases} -b_1+c_1 &= -1 \\ b_2+c_2 &= 2 \\ b_1-b_2 &= 0 \end{cases}$ 3 equations, 4 unknowns $\implies$ infinitely many choices.
	
	Suppose $q_2'(-1)=0$ (Arbitrary) $\sim$ best choice: $f'(-1)$ complete spline.
	\begin{align*}
		p_1'(-1) = 0 \implies b_1+2c_1x\big|_{x=-1} = 0 \\
		b_1 - 2c_1 = 0
	\end{align*}
	$$ \begin{cases} -b_1 + c_1 &= -1 \\ b_1-2c_1 &= 0 \\ b_1-b_2 &= 0 \\ b_2 + c_2 &=2 \end{cases}$$
	\begin{align*}
		-c_1 = -1 \\
		\fbox{$c_1=1$} \\
		b_1-2 = 0 \\
		\fbox{$b_1=2$} \\
		2-b_2=0 \\
		\fbox{$b_2=2$} \\
		2+c_2=2 \\
		\fbox{$c_2=0$}
	\end{align*}
	$ q_2(x) = \begin{cases} 1+2x+x^2, & -1\leq x<0 \\ 1+2x, & 0\leq x\leq 1 \end{cases}$
\end{example}

\subsection*{4.8.2 Cubic $B$-splines}

\ul{Goal}: Efficiently construct piecewise cubic splines.
\ul{Idea}: Define basis functions $B_i(x)$ such that $$q_3(x) = \sum_i c_iB_i(x)$$ for some constants $c_i$ [determined by given function values $f(x_k)$].
\begin{enumerate}
	\item[$*$] smoothness built into $B_i(x)$.
\end{enumerate}
Define a single reference function $B(x)$ by
\begin{align*}
	B(x) =
	\begin{cases}
		0 & x\leq -2 \\
		(x+2)^3 & -2<x\leq-1 \\
		1+3(x+1)+3(x+1)^2-3(x+1)^3 & -1<x\leq0 \\
		1+3(1-x)+3(1-x)^2-3(1-x)^3 & 0<x\leq1 \\
		(2-x)^3 & 1<x\leq 2 \\
		0 & x > 2
	\end{cases}
\end{align*}
Then, $B(x)$ is $C^2$, symmetric, and only locally defined.
\begin{align*}
	B(2) &= B(-2) = 0 \\
	B(-1) &= B(1) = 1 \\
	B(0) &= 4 \\
	B'(-2) &= B'(0) = B'(2) = 0 \\
	B'(-1) &= 3 \\
	B'(1) &= -3
\end{align*}
Using translations and scaling:
$B_i(x) \equiv B\plr{\frac{x-x_i}h}$ for $h=x_j-x_{j-1}$ (uniform spacing)
\begin{itemize}
	\item $B_i$ is centered at $x_i$
	\item $B_i$ is zero for $x\notin[x_{i-2},x_{i+2}]$.
\end{itemize}

\subsubsection*{Construction $q_3(x)$ (cubic spline)}

Let $x_i=a+ih$, $h=\frac{b-a}n$.
Given $a = x_0 < x_1 < x_2 < \dots < x_{n-1} < x_n = b$ and $f(x_0),f(x_1),\dots,f(x_n)$.
Introduce extra  nodes
\begin{align*}
	x_{-3} &= a-3h, & x_{-2} &= a-2h, & x_{-1} &= a-h \\
	x_{n+1} &= b+h, & x_{n+2} &= b+2h, & x_{n+3} &= b+3h
\end{align*}
Let \fbox{$\ds q_3(x) = \sum_{i=-1}^{n+1} c_i B_i(x)$} $\sim$ extended because $B_{-1}(x)$ is nonzero for $x$ in $[a,b]$ ($B_{-2}(x)=0$ for all $x\in[a,b]$).
\ul{Goal}: \fbox{$q_3(x_k) = f(x_k)$ for all $0\leq k\leq n$.}
\begin{enumerate}
	\item[$*$] $q_3$ is $C^2$ by construction using $B$-splines.
\end{enumerate}
\begin{align*}
	f(x_k) &= q_3(x_k) \\
		   &= \sum_{i=-1}^{n+1} c_i B_i(x_k) \\
		   &= c_{k-1} B_{k-1}(x_k) + c_k B_k(x_k) + c_{k+1} B_{k+1}(x_k) \\
	(*)    &= c_{k-1} B(-1) + c_k B(0) + c_{k+1} B(1) \\
		   &= c_{k-1} + 4 c_k + c_{k+1}.
\end{align*}
Matrix Form
$$ \mat{
	4 & 1 & & & & \\
	1 & 4 & 1 & & & \\
	  & 1 & 4 & 1 &  &\\
	  &   & \ddots & \ddots & \ddots & \\
	  &   &   &  1 & 4 & 1 \\
	  &   &   &    & 1 & 4} \mat{c_0\\c_1\\c_2\\\vdots\\c_{n-1}\\c_n} = \mat{f(x_0) \fbox{$-c_{-1}$} \\ f(x_1) \\ f(x_2) \\ \vdots \\ f(x_{n-1}) \\ f(x_n)\fbox{$-c_{n+1}$}} $$
\begin{align*}
	\begin{cases}
		n+1 \text{ equations} & \text{(interpolation)} \\
		n+3 \text{ unknowns} & c_{-1} = \, ?,\ c_{n+1} = \, ?
	\end{cases}
\end{align*}
This is a cubic spline for any choice of $c_{-1},c_{n+1}$.
(satisfies the definition)
\begin{question}
	What is the \say{best} choice?
\end{question}
\subsubsection*{Complete Splines}

Assume $f'(x_0)$ and $f'(x_n)$ are given.
\begin{align*}
	f'(x_0) = q'_3(x_0) &= c_{-1} B_{-1}'(x_0) + c_0 B_0'(x_0) + c_1 B_1'(x_0) \\
			  &= c_{-1} \frac1h B'(1) + c_1 \frac1h B'(-1) \\
			  &= -\frac3h c_{-1} + \frac3h c_1. \\
	\implies & c_{-1} = -\frac h3 q'_3(x_0) + c_1
\end{align*}
Choose $q_3'(x_0) = f'(x_0)$.
So \ul{$c_{-1} = -\frac h3 f'(x_0) + c_1$}.
\begin{align*}
	\implies 1^\text{st} \text{ eqn} &: & 4c_0 + c_1 &= f(x_0) - c_{-1} \\
									 &  & &= f(x_0) + \frac h3 f'(x_0) - c_1 \\
									 & & \implies & \fbox{$4c_0 + 2c_1 = f(x_0) + \frac h3 f'(x_0)$} \\
	\text{Last eqn} &: & & \fbox{$ 2c_{n-1} + 4c_n = f(x_n) - \frac h3 f'(x_n)$}
\end{align*}
$\implies n+1$ equations, $n+1$ unknowns $(c_0,\dots,c_n) \implies $ has 1 unique solution.
\begin{enumerate}
	\item[$*$] Finding a complete cubic spline is equivalent to solving an $(n+1)\times(n+1)$ system of linear equations that is diagonally dominant and tri-diagonal (see \S2.6).
\end{enumerate}

\subsubsection*{Evaluation}

Given $c_0,c_1,\dots,c_n$ ($c_{-1},c_{n+1}$), then $\ds q_3(x) = \sum_{i=1}^{n+1} c_i B_i(x)$.
Suppose $x\in[x_{k-1},x_k]$ for some $1\leq k\leq n$.
\begin{align*}
	\implies q_3(x) &= c_{k-2} B_{k-2}(x) + c_{k-1} B_{k-1}(x) + c_kB_k(x) + c_{k+1}B_{k+1}(x).
\end{align*}
For a uniform grid: $k = \lfloor \frac{x-x_0}h \rfloor +1$, round down to nearest integers (floor).

\begin{theorem}
	[Convergence]
	If $f\in\Cpb4ab$ with $\max (x_k-x_{k-1})\leq h$ for all $k$, then for all $a\leq x\leq b$, $$ |f(x)-q_3(x)| \leq \frac5{384}h^4 \max_{a\leq\xi\leq b} |f^{(4)}(\xi)|. $$
	Moreover, $$|f^{(k)}(x)-q_3^{(k)}(x)| \leq C_kh^{4-k}\max_{a\leq\xi\leq b}|f^{(4)}(\xi)|$$ for some constants $C_1,C_2,C_3$.
	\begin{align*}
		\implies |f'(x)-q'(x)| \leq C_1h^3 \max |f^{(4)}(\xi)|
	\end{align*}
\end{theorem}
\begin{note}:
	\begin{enumerate}
		\item Accuracy consistent with piecewise cubic but better approximates derivatives due to smoothness. % of spline.
		\item see Example 4.8 for building $q_3(x)$ using $B$-splines.
		\item The construction with $B$-splines can be generalized/adapted to any degree spline.

			lines are $C^0$, quadratic are $C^1$
		\item If $f'(x_0),f'(x_N)$ are unknown, they can be approximated using finite differences for complete splines.
			Need to be $\mc O(h^4)$.
		\item Common alternative construction finds $q''_3(x)$ by solving a system of equations (tri-diagonal \& diagonally dominant) (called moments) and then integrate twice locally and choose constants based on interpolation constraints. $f(x_{k-1}),f(x_k)$
	\end{enumerate}
\end{note}

\section*{Ch. 5 Numerical Integration}

\ul{Goal}: Approximate $I(f)=\int_a^b f(x)\,dx$.
\ul{Idea}: $\ds I(f) \approx I_n(f) \equiv \sum_{i=0,1}^n \omega_if(x_i)$ for some
\begin{itemize}
	\item nodes/abscissas $x_i$
	\item weights $\omega_i$
\end{itemize}
Then, $\ds\lim_{n\ra\infty} I_n(f) = I(f)$ for $f$ sufficiently nice.
\begin{definition}
	$I_n(f)$ called a \ul{quadrature rule}.
\end{definition}

\begin{example}
	Trapezoidal Rule (\S2.5)
	$$ T_n(f) = \frac h2 \blr{ f(x_0)+2f(x_1)+\dots+2f(x_{n-1})+f(x_n) } $$
	$\set{x_i}_{i=0}^n$ uniformly spaced, $\omega_0 = \frac h2,\omega_n = \frac h2, \omega_i = h$ for $i=1,\dots,n-1$.
	$$ |I(f)-I_n(f)| \leq \frac{b-a}{12}h^2 \max_{a\leq\xi\leq b}|f''(\xi)| \ra 0 $$
	as $h\ra 0$ ($n\ra\infty$) for $h = \frac{b-a}n$. Better method is corrected trapezoidal rule.
\end{example}

\subsection*{\S5.1 A review of the Definite Integral}

\begin{itemize}
	\item Choose $n>0$.
	\item Pick $n-1$ points such that $x_i^{(n)}\in(a,b)$ with $x_i^{(n)}\neq x_j^{(n)}$, $x_0^{(n_)}=a$, $x_n^{(n)}=b$.
	\item Let $h_i^{(n)} = x_i^{(n)}-x_{i-1}^{(n)}$ and $h^{(n)}=\max_i \set{h_i^{(n)}}$.
	\item Define \fbox{$\ds I_n(f,\vec\eta\,^{(n)} = \sum_{i=1}^n f(\eta_i\,^{(n)})h_i^{(n)}$} for some points $\vec\eta\,^{(n)}$ such that $x_{i-1}^{(n)} \leq \eta_i\,^{(n)} \leq x_i^{(n)}$ (arbitrary).
\end{itemize}
\begin{definition}
	[Riemann Integral]
	Suppose $\lim_{n\ra\infty} h^{(n)}=0$.
	Then $C = \int_a^b f(x)\,dx$ exists for a given $a,b,f$ \ul{if} $\ds\lim_{n\ra\infty} I_n(f,\vec\eta\,^{(n)}) = C$ independent of the choices for $\vec\eta\,^{(n)}$ and $\set{x_i^{(n)}}$.
\end{definition}
\begin{example}:
	\begin{itemize}
		\item $L_n(f)$ - left-endpoint rule $\eta_i=x_{i-1}$
		\item $R_n(f)$ - right-endpoint rule $\eta_i=x_i$
		\item $M_n(f)$ - midpoint rule $\eta_i=\frac{x_{i-1}+x_i}2$
	\end{itemize}
	\begin{enumerate}
		\item[$*$] Riemann sums integrate a piecewise constant-valued approximation/interpolant of $f$ exactly, where $p_0(x) = f(\eta_i)$ for all $x_{i-1}<x<x_i$.
		\item[$*$] They are all quadrature rules $\ds I_n(f) = \sum_{i=i_0}^n \omega_i f(x_i)$ for $x_i=\eta_i^{(n)}$, $\omega_i = h_i^{(n)}$, $i_0=1$.
		\item[$*$] converges for $f$ continuous.
		\item[$*$] Typically say it converges slowly due to the inaccuracy of a piecewise \ul{constant} approximation.
	\end{enumerate}
\end{example}

\begin{theorem}
	Suppose a quadrature rule $I_n$ integrates all polynomials of degree $\leq N$ exactly.
	Let $f(x)=p_N(x) + R_N(x)$ for $p_N$ a polynomial of degree $\leq N$ (Ex. Taylor's Remainder Theorem).
	Then $$I(f)-I_n(f) = I_n(R_N)-I_n(R_N).$$
\end{theorem}
\begin{proof}
	\begin{align*}
		I(f)-I_n(f) &= I\plr{p_N+R_N}-I_n\plr{p_N+R_N} \\
					&= I(p_N)+I(R_N) - \us{=I(p_N)}{I_n(p_N)} - I_n(R_N) \\
					&= I(R_N)-I_n(R_N).
	\end{align*}
\end{proof}
\begin{enumerate}
	\item[$*$] To gauge how accurate a given rule $I_n(f)$ is, we see what the largest degree $N$ is such that $I(p)=I_n(p)$ for all polynomials $p$ of degree $\leq N$.
		$L_n,R_n:N=0$. $T_n,M_n:N=1$.
\end{enumerate}

\subsection*{\S5.4 The Midpoint Rule}

\ul{Goal}: show it is more accurate than $L_n,R_n$, and $T_n$.

Riemann sums: $\int_{x_{i-1}}^{x_i} f(x)\,dx \approx \int_{x_{i-1}}^{x_i} f(\eta_i)\,dx = f(\eta_i)(x_i-x_{i-1})$,
Midpoint rule: $\eta_i=\frac{x_{i-1}+x_i}2$.

\begin{observe}
	$f(x) = f(\eta_i)+(\eta_i-x)f'(\eta_i) + \frac12(\eta_i-x)^2f''(\xi_i)$ by Taylor's Theorem.
\end{observe}
\begin{align*}
	\int_{x_{i-1}}^{x_i} f(x)\,dx - \int_{x_{i-1}}^{x_i} f(\eta_i)\,dx
	&= \int_{x_{i-1}}^{x_i} \blr{ \us{(1)}{(\eta_i-x)f'(\eta_i)} + \us{(2)}{\frac12(\eta_i-x)^2f''(\os{\xi_i(x)}{\xi_i})} } \\
	(1) &= f'(\eta_i)\int_{x_{i-1}}^{x_i}(\eta_i-x)\,dx
		= f'(\eta_i)\blr{\eta_ix-\frac12x^2}_{x_{i-1}}^{x_i} \\
		&= f'(\eta_i)\blr{\eta_i(x_i-x_{i-1}) - \frac12\set{x_i^2-x_{i-1}^2} } \\
		&= f'(\eta_i)\blr{\eta_i(x_i-x_{i-1}) - \frac12(x_i-x_{i-1})(x_i+x_{i+1}) } \\
		&= f'(\eta_i)(x_i-x_{i-1})\blr{\eta_i - \frac{x_i+x_{i+1}}2 } \\
		&= \begin{cases} 0 & \text{if } \eta_i = \frac{x_{i-1}+x_i}2, \\ \mc O(h^2) & \text{otherwise}. \end{cases} \\
	(2) &\leq \int_{x_{i-1}}^{x_i} \frac12h^2 \max_{a\leq\xi\leq b} |f''(\xi)|\,dx
		= \frac12h^3\max_{a\leq\xi\leq b} |f''(\xi)|
\end{align*}
$\int_{x_{i-1}}^{x_i} 1\,dx = h$ Integral MVT
$$ (2) = \frac1{24}(x_i-x_{i-1})^3 f''(\xi_i')$$ for some $\xi_i'$.

\begin{align*}
	\int_{x_{i-1}}^{x_i}f(x)\,dx - \int_{x_{i-1}}^{x_i} f(\eta_i)\,dx 
	&= (1) + (2) \\
	(1) &= \begin{cases} 0 & \text{if } \eta_i = \frac{x_{i-1}+x_i}2, \\ \mc O(h^2) & \text{otherwise}. \end{cases} \\
	(2) &= v\frac1{24}(x_i-x_{i-1})^3 f''(\xi_i') \qquad \text{for some }\xi_i' \\
	I(f) - I_n(f) &= \int_a^b f(x)\,dx- \sum_{i=1}^n f(\eta_i)h \\
				  &= \sum_{i=1}^n \int_{x_{i-1}}^{x_i} [f(x)-f(\eta_i)]\,dx = c\sum_{i=1}^n h^3 = ch^3n = ch^2(b-a) \\
				  &\leq \begin{cases} \mc O(h^2) & \text{if } \eta_i = \frac{x_{i-1}+x_i}2 \\ \mc O(h) & \text{otherwise} \end{cases}
\end{align*}
Exact result:
\begin{itemize}
	\item[$M_n:$] $I(f)-I_n(f)=\frac{b-a}{24} h^2 f''(\xi)$ for some $a\leq\xi\leq b$.
	\item[$T_n:$] $T(f)-T_n(f)=\frac{b-a}{12} h^2 f''(\xi)$
	\item[$*$] $M_n$ is typically considered more accurate than $L_n,R_n,T_n$.
	\item $L_n,R_n$ exact for constant functions.
	\item $I_n,M_n$ are exact for linear functions.
	\item $M_n$ uses a piecewise constant interpolant, but is exact for linear all functions.
	\item[$*$] $\exists$ integral fules more accurate than $M_n$ despite having the same form $I_n = \sum_{i=1}^n \omega_i f(x_i)$.
\end{itemize}

Motivation: integrate piecewise polynomial interpolants instead of using Riemann sums (assuming piecewise constant).
$M_n$ integrates a piecwise constant (discontinuous) interpolate where $p_0(\eta_i)=f(\eta_i)$.
\begin{example}
	Apply midpoint rule with $h=\frac14$ to approximate $\int_0^1 \ln(1+x)\,dx=2\ln 2-1$.
	How small does $h$ need to be to guarantee $|I-M_n(f)|\leq 10^{-6}$ ?
	\begin{align*}
		M_4(f) &= \sum_{i=1}^4 \ln(1+\eta_i)\cdot h \\
			   &= \frac14\blr{ \ln\plr{1+\frac13}+\ln\plr{1+\frac38}+\ln\plr{1+\frac58}+\ln\plr{1+\frac78}} \\
			   &= \frac14 \ln\plr{\frac{19305}{4096}} \approx 0.387588 \\
		I *= 2\ln 2-1 \approx 0.38629
	\end{align*}
	\begin{align*}
		\frac{b-a}{24} h^2 \max_{a\leq\xi\leq b} |f''(\xi)| &\leq 10^{-6} \\
		\frac1{24} h^2 \max_{0\leq\xi\leq 1} \os{=1}{\mlr{-\frac1{(1+x)^2}}} & \leq 10^{-6} \\
		h^2 &\leq 24\cdot 10^{-6} \\
		\fbox{$ h \leq \frac{\sqrt6}{500} \approx 0.0004899$}
	\end{align*}
	$n = \frac1h$, $n\geq 204.1$, \fbox{$n\geq 205$ for $M_n$}.
\end{example}

\subsection*{\S5.2 Improving the Trapezoidal Rule}

\ul{Goal}: Derive an $\mc O(h^4)$ version of $T_n$.
Define $T_n^c(f) = T_n(f) = -\frac1{12} h^2 \blr{ f'(b)-f'(a) }$, which can be approximated with the right choice of difference quotients $\implies$ don't need $f'$.
\begin{observe}
	\begin{align*}
		\int_a^b f(x)\,dx - T_1^c(f)
		&= \us{\text{\S2.5}}{\fbox{$\int_a^b f(x)\,dx - T_1(f)$}} + \frac1{12}(b-a)^2\blr{f'(b)-f'(a)} \\
		&= \ul{-\frac1{12}(b-a)^3 f''(\xi)} + \frac1{12} (b-a)^3 f''(\eta)^{\la\text{\text{MVT}}} \\
		&= \frac1{12} (b-a)^2 \blr{f''(\eta)-f''(\xi)} \qquad |\eta-\xi|\leq b-a \\
		&= \frac1{12} (b-a)^4 f'''(c)
	\end{align*}
	In composite form:
	$\ds |I(f)-T_n^c(f)| \leq ch^3 \max_{a\leq c\leq b} |f'''(c)| $
	lost a power when summing.
	Lazy analysis $\implies$ gained one order of accuracy.
\end{observe}
\begin{fact}
	Error $= \mc O(h^2)$ using a more refined analysis (Grad HW).
\end{fact}
\begin{fact}
	$T_n^c$ is equivalent to exactly integrating the piecewise cubic Hermite interpolate, where interior derivatives cancel in the summation (requires uniform spacing).
\end{fact}

\begin{example}
	Apply the trapezoidal rule and the corrected trapezoidal rule to approximate $$I=\int_0^1 \ln(1+x)\,dx = 2\ln2-1$$ with $h=\frac14$.
	\begin{align*}
		T_4(f) &= \frac h2\blr{\ln(1+0)+2\ln\plr{1+\frac14}+2\ln\plr{1+\frac12}+2\ln\plr{1+\frac34}+\ln(1+1)} \\
			   &\approx 0.3836995. \\
		T_4^c(f) &= T_4(f)-\frac1{12}h^2\blr{\frac1{1+x}\bigg|_{x=1}-\frac1{1+x}\bigg|x_{x=0}} \\
				 &= T_4(f) + \frac1{384} \approx 0.386303676. \\
		I &= 2\ln2-1 \approx 0.38629436. \\
		|I-T_4| &\approx 0.00259. \\
		|I-T_4^c| &\approx 0.00000932.
	\end{align*}
\end{example}

\subsubsection*{\ul{Application} Error Estimation}

The correction term is easily computable $\frac1{12}h^2[f'(b)-f'(a)]$ when compared to the exact error $$\frac1{12}h^3\sum_{i=1}^nf''(\xi_i)$$ and approximate the error to high order. (for $f$ nice)
$\implies$ Bound the approximate error when choosing $h$ $$\frac1{12}h^2|f'(b)-f'(a)|\leq\text{TOL}.$$

\subsection*{\S5.3 Simpson's Rule}

\ul{Goal}: Develop a more accurate method by exactly integrating a piecewise \ul{quadratic} interpolant.
\begin{align*}
	\int_a^b f(x)\,dx &\approx \int_a^b p_2(x)\,dx \qquad \text{where} \\
	p_2(a) &= f(a) \\
	* \ p_2\plr{\frac{a+b}2} &= f\plr{\frac{a+b}2} \\
	p_2(b) &\approx f(b) \\
\end{align*}
\begin{enumerate}
	\item[$*$] increased accuracy by using midpoint.
\end{enumerate}
$$ p_2(x) = f(a) L_0(x) + f\plr{\frac{a+b}2}L_1(x) + f(b)L_2(x)$$ for the Lagrange basis functions $L_j$.
\begin{fact}For $h = \frac{b-a}2$,
	\begin{align*}
		\int_a^b L_0(x)\,dx &= \frac h3, &
		\int_a^b L_1(x)\,dx &= \frac{4h}3, &
		\int_a^b L_2(x)\,dx &= \frac h3.
	\end{align*}
\end{fact}
Thus
\begin{align*}
	\int_a^b f(x)\,dx &\approx \int_a^b p_2(x)\,dx \\
					  &= f(a)\int_a^b L_0(x)\,dx + f\plr{\frac{a+b}2}\int_a^b L_1(x)\,dx + f(b)\int_a^b L_2(x)\,dx \\
					  &= \frac h3\blr{f(a)+4f\plr{\frac{a+b}2}+f(b)} \\
					  &\equiv S_2(f).
\end{align*}
Letting $p_2$ be the \ul{piecewise} quadratic interpolant for a \ul{uniform} mesh $x_i=a+ih$, $i=0,1,\dots,n=2m$, we have $S_n(f) = \frac h3\blr{f(x_0)+4f(x_1)+2f(x_2)+4f(x_3)+ \dots + 2f(x_{n-2})+4f(x_{n-1})+f(x_n)}$.
\begin{enumerate}
	\item[$*$] $n$ must be an even number, $n=2m$.
\end{enumerate}
\begin{example}
	Apply Simpson's rule with $h=\frac14$ to approximate $\int_0^1 \frac1{1+x^3}\,dx$.
	\begin{align*}
		S_4\plr{\frac1{1+x^3}}
		&= \frac{1/4}3\blr{ f(0)+4f\plr{\frac14}+2f\plr{\frac12}+4f\plr{\frac34}+f(1)} \\
		&= \frac1{12}\blr{1+4\plr{\frac{64}{65}}+2\plr{\frac89}+4\plr{\frac{64}{91}}+\frac12} \\
		&= \frac{82141}{98280} \approx 0.835\ul{7}855. \\
		\int_0^1 \frac1{1+x^3}\,dx &= \frac13\ln2+\frac19\sqrt3\pi \\
								   &\approx 0.835\ul{6}488
	\end{align*}
\end{example}

\subsubsection*{Convergence}

Interpolation: $f(x)-p(x) = \frac{\omega_2(x)}{3!} f^{(3)}(\xi_x)$ for some $\xi_x$ in $(a,b)$, where $\omega_2(x) = (x-a)\plr{x-\frac{a+b}2}(x-b)$.
\begin{align*}
	|I(f)-S_2(f)|
	&= \mlr{\int_a^b\blr{f(x)-p_2(x)}\,dx}
	= \mlr{\int_a^b \frac{\omega_2(x)}{3!} f^{(3)}(\xi_x)\,dx} \\
	&\leq \frac{(b-a)^3}6\int_a^b \max_{a\leq\xi\leq b} \mlr{f^{(3)}(\xi)}\,dx
	= \frac{(b-a)^4}6 \max_{a\leq\xi\leq b}\mlr{f^{(3)}(\xi)} \\
	\implies |I(f)-S_{2m}(f)|
	&\leq \frac{(b-a)h^3}5 \max_{a\leq\xi\leq b} \mlr{f^{(3)}(\xi)}.
\end{align*}
\begin{enumerate}
	\item Better than $T_n$, $\mc O(h^2)$
	\item exact for quadratics since then $f^{(3)}=0$.
\end{enumerate}

\ul{Goal}: Show the method is $\mc O(h^4)$ error [same as $T_n^c$]

\begin{observe}
	$I(x^3) = \int_a^b x^3\,dx = S_2(x^3)$, $x_0=a$, $x_1 = \frac{a+b}2$, $h=\frac{b-a}2$, $x_2=b$, so $I(x^3) = \frac14 x^4\big|_a^b = \frac14[b^4-a^4]$.
\end{observe}
\begin{align*}
	S_2(x^3) &= \frac h3\blr{ f(a)+4f\plr{\frac{a+b}3}+f(b)} \\
			 &= \frac{(b-a)/2}3\blr{a^3+4\plr{\frac{a+b}2}^3+b^3} \\
			 &= \frac{b-a}6 \blr{a^3+\frac12(a+b)^3+b^3} \\
			 &= \frac{b-a}{12} \blr{2a^3+(a+b)^3+2b^3} \\
			 &= \frac{b-a}{12} \blr{2a^3+a^3+3a^2b+3ab^2+b^3+2b^3} \\
			 &= \frac{b-a}4 \blr{a^3+a^2b+ab^2+b^3} \\
			 &= \frac14\blr{a^3b+a^2b^2+ab^3+b^4-a^4-a^3b-a^2b^2-ab^3} \\
			 &= \frac14[b^4-a^4] = \frac14x^4\bigg|_a^b = \int_a^b x^3\,dx = I(x^3).
\end{align*}
\begin{enumerate}
	\item[$*$] Required choosing $x_1 = \frac{a+b}2$.
\end{enumerate}
\begin{theorem}
	Simpson's Rule is exact for all cubic functions.
\end{theorem}
\begin{proof}
	Let $p_3$ be a polynomial of degree $\leq 3$.
	Then $p_3(x) = cx^3 + p_2(x)$ for some $c$ and polynomial $p_2$ with degree $\leq 2$.
	Thus, $S_n(p_3) = S_n(cx^3+p_2(x)) = c S_n(x^3) + S_n(p_2) = cI(x^3)+I(p_2) = I(cx^3+p_2(x)) = I(p_3)$.
\end{proof}
\begin{theorem}
	$|I(f)-S_2(f)| = \mc O\plr{(b-a)^5}$.
\end{theorem}
\begin{proof}
	Define the Hermite cubic interpolant $q_3(x)$ by
	\begin{itemize}
		\item $q_3(a) = f(a)$
		\item $q_3(b) = f(b)$
		\item $q_3\plr{\frac{a+b}2} = f\plr{\frac{a+b}2}$
		\item $q_3'\plr{\frac{a+b}2} = f'\plr{\frac{a+b}2}$.
	\end{itemize}
	Then, $\exists c$ such that, for $a\leq x\leq b$, 
	$f(x)-q_3(x) = c\,\psi(x)f^{(4)}(\xi_x)$ with $\psi(x) = (x-a)\plr{x-\frac{a+b}2}^2(x-b)$ (interpolation error).
	Note $|\psi(x)|\leq (b-a)^4$.
	Thus,
	\begin{align*}
		|I(f)-S_2(f)| &= |I(f)-S_2(q_3)| \qquad \text{--- since function values agree}\\
					  &= \mlr{\int_a^b [f(x)-q_3(x)]\,dx} \qquad \text{--- } S_2(q_3)=I(q_3) \text{ -- cubic} \\
					  &\leq c(b-a)^4 \int_a^b \mlr{f^{(4)}(\xi_x)}\,dx \\
					  &\leq c(b-a)^5 \max_{a\leq\xi\leq b} \mlr{f^{(4)}(\xi)}
	\end{align*}
\end{proof}
\begin{corollary}
	$\ds|I(f)-S_{2m}(f)| \leq ch^4(b-a)\max_{a\leq\xi\leq b} \mlr{f^{(4)}(\xi)}$.
\end{corollary}
\begin{fact}
	$I(f)-S_2(f) = -\frac1{90}\plr{\frac{b-a}2}^5 f^{(4)}(\xi)$ and
	$\ds|I(f)-S_{2m}(f)|\leq\frac{b-a}{180}h^4\max_{a\leq\xi\leq b}\mlr{f^{(4)}(\xi)}$.
\end{fact}
\begin{example}
	How small of an $h$ is needed to have $|I(f)-S_{2m}(f)|\leq 10^{-6}$ for $I(f) = \int_0^1 \ln(1+x)\,dx = 2\ln2-1$?
	\begin{align*}
		\frac{b-a}{180} h^4 \max_{a\leq\xi\leq b} \mlr{f^{(4)}(\xi)}
		&\leq 10^{-6} \\
		\frac1{180} h^4 \max_{0\leq\xi\leq1} \mlr{-\frac6{(x+1)^4}}
		&\leq 10^{-6} \\
		\frac1{180} h^4 \cdot 6 &\leq 10^{-6} \\
		h^4 &\leq \frac{180}6\cdot10^{-6} = \frac3{100,000} \\
		h &\leq \frac1{10} \nrt4{\frac3{10}} \approx 0.074008.
	\end{align*}
	$2m = \frac1h = 13.15$, $2m\geq14$ --- next bigest even number \fbox{$n\geq14$}, $M_n(f)\implies n\geq 205$.
\end{example}

\subsection*{Gaussian Quadrature}

\ul{Goal}: $I_n = \sum_{i=1}^n \omega_i^{(n)} f(x_i^{(n)})$ on $(a,b)$.
Choose the weights and \ul{nodes} such that $I_n(p) = I(p)$ for $p$ a polynomial of degree $\leq d$ with $d$ as big as possible.
\ul{Idea}: $f(x) = p_d(x) + R(x)$, Error $= I(R)-I_n(R)$ should be smaller when $d$ is maximized (Taylor's series: $R\ra0$ as $d\ra\infty$).

\begin{observe}
	\begin{align*}
		M_1(f) &= \int_a^b f\plr{\frac{a+b}2}\,dx = (b-a)f\plr{\frac{a+b}2} \\
		M_2(f) &= \frac{b-a}2f\plr{a+\frac{b-a}4}+\frac{b-a}2f\plr{a+\frac34(b-a)}
	\end{align*}
	writing $M_n(f) = I_n(f) = \sum_{i=0}^n \omega_i^{(n)} f(x_i^{(n)})$, we have the weights and nodes depend upon $a,b$.
\end{observe}
\begin{itemize}
	\item[$*$] Make one rule on the interval $[-1,1]$ and describe a way to adapt the rule to $[a,b] \implies$ only need one list of weights and nodes.
\end{itemize}
\ul{Substitution} $a\leq x\leq b \longleftrightarrow -a\leq u\leq1$, let $\fbox{$x = a+\frac{b-a}2(u+1)$}=\frac{b-a}2u+\frac{a+b}2$, so $x(-1)=a$ and $x(1)=b$.
Then $u = -1+2\frac{x-a}{b-a} = \frac{2x-a-b}{b-a}$, so $u(a)=-1$ and $u(b)=1$.
Thus $du=\frac2{b-a}\,dx$ and \fbox{$dx=\frac{b-a}2\,du$}.
$$ \int_a^b f(x)\,dx = \int_{-1}^1 f\plr{a+\frac{b-a}2(u+1)}\cdot\frac{b-a}2\,du$$
Let $F(u) = f\plr{a+\frac{b-a}2(u+1)}\cdot\frac{b-a}2$.
Then $$ \us{a\leq x\leq b}{\int_a^b f(x)\,dx}=\us{-1\leq u\leq1}{\int_{-1}^1 F(u)\,du}.$$
Replacing $f(x)$ with $F(u)$, we integrate $F(u)$ over the interval $[-1,1]$.
\begin{enumerate}
	\item[$*$] The rescaling preserves the definite integral.

		We only need rules on $-1\leq u\leq1$.
\end{enumerate}

\ul{Gaussian Quadrature}: $G_n(f) = \sum_{i=\ul{\ul{1}}}^n \omega_i^{(n)} F(u_i^{(n)})$ for $-1\leq u_i^{(n)}\leq 1$ for all $i$.
\begin{align*}
	n & & u_i^{(n)} & & \omega_i^{(n)} \\
	\hline
	1 & & 0 & & 2 \\
	2 & & \pm\sqrt{\frac13} & & 1 \\
	3 & & 0,\pm\sqrt{\frac35} & & \frac89,\frac59 \\
	4 & & \pm\sqrt{\frac37-\frac27\sqrt{\frac65}}, \pm\sqrt{\frac37+\frac27\sqrt{\frac65}} & & \frac{18+\sqrt{30}}{36}, \frac{18-\sqrt{30}}{36}
\end{align*}
\begin{example}
	\begin{align*}
		G_3(f) &= \sum_{i=1}^3 \omega_i^{(3)} F(u_i^{(3)}) \\
			   &= \omega_1^{(3)} F(u_1^{(3)}) + \omega_2^{(3)} F(u_2^{(3)}) + \omega_3^{(3)} F(u_3^{(3)}) \\
			   &= \frac59 F\plr{-\sqrt{\frac35}} + \frac89 F(0) + \frac59 F\plr{\sqrt{\frac35}}
	\end{align*}
\end{example}
\begin{fact}
	$G_n(f)$ is exact if $f$ is a polynomial of degree $d\leq2n-1$.
\end{fact}
\begin{example}
	$G_3(x^p)$ is exact for $0\leq p\leq 2(3)-1=5$ --- Same number of points as Simpson's rule $S_1$ ($u=01,0,1$, $0\leq p\leq3$).
\end{example}
\begin{example}
	\begin{align*}
		I &= \int_0^1 \ln(1+x)\,dx = 2\ln(2)-1 \\
		f(x) &= \ln(1+x), \qquad a=0, \qquad b=1 \\
		F(u) &= \ln\plr{1+\frac12(u+1)}\plr{\frac{1-0}2} = \ln\plr{\frac{u+3}2}\cdot\frac12. \\
		G_1(f) &= 2\cdot\frac12\ln\plr{\frac{0+3}2}=\ln\plr{\frac32}\ \plr{= M_1(f)}. \\
		G_2(f) &= 1\cdot\frac12\ln\plr{\frac{\sqrt{\frac13}+3}2}+1\cdot\frac12\ln\plr{\frac{-\sqrt{\frac13}+3}2}
			   = \frac12\ln\plr{\frac{13}6}. \\
		G_3(f) &= \frac59\cdot\frac12\ln\plr{\frac{-\sqrt{\frac35}+3}2} + \frac89\cdot\frac12\ln\plr{\frac32} + \frac59\cdot\frac12\ln\plr{\frac{\sqrt{\frac35}+3}2}
			   = \ln\blr{\plr{\frac75}^{5/18}\plr{\frac32}^{13/18}}.
	\end{align*}
	\begin{align*}
		\text{Rule} & & \text{Error} \\
		G_1 & & 0.01917 \\
		G_2 & & 0.000306 \\
		G_3 & & 0.0000060605
	\end{align*}
	Simpson's rule with error $\leq 6.1\times10^{-6} \implies n\geq10$ (8.5978), 11 points using $x_0=a$, but $G_3$ used 3 points.
\end{example}

\end{document}
